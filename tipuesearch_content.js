var tipuesearch = {"pages":[{"title":"Contact","text":"Hey there! I'm Jack. I recently completed my bachelor of Electrical Engineering at the University of Newcastle. I like to play guitar/drums, make things, and getting in or under the water. This site is just meant to function as half blog, half showcase of a few things I like to do. Please look around, and if you'd like to contact me about anything at all, please do! Your Name: Your Email: Message: Send","tags":"misc","url":"https://jackmckew.dev/pages/contact.html","loc":"https://jackmckew.dev/pages/contact.html"},{"title":"CV/Professional","text":"My CV and honours thesis are linked below (PDF download). My experience thus far within the electrical industry is: 2016-2018 at Hunter H2O as an undergraduate working on PLC programming, PLC systems, radio network system design, SCADA systems, HMI development, cable schedules, electrical protection schemes 2017 - Current at AECOM as a graduate electrical engineer (undergraduate from 2017-2018) working on lighting design, reticulation design, earthing system design, switchboard/switchroom design, communication network design, lightning protection system design and site inspections Within AECOM, I have also developed several in-house automated data workflows (preparing, visualizing and summarising data sets) software within Python and packaging/distributing software across multiple offices (even made it to Japan!) along with support and documentation maintenance. If you have any opportunities you think I would be a good fit for, please don't hesitate to contact me by emailing me directly at jackmckew2@gmail.com . Curriculum Vitae Honour's Thesis: \"In Home Appliance Scheduler Using Home Area Network\"","tags":"misc","url":"https://jackmckew.dev/pages/cv-professional.html","loc":"https://jackmckew.dev/pages/cv-professional.html"},{"title":"Test Article Linking Jupyter","text":"","tags":"Python","url":"https://jackmckew.dev/test-article.html","loc":"https://jackmckew.dev/test-article.html"},{"title":"Making Executables Installable with Inno Setup","text":"Following on from last week's post on making executable GUIs with Gooey, this post will cover how to make the executables we have packaged up into installable files so our users can run them easily. Once we have created the executable file for our GUI (which will be located in the dist folder: Now we are going to use a program called Inno Setup, which can be downloaded from: http://www.jrsoftware.org/isinfo.php. After you've installed Inno Setup, run these commands: 1) Select create a new script file using the Script Wizard 2) Fill in the application information 3) Leave defaults 4) Select the *.exe file found in the dist folder 5) Select shortcut choices 6) Add any license or information files 7) Select install mode 8) Select the languages 9) Provide compiler settings and icon for installable 10) Leave default 11) Compile new script 12) Share around the executable installer! Once installed, it will now act and behave like any other software installed on your computer!","tags":"Software Development","url":"https://jackmckew.dev/making-executables-installable-with-inno-setup.html","loc":"https://jackmckew.dev/making-executables-installable-with-inno-setup.html"},{"title":"Making Executable GUIs with Python, Gooey & Pyinstaller","text":"Today we will go through how to go from a python script to packaged executable with a guided user interface (GUI) for users. First off we still start by writing the scripts that we would like to share with others to be able to use, especially for users that may be uncomfortable in a programming environment and would feel at home with a GUI. My personal favourite part about Gooey , is that you are essentially creating a command line interface (CLI) tool, which Gooey then uses to generate a GUI. This eliminates having two separate code bases to facilitate CLI & GUI users, which can be very painful at times. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def print_file_name ( path , filesize ): \"\"\" Inputs: path (str): filepath to file selected filezize (bool): whether to print the file size or not Prints file name of file from path given and if filesize is true then will print the total size of the file in bytes \"\"\" print ( os . path . basename ( path )) if filesize : print ( f \"File size: {os.path.getsize(path)} bytes\" ) def get_files_in_folder ( path , extension ): \"\"\" Inputs: path (str): path to folder selected extension (str): extension to filter by Prints all files in folder, if an extension is given, will only print the files with the given extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f The 2 functions defined above are for getting information of selected files, or returning a list of files found within a folder (and subfolders). Now to use Gooey , we need to define a 'main' function for parsing the arguments for the GUI to generate controls. As Gooey is based on the argparse library, if you have previously built CLI tools with argparse, the migration to Gooey is quite simplistic. However as there is always edge cases, ensure to check your tools functionality once you have developed it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Gooey ( optional_cols = 2 , program_name = \"Gooey Executable with Pyinstaller\" ) def parse_args (): prog_descrip = 'Pyinstaller example with Gooey' parser = GooeyParser ( description = prog_descrip ) sub_parsers = parser . add_subparsers ( help = 'commands' , dest = 'command' ) first_parser = sub_parsers . add_parser ( 'file' , help = 'This function prints the chosen file name' ) first_parser . add_argument ( 'file_path' , help = 'Select a random file' , type = str , widget = 'FileChooser' ) first_parser . add_argument ( '--file-size' , help = 'Do you want to print the file size?' , action = 'store_true' ) second_parser = sub_parsers . add_parser ( 'folder' , help = 'This funtion prints all files in a folder' ) second_parser . add_argument ( 'folder_path' , help = 'Select a folder' , type = str , widget = 'DirChooser' ) second_parser . add_argument ( '--file-type' , help = 'Specify file type with .jpg' , type = str ) args = parser . parse_args () return args By using the Gooey decorator we are able to define many different layout options for our GUI. Since we are trying to enable users to use multiple scripts which are different and separate, I personally like to the optional columns layout, but there are many other types of layouts which can be seen here: https://github.com/chriskiehl/Gooey#layout-customization. Following this we create our argument parsing function, and in which we define parsers, subparsers and add the arguments. This post will not be covering how to write CLIs, but it is on the list for future posts. To complete the script, we need to put in the functionality at startup. 1 2 3 4 5 6 if __name__ == '__main__' : conf = parse_args () if conf . command == 'file' : print_file_name ( conf . file_path , conf . file_size ) elif conf . command == 'folder' : print ( get_files_in_folder ( conf . folder_path , conf . file_type )) By embedding the command names within the arguments we are able to use a variety of functions which may or may not be interconnected. Once this file is run will generate the following: Which are fully embedded within the windows file explorer system for selecting files, folders, etc. Now to package this GUI as an executable, we use PyInstaller . By following Chris Kiehl's (Developer of Gooey) instructions on using Pyinstaller and Gooey: https://chriskiehl.com/article/packaging-gooey-with-pyinstaller. All we need to is create a build.spec file within our directory and run pyinstaller build.spec. This will then generate a build folder and a dist folder within your current directory. The build folder will contain all the files used in generating the executable, which is found within the dist folder. The code in it's entirety is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 from gooey import Gooey , GooeyParser import os @Gooey ( optional_cols = 2 , program_name = \"Gooey Executable with Pyinstaller\" ) def parse_args (): prog_descrip = 'Pyinstaller example with Gooey' parser = GooeyParser ( description = prog_descrip ) sub_parsers = parser . add_subparsers ( help = 'commands' , dest = 'command' ) first_parser = sub_parsers . add_parser ( 'file' , help = 'This function prints the chosen file name' ) first_parser . add_argument ( 'file_path' , help = 'Select a random file' , type = str , widget = 'FileChooser' ) first_parser . add_argument ( '--file-size' , help = 'Do you want to print the file size?' , action = 'store_true' ) second_parser = sub_parsers . add_parser ( 'folder' , help = 'This funtion prints all files in a folder' ) second_parser . add_argument ( 'folder_path' , help = 'Select a folder' , type = str , widget = 'DirChooser' ) second_parser . add_argument ( '--file-type' , help = 'Specify file type with .jpg' , type = str ) args = parser . parse_args () return args def print_file_name ( path , filesize ): \"\"\" Inputs: path (str): filepath to file selected filezize (bool): whether to print the file size or not Prints file name of file from path given and if filesize is true then will print the total size of the file in bytes \"\"\" print ( os . path . basename ( path )) if filesize : print ( f \"File size: {os.path.getsize(path)} bytes\" ) def get_files_in_folder ( path , extension ): \"\"\" Inputs: path (str): path to folder selected extension (str): extension to filter by Prints all files in folder, if an extension is given, will only print the files with the given extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f if __name__ == '__main__' : conf = parse_args () if conf . command == 'file' : print_file_name ( conf . file_path , conf . file_size ) elif conf . command == 'folder' : print ( get_files_in_folder ( conf . folder_path , conf . file_type ))","tags":"Python","url":"https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html","loc":"https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html"},{"title":"Hands On Machine Learning Chapter 3","text":"Chapter 3 is focusing in on classification systems. As brought up earlier, most common supervised machine learning tasks are regression (predicting values) and classification (predicting classes). This chapter goes through the 'Hello World' of classification tasks, the MNIST dataset. The MNIST dataset is a set of 70,000 images of handwritten digits written by high school students and employees of the US Census Bureau. Thankfully each image is also labelled with the digit it represents. Chapter 3 also introduces one of my personal favourite ways of evaluating classification performance, a confusion matrix. A confusion matrix is built up of rows and columns, rows representing the actual classification and columns representing the predicted classification . In a perfect classifier, the diagonal from left to right will be full of numbers ( true positives (TP) and true negatives (TN) and every where else will be 0. Whenever there is a number to the upper right of the diagonal, this represents any false positives (FP), while the lower left of the diagonal, representing false negatives (FN). Another way to assess the performance is to use the accuracy of the positive predicts, called the precision of the classifier. $$ \\frac{TP}{TP + FP} $$ Another metric that goes hand-in-hand with precision is the recall of a classifier. Which is the ratio of true positives that are correctly classified. $$ \\frac{TP}{TP+FN} $$ Or you can combine both precision and recall into a single metric, namely the F1 score . The F1 score is the harmonic mean of precision and recall. The harmonic mean gives much more weight to the low values, meaning the F1 score will only be high if the recall and precision are high. $$ \\frac{TP}{TP+\\frac{FN+FP}{2}} $$ Precision/Recall Tradeoff As above, when comparing precision and recall, you cannot have 100% of either, instead it is a trade off. With a precision of 100%, means all the samples classified as positive are true positives, however there may be a lot more that are now false positives. With a recall of 100%, all samples classified will include all of the true positives, however now all the false positives are included. Deciding the trade off comes down to the application. For example, if you wanted to create a classifier that detects websites that are safe for kids, you would prefer a classifier that rejects many good websites (low recall), but keeps only safe ones (high precision). On the other hand, if you wanted to create a classifier that detects threats in messages, it is probably fine to have a 25% precision, as long as it has 99% recall; meaning the authorities will get a few false alerts, but almost all threats will be identified. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-3.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-3.html"},{"title":"Book Review: Principles by Ray Dalio","text":"I've just finished reading Ray Dalio's book Principles , and wanted to use this space to jot down some of my personal takeaways and thoughts. To give some background, Ray Dalio, founder of Bridgewater Assosciates, one of the largest hedge funds in the world, wrote this book around the unique principles that he discovered through his over forty year career that led to his and Bridgewater's success. Dalio believes that it is these principles, that are the reason behind whatever success he has had. Before I get into the thick of it, I would recommend this book to anyone that is feeling they are lacking some direction in their life. One of the organizational techniques I've picked up lately is Bullet Journaling , and one of the takeaways I got from the book was to start prioritizing my to-dos, in comparison to what I do now which is just dump it all onto the paper. The book is broken into two major sections: Life Principles and Work Principles, followed by descriptions of the tools developed to work alongside the principles to help keep the concept of an idea meritocracy intact. The concept of an idea meritocracy, is where you can have a community/company in which the best idea wins, no matter who/where it came from. This is in comparison to regular company structures where the decisions come from top down, which has its pros/cons as well. Life Principles On page 246, there is a summary of the life principles which are fully detailed in the preceding chapter. While I won't go into detail on Ray's perspective of the points, however I'll give a brief description on my personal takeaway. Please note the points are listed in no particular order. 1.1 - Be a hyperrealist - you need to understand, accept and work with reality to be able to appreciate and move forward with things effectively 1.3 - Radical transparency - while this can be a tough habit to develop, it pays itself off time and time again for both yourself and the people around you 1.4 - Look to nature to learn how reality works - if you haven't noticed in my blog posts, I am extremely curious about the natural way of evolution and look to bring it's methods into my everyday life and projects. I believe that Dalio also appreciates the power of evolution and is integrating it into his work (as he later describes) 2 - 5 Step process to get what you want out of life Have clear goals Identify and don't tolerate the problems that stand in the way of our achieving those goals Accurately diagnose the problems to get at their root causes Design plans that will get you around them Do what's necessary to push these designs through to results 3 - Radical open mind - something that I have blogged about in the past, open mindedness, you can find that post here: https://jmckew.com/2018/12/07/episode-3-open-mind/ 4.5 - Getting the right people in the right roles in support of your goal is the key to succeeding at whatever you choose to accomplish 5.10 - Believability weight your decision making - I believe when gathering opinions and thoughts on something, you must weight the opinions based upon how 'believable' the source is Work Principles Dalio believes that culture is the most important part of any organization. I also share the same perspective that any company consists of two major components: culture and people. If the culture isn't right, the right people won't stay. 1.1 - Realize you have nothing to fear from knowing the truth - while sometimes harsh, having more information can always be beneficial if looked at from the right perspective 3.1 - Recognize that mistakes are a natural part of the evolutionary process - keeping with the theme of evolution is one of the greatest things in nature, it must be accepted that mistakes are apart of it all and are inevitable 3.2 - Don't worry about looking good, worry about achieving your goals - this is one that I try to embrace more and more each time I do something, admittedly at times I will withhold from sharing with others from how much progress has been made, by not sharing, everyone is losing out from sharing the learning experience as well 3.4 - Remember to reflect when you experience pain - it is always a good idea to look back on an experience that didn't go to plan or badly to make sure you adapt and evolve for next time 5.2 - Find the most believable people possible to disagree with you and try to understand their reasoning - a great way at validating your reasoning behind something is to ask someone else to poke holes through it to see if it still stands I believe that the way that you perceive the points in this book is dependent on the stage of life that you are at personally. However, even reading through them now, while I can't resonate with them at this point in time, I am glad that they have planted the seed for later on, which may be of great assistance. That is why I believe you should read this book.","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-principles.html","loc":"https://jackmckew.dev/book-review-principles.html"},{"title":"Intro to Games in Python with Pyglet","text":"Recently, I've been researching ways that I could run a 2D simulation (and hopefully 3D) with many moving pieces and there was a desire to make it interactive as well. I stumbled through many visualisation frameworks such as: p5 pygame plotly panda3d bokeh many others Eventually, through the motivation of another side project (looking into training neural networks to learn how to play games) and inspired by this video from Code Bullet https://www.youtube.com/watch?v=r428O_CMcpI ; I decided on attempting to use Pyglet to do these simulations. While the aforementioned simulations won't be covered in this post, this post aims to demonstrate how I adapted the in-depth tutorial on the Pyglet website (which goes through how to recreate asteroids in Pyglet) to generate vector based objects which can crash into each other. First off as always, start by setting up a virtual environment with your preferred method ( Anaconda or follow my workflow ), since Pyglet has no external dependencies, all you need to do is install the pyglet package. I won't go through all the code in the example, and how it works, I will only go through what I changed in the case to get where I wanted to go. To begin, and make things a bit easier, I downloaded the pyglet-master repository from GitHub ( https://github.com/pyglet/pyglet ) so I didn't have to create and copy the file contents one by one. After going through the different versions with the examples > game folder, I decided all I required was the simple functionality of collision and any further into developing the game wasn't needed for this stage, so I copied out the version 3 folder. If we run 'asteroid.py' from within the version 3 folder, we are met with this screen Now since all I am trying to do is generate multiple objects (which will be shown with the player symbol to indicate direction), I can comment out the lines which give the lives, score, title and interactive player. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Set up the two top labels # score_label = pyglet.text.Label(text=\"Score: 0\", x=10, y=575, batch=main_batch) # level_label = pyglet.text.Label(text=\"Version 3: Basic Collision\", # x=400, y=575, anchor_x='center', batch=main_batch) # Initialize the player sprite # player_ship = player.Player(x=400, y=300, batch=main_batch) # Make three sprites to represent remaining lives # player_lives = load.player_lives(2, main_batch) # Make three asteroids so we have something to shoot at # asteroids = load.asteroids(3, player_ship.position, main_batch) asteroids = load . asteroids ( 100 ,( window_width // 2 , window_height // 2 ), main_batch ) # Store all objects that update each frame in a list # game_objects = [player_ship] + asteroids game_objects = asteroids # Tell the main window that the player object responds to events # game_window.push_handlers(player_ship.key_handler) Now that we've done that, we need to modify the asteroids generator function to use the player sprite. In load.py, you can change simply the img argument to the player image sprite reference like so: 1 new_asteroid = physicalobject . PhysicalObject ( img = resources . player_image , x = asteroid_x , y = asteroid_y , batch = batch ) Now if we run this, the animation will look a little off, because the objects won't be traveling the direction in the direction that the sprite is pointing. This is due to the existing velocity calculation being a random number for both the X and Y component. To make the player sprites move in the direction they are rotated in, and maintain the existing codebase, we will need to convert from polar notation to cartesian . To do this, we add an extra 2 functions into 'util.py' which will do this for us: 1 2 3 4 5 6 7 8 9 def cart2pol ( x , y ): rho = math . sqrt ( x ** 2 + y ** 2 ) phi = math . arctan2 ( y , x ) return ( rho , phi ) def pol2cart ( rho , phi ): x = rho * math . sin ( math . radians ( phi )) y = rho * math . cos ( math . radians ( phi )) return ( x , y ) Note the use of radians in pol2cart, this is due to the affect of quadrants and trigonometric functions . I won't go into detail, but it won't behave like you expect it to. Now to get our player sprites moving in the direction they are rotated, update the code which generates the 'asteroids' to utilise our new function: 1 2 3 new_asteroid . rotation = random . randint ( 0 , 360 ) new_asteroid . velocity_speed = random . random () * 40 new_asteroid . velocity_x , new_asteroid . velocity_y = util . pol2cart ( new_asteroid . velocity_speed , new_asteroid . rotation ) Now when we go and run our main file again, we will met with a screen like this: Where the player sprites will float around in the direction they are pointing, until they crash into another sprite, causing both of them to disappear. This is a quick intro to Pyglet, I am hoping to expand on this simulation and am positive I will be doing further write ups with it in the future.","tags":"Python","url":"https://jackmckew.dev/intro-to-games-in-python-with-pyglet.html","loc":"https://jackmckew.dev/intro-to-games-in-python-with-pyglet.html"},{"title":"Introduction to Pytest & Pipenv","text":"Unit tests in general are good practice within software development, they are typically automated tests written to ensure that a function or section of a program (a.k.a the 'unit') meets its design and behaves as intended. This post won't go into testing structures for complex applications, but rather just a simple introduction on how to write, run and check the output of a test in Python with pytest. As this post is on testing, I also thought it might be quite apt for trialing out a difference package for dependency management. In the past I've used anaconda, virtualenv and just pip, but this time I wanted to try out pipenv. Similar to my post Python Project Workflow where I used virtualenv, you must install pipenv in your base Python directory, and typically add the Scripts folder to your path for ease later on. Now all we need to do is navigate to the folder and run: 1 pipenv shell This will create a virtual environment somewhere on your computer (unless specified) and create a pipfile in the current folder. The pipfile is a file that essentially describes all the packages used within the project, their version number & so on. This is extremely useful when you pick it back up later on and find where you were at or if you wish to share this with others, they can generate their own virtual environment simply from the pipfile with: 1 pipenv install -- dev Enough about pipenv, let's get onto trying out pytest. For this post I will place both my function and it's tests in the same file, however, from my understanding it's best practice to separate them, specifically keeping all tests within an aptly named 'tests' directory for your project/package. First off let's define the function we intend to test later: 1 2 def subtract ( number_1 , number_2 ): return number_1 - number_2 Now we want to test if our function returns 1 if we give it number_1 = 2 and number_2 = 1: 1 2 3 4 import pytest def test_subtract (): assert subtract ( 2 , 1 ) == 1 To run this test, open the pipenv shell like above in the directory of the file where you've written your tests and run: 1 pytest file_name . py This will output the following: Each green dot represents a single test, and we can see that our 1 test passes in 0.02 seconds. To get more information from pytest, use the same command with -v (verbose) option: 1 pytest file_name . py - v Now we might want to check that it works for multiple cases, to do this we can use the parametrize functionality of pytest like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 import pytest def subtract ( number_1 , number_2 ): return number_1 - number_2 @pytest.mark.parametrize ( 'number_1, number_2, expected' , [ ( 2 , 1 , 1 ), ( 5 , 1 , 4 ), ( 6 , 2 , 4 ), ( - 2 , 1 , - 3 ), ]) def test_subtract ( number_1 , number_2 , expected ): assert expected == subtract ( number_1 , number_2 ) Once run with the verbose command, we get the output: Hopefully this post is a gentle introduction to what unit testing can be in Python.","tags":"Python","url":"https://jackmckew.dev/introduction-to-pytest-pipenv.html","loc":"https://jackmckew.dev/introduction-to-pytest-pipenv.html"},{"title":"Inheritance in Python","text":"As Python is a high level, general purpose programming language, which supports users to define their own types using classes, which are most often following the concept of object-oriented programming. Object-oriented programming is a type of software design in which users not only define the data type (eg, int) of a data structure, but also the types of functions that can be applied. Object-oriented programming is built up of a lot of concepts, to name a few: Inheritance Abstraction Class Encapsulation so on This post will cover an introduction to the concept of inheritance using Python and the animal kingdom. First off, we are going to start by defining our 'base' class (also known as abstract class) of our Animal with common properties: 1 2 3 4 5 6 7 8 9 10 11 12 class Animal (): def __init__ ( self , name = 'Animal' ): self . name = name def family ( self ): print ( \"Animal Kingdom\" ) def speak ( self ): raise Exception ( \"Not implemented yet (define speak)\" ) def eat ( self ): raise Exception ( \"Not implemented yet (define eat)\" ) Now that we have our base class, we can define a subclass 'Dog' that will be able to speak if we define the function inside, but we can also see that it derives from it's parent class 'Animal' by printing out it's family. 1 2 3 4 5 6 7 8 9 10 11 class Dog ( Animal ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) def speak ( self ): print ( \"Woof!\" ) dog = Dog ( \"Jay\" ) dog . speak () dog . family () Which will print out: 1 2 Woof ! Animal Kingdom See my post on dunders (double underscores) to get a better understanding of how the __init__ function is working: https://jmckew.com/2019/09/06/dunders-in-python/ Now we can define any subclass which can derive from our parent class 'Animal', or even more we can derive a class from 'Dog' and it will have all it's properties: 1 2 3 4 5 6 7 class JackRussell ( Dog ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) dog_2 = JackRussell ( 'Jeff' ) dog_2 . speak () dog_2 . family () Which will also print: 1 2 Woof ! Animal Kingdom Now what if we wanted to specify the family that all of our dog classes are, we can do this by overriding their parent class (similar to how we are overriding the speak function): 1 2 3 4 5 6 7 8 9 class Dog ( Animal ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) def family ( self ): print ( \"Mammal\" ) def speak ( self ): print ( \"Woof!\" ) Which then when we run both the below code: 1 2 3 4 5 6 7 8 9 10 11 dog = Dog ( \"Jay\" ) dog . speak () dog . family () class JackRussell ( Dog ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) dog_2 = JackRussell ( 'Jeff' ) dog_2 . speak () dog_2 . family () We will now get: 1 2 3 4 Woof ! Mammal Woof ! Mammal You should now be comfortable in understanding how inheritance works. Normally, it's best practice to inherit only from a single parent class when creating subclasses. As multiple inheritance makes your programs less complicated and easier to manage. However, for large programs, it is very difficult to avoid multiple inheritance.","tags":"Python","url":"https://jackmckew.dev/inheritance-in-python.html","loc":"https://jackmckew.dev/inheritance-in-python.html"},{"title":"Dunders in Python","text":"A 'dunder' (double underscores) in Python (also known as a magic method) are the functions within classes having two prefix and suffix underscores in the function name. These are normally used for operator overloading (eg, __init__, __add__, __len__, __repr__, etc). For this post we will build a customized class for vectors to understand how the magic methods can be used to make life easier. First of all before we get into the magic methods, let's talk about normal methods. A method in Python is a function that resides in a class. To begin with our Vector class, we initialise our class and give it a function, for example: 1 2 3 4 class Vector (): def say_hello (): print ( \"Hello! I'm a method\" ) Now to call the method, we simply call the function name along with the Vector instance we wish to use: 1 Vector . say_hello () This will print: 1 Hello ! I ' m a method Now for our vector class, we want to be able to initialise it with certain constants or variables for both the magnitude and direction of our vector. We use the __init__ magic method for this, as it is invoked without any call, when an instance of a class is created. 1 2 3 class Vector (): def __init__ ( self , * args ): self . values = args Now when we create an instance of our Vector class, we can give it certain values that it will store in a tuple: 1 2 3 vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) Which will print: 1 < __main__ . Vector object at 0x03E90530 > But to us humans, this doesn't mean much more than we know what the name of the class is of that instance. What we really want to see when we call print on our class is the values inside it. To do this we use the __repr__ magic method: 1 2 3 4 5 6 7 8 9 class Vector (): def __init__ ( self , * args ): self . values = args def __repr__ ( self ): return str ( self . values ) vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) Which will print: 1 ( 1 , 2 , 3 ) This is exactly what we want! Now what if we wanted to create a Vector, but we weren't sure what values we wanted to give it yet. What would happen if we didn't give it any values? Would it default to (0,0) like we would hope? 1 2 3 empty_vector = Vector () print ( empty_vector ) Which will print: 1 () Not exactly how we need it, so we would need to run a check when the class is being initialized, to ensure that there are values being provided: 1 2 3 4 5 6 7 8 class Vector (): def __init__ ( self , * args ): if len ( args ) == 0 : self . values = ( 0 , 0 ) else : self . values = args def __repr__ ( self ): return str ( self . values ) Which when initialise an empty instance of our Vector now, it will create a (0,0) vector for us! Now what if we wanted to be able to check how many values were inside our vector class? To do this we can use the __len__ magic method>: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Vector (): def __init__ ( self , * args ): if len ( args ) == 0 : self . values = ( 0 , 0 ) else : self . values = args def __repr__ ( self ): return str ( self . values ) def __len__ ( self ): return len ( self . values ) vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) print ( len ( vector_1 )) Which will print: 1 2 ( 1 , 2 , 3 ) 3 Hopefully this post has given you insight into how dunders/magic methods could be used to super power your classes and make life much easier! You can find more information and examples about dunders in Python at: <https://docs.python.org/3/reference/datamodel.html#special-method-names","tags":"Python","url":"https://jackmckew.dev/dunders-in-python.html","loc":"https://jackmckew.dev/dunders-in-python.html"},{"title":"Python Project Workflow","text":"This post will go through my personal preference on project structure and workflow for creating a new project and an insight how I work on my projects from a development point of view. I will go from the very start as if I did not have Python/Git installed on my machine whatsoever. First of all, we need to get Python! Head over to https://www.python.org/downloads/ to get the version of Python you need (or default to the latest Python 3 stable release). For version control in my projects, I also like to use Git so, head on over to https://git-scm.com/downloads to download Git for your operating system. Now once these are installed (if you put them in the default location), Python will default to be located in: C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python37-32. For the next few steps to ensure we are setting up virtual environments for our projects open command prompt here if you are on windows. This will look something like this: The 'cd' command in windows (and other OS) stands for change directory, follow this with a path and you will be brought to that directory. Next whenever I first install Python I like to update pip to it's latest release, to do this use the command in this window: 1 python -m pip install --upgrade pip With pip upgraded to it's current release, it's time to install some very helpful packages for setting up projects: virtualenv and cookiecutter. To install these navigate to the the Scripts folder within the current directory with cd ('cd Scripts') and run 'pip.exe install virtualenv cookiecutter', pip will then work it's magic and install these packages for you. If you take a peek into the Scripts folder now in your Python directory, it'll look a little like this: Now something that I personally like to do is add this folder to your system environment variables in Windows so it's much easier to run any packages in your root Python installation on your PC. To do this: type in 'system environment' into the search command select environment variables from the bottom right corner edit system (or user) path variable browse and select the Scripts directory in your Python installation If you chose to do this step, you will now be able to create virtual environments and cookiecutter templates without having to specify the directory to the executables. It's now time to create a project from scratch. So navigate to where you like to keep your projects (mostly mine is in Documents\\Github\\) but you can put them anywhere you like. Now run command prompt again (or keep the one you have open) and navigate to the dedicated folder (or folders) using cd. For most of my projects lately being of data science in nature, I like to use the cookiecutter-data-science template which you can find all the information about here: https://drivendata.github.io/cookiecutter-data-science/ . To then create a project it is as simple as running: 1 cookiecutter https://github.com/drivendata/cookiecutter-data-science Provide as much information as you wish into the questions and you will now have a folder created wherever you ran the command with all the relevant sections from the template. Whenever starting a new Python project, my personal preference is to keep the virtual environment within the directory, however this is not always a normal practice. To create a virtual environment for our Python packages, navigate into the project and run (if you added Scripts to your Path): 1 virtualenv env This will then initialise a folder within your current directory to install a copy of Python and all it's relevant tools with a folder ('env'). Before we go any further, this is the point that I like to initialise a git repository. To do this, run git init from your command line from within the project directory. Now to finish off the final steps of the workflow that will affect the day-to-day development, I like to use pre-commit hooks to reformat my with black and on some projects check for PEP conformance with flake8 on every commit to my projects repository. This is purely a personal preference on how you would like to work, others like to use pytest and more to ensure their projects are working as intended, however I am not at that stage just yet. To install these pre-commits into our workflow, firstly initialise the virtual environment from within our project by navigating to env/Scripts/activate.bat. This will activate your project's Python package management system and runtime, following this you can install packages from pip and otherwise. For our pre-commits we install the package 'pre-commit': 1 pip install pre-commit Following this to set up the commit hooks create a '.pre-commit-config.yaml' within your main project directory. This is where we will specify what hooks we would like to run before being able to commit. Below is a sample .pre-commit-config.yaml that I use in my projects: 1 2 3 4 5 6 7 8 9 10 11 12 13 repos: - repo: https://github.com/ambv/black rev: stable hooks: - id: black language_version: python3.7 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace Now to install these, activate your virtual environment like above, navigate to the project directory and run 'pre-commit install'. This will install the pre-commit hooks within your git directory. Before going any further, I highly recommend to run 'pre-commit run --all-files' to both ensure pre-commit is working as expected and check if there is any project specific settings you may have to set. On the default cookiecutter data science template with the settings as per above this will show on the pre-commit run (after you have staged changes in git (use git add -A for all)): We can see a different opinions in code formatting appearing already from flake8's output. The black code formatter in Python's code length is 88 characters , not 79 like PEP8. So we will add a pyproject.toml to the project directory where we can specify settings within the black tool: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [ tool.black ] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | \\.docs | _build | buck-out | build | dist )/ ''' For any flake8 specific settings (such as error codes to ignore), we can set a .flake8 file in the project directory as well, which may look like: 1 2 3 4 5 [ flake8 ] ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 max-complexity = 18 select = B,C,E,F,W,T4,B9 Finally we are able to run a commit to our project!","tags":"Python","url":"https://jackmckew.dev/python-project-workflow.html","loc":"https://jackmckew.dev/python-project-workflow.html"},{"title":"Linear Regression: Under the Hood with the Normal Equation","text":"Let's dive deeper into how linear regression works. Linear regression follows a general formula: $$ \\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n $$ Where \\(\\hat{y}\\) is the predicted value, \\(n\\) is the number of features, \\(x_i\\) is the \\(i&#94;{th}\\) feature value and \\(\\theta_n\\) is the \\(n&#94;{th}\\) model parameter. This function is then vectorised which speeds up processing on a CPU, however, I won't go into that further. How does the linear regression model get 'trained'? Training a linear regression model means setting the parameters such that the model best fits the training data set. To be able to do this, we need to be able to measure how good (or bad) the model fits the data. Common ways of measuring this are: Root Mean Square Error (RMSE) Mean Absolute Error (MAE) R-Squared Adjusted R-Squared many others From here on, we will refer to these as the cost function, and the objective is to minimise the cost function. The Normal Equation To find the value of \\(\\theta\\) that minimises the cost function, there is a mathematical equation that gives the result directly, named the Normal Equation $$ \\hat{\\theta} = (X&#94;T \\cdot X)&#94;{-1}\\cdot X&#94;T \\cdot y $$ Where \\(\\hat{\\theta}\\) is the value of \\(\\theta\\) that minimises the cost function and \\(y\\) (once vectorised) is the vector of target values containing \\(y&#94;{(1)}\\) to \\(y&#94;{(m)}\\) . For example if this equation was run on data generated from this formula: 1 2 3 4 import numpy as np X = 10 * np . random . rand ( 100 , 1 ) y = 6 + 2 * X + np . random . rand ( 100 , 1 ) Now to compute \\(\\hat{\\theta}\\) with the normal equation, we can use the inv() function from NumPy's Linear algebra module: 1 2 X_b = np . c_ [ np . ones (( 100 , 1 )), X ] theta_best = np . linalg . inv ( X_b . T . dot ( X_b )) . dot ( X_b . T ) . dot ( y ) With the actual function being \\(y = 6 + 2x_0 + noise\\) , and the equation found: 1 2 array ([[ 5.96356419 ], [ 2.00027727 ]]) Since the noise makes it impossible to recover the exact parameters of the original function now we can use \\(\\hat{\\theta}\\) to make predictions: 1 y_predict = X_new_b . dot ( theta_best ) With y_predict being: 1 2 [[ 5.96356419 ] [ 9.96411873 ]] The equivalent code using Scikit-Learn would look like: 1 2 3 4 5 from sklearn.linear_model import LinearRegression lin_reg = LinearRegression () lin_reg . fit ( X , y ) print ( lin_reg . intercept_ , lin_reg . coef_ ) print ( lin_reg . predict ( X_new )) And it finds: 1 2 3 [ 5.96356419 ] [[ 2.00027727 ]] [[ 5.96356419 ] [ 9.96411873 ]] Using the normal equation to train your linear regression model is linear in regards to the number of instances you wish to train on, meaning you will need to be able to fit the data set in memory. There are many other ways of to train a linear regression, some which are better suited for large number of features, these will be covered in later posts. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Data Science","url":"https://jackmckew.dev/linear-regresssion-under-the-hood-with-the-normal-equation.html","loc":"https://jackmckew.dev/linear-regresssion-under-the-hood-with-the-normal-equation.html"},{"title":"Intro to Web Scraping","text":"Following on from last weeks post where we analysed the amount of repeated letters within current New Zealand town names . There was still one part of that analysis that really bugged me, and if you noticed it was from the data set that was used was using the European town names not the original Maori names. This post will be dedicated to introducing web scraping where we will extract the Maori names and run a similar analysis to present an interactive graph. As like previously, let's take a look at the interactive graph before getting into how it was created. In [11]: from bokeh.resources import CDN from bokeh.embed import file_html html = file_html ( p , CDN , \"NZ_City_Letter_Analysis\" ) from IPython.core.display import HTML HTML ( html ) Out[11]: NZ_City_Letter_Analysis Bokeh.set_log_level(\"info\"); {\"6a79ccec-7604-45e6-8eef-74d2887334ba\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"1010\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"data_source\":{\"id\":\"1001\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1037\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1038\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1040\",\"type\":\"CDSView\"}},\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"Count\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"index\"}},\"id\":\"1037\",\"type\":\"VBar\"},{\"attributes\":{\"source\":{\"id\":\"1001\",\"type\":\"ColumnDataSource\"}},\"id\":\"1040\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"formatter\":{\"id\":\"1043\",\"type\":\"CategoricalTickFormatter\"},\"ticker\":{\"id\":\"1015\",\"type\":\"CategoricalTicker\"}},\"id\":\"1014\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1015\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"callback\":null,\"data\":{\"Count\":[10,1,0,1,4,1,6,3,5,0,4,2,3,6,6,3,0,5,2,4,4,1,3,0,1,0],\"Word_Name\":[\"Aratiatia Lakes, Aratiatia Rapids\",\"Kaik\\u016bmera Bay\",\"Ahuroa\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Keteketerau\",\"Kaipara Flats\",\"Mangangarongaro: Mangangarongaro Stream\",\"Whatiwhatihoe\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Ahuroa\",\"Pukek\\u0101k\\u0101riki\",\"M\\u0101k\\u014dhine Valley\",\"Mangangarongaro: Mangangarongaro Stream\",\"Mangangarongaro: Mangangarongaro Stream\",\"Mokoroa: Mokoroa Stream\",\"Pukepiripiri\",\"Ahuroa\",\"Mangangarongaro: Mangangarongaro Stream\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Motukauri: Motukauri Island\",\"Awah\\u014dhonu River\",\"Waianiwaniwa\",\"Ahuroa\",\"Kaik\\u016bmera Bay\",\"Ahuroa\"],\"index\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]},\"selected\":{\"id\":\"1049\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1048\",\"type\":\"UnionRenderers\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"overlay\":{\"id\":\"1047\",\"type\":\"BoxAnnotation\"}},\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"1015\",\"type\":\"CategoricalTicker\"}},\"id\":\"1017\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Word\",\"@Word_Name\"]]},\"id\":\"1002\",\"type\":\"HoverTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1043\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"ResetTool\"},{\"attributes\":{\"text\":\"Letter Counts\"},\"id\":\"1004\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1028\",\"type\":\"HelpTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1047\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1022\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"factors\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]},\"id\":\"1006\",\"type\":\"FactorRange\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1023\",\"type\":\"PanTool\"},{\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"id\":\"1026\",\"type\":\"SaveTool\"},{\"id\":\"1027\",\"type\":\"ResetTool\"},{\"id\":\"1028\",\"type\":\"HelpTool\"},{\"id\":\"1002\",\"type\":\"HoverTool\"}]},\"id\":\"1029\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"Count\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"index\"}},\"id\":\"1038\",\"type\":\"VBar\"},{\"attributes\":{\"below\":[{\"id\":\"1014\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"1017\",\"type\":\"Grid\"},{\"id\":\"1022\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1018\",\"type\":\"LinearAxis\"}],\"plot_height\":250,\"renderers\":[{\"id\":\"1039\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1004\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1029\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"1006\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"1010\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"1008\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1012\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1008\",\"type\":\"DataRange1d\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.3.1\"}} (function() { var fn = function() { Bokeh.safely(function() { (function(root) { function embed_document(root) { var docs_json = document.getElementById('1114').textContent; var render_items = [{\"docid\":\"6a79ccec-7604-45e6-8eef-74d2887334ba\",\"roots\":{\"1003\":\"44d6f2ea-a84c-406e-98f4-090d20f0aa6f\"}}]; root.Bokeh.embed.embed_items(docs_json, render_items); } if (root.Bokeh !== undefined) { embed_document(root); } else { var attempts = 0; var timer = setInterval(function(root) { if (root.Bokeh !== undefined) { embed_document(root); clearInterval(timer); } attempts++; if (attempts > 100) { console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\"); clearInterval(timer); } }, 10, root) } })(window); }); }; if (document.readyState != \"loading\") fn(); else document.addEventListener(\"DOMContentLoaded\", fn); })(); Similarly with most of my posts of this nature, we always begin by getting the data. To find a data set that gives us as many Maori town or place names as possible proved to be quite challenging, but luckily for Maori Language week NZhistory.gov.nz posted a table of a 1000 Maori place names, their components and the meaning. This data can be found: https://nzhistory.govt.nz/culture/maori-language-week/1000-maori-place-names . Unlike last time however with our world city names from Kaggle , this data isn't nicely supplied to us in an Excel format. While it may be possible to directly copy-paste from the website into a spreadsheet, I think this is a great way to ease into web scraping. What is Web Scraping? Web scraping, web harvesting or web data extraction is the process of extracting data from websites. To do this in Python, while there is multiple ways to achieve this (requests + beautiful soup, selenium, etc), my personal favourite package to use is Scrapy . While it may be daunting to begin with from a non object-oriented basis, you will soon appreciate it more once you've begun using it. Initially the premise around the Scrapy package is to create 'web spiders'. If we take a look of the structure of the first example on the Scrapy website we get an understanding on how to structure our web spiders when developing: 1 2 3 4 5 6 7 8 9 10 11 import scrapy class BlogSpider ( scrapy . Spider ): name = 'blogspider' start_urls = [ 'https://blog.scrapinghub.com' ] def parse ( self , response ): for title in response . css ( '.post-header>h2' ): yield { 'title' : title . css ( 'a ::text' ) . get ()} for next_page in response . css ( 'a.next-posts-link' ): yield response . follow ( next_page , self . parse ) First of all we can see that the custom spider is essentially an extension of the scrapy.Spider class. It is to be noted that the name and start_urls variables (which are apart of the class) are special in the sense the scrapy package uses them as configuration settings. When it comes to web scraping, if you have had experience using HTML, CSS and/or Javascript, this experience will become extremely useful; that is not to say it is not possible without experience, it's just a learning curve. Following on we can see a function for parsing (also specially named) in which there are 2 loops, the first for loop is going to loop through all title's marked as headers (specifically h2) and return a dictionary with the text in the heading. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class NameSpider ( scrapy . Spider ): name = 'names' start_urls = [ 'https://nzhistory.govt.nz/culture/maori-language-week/1000-maori-place-names/' ] def parse ( self , response ): def extract_from_table ( table_row , table_col ): return response . xpath ( f \"//tr[{table_row}]//td[{table_col}]//text()\" ) . get () for i in range ( 2 , 1000 ): yield { 'Place Name' : extract_from_table ( i , 1 ), 'Components' : extract_from_table ( i , 2 ), 'Meaning' : extract_from_table ( i , 3 ) } Now that we have created our spider that looks through each row of the table on the webpage (more information on determining this can be found: https://docs.scrapy.org/en/latest/intro/tutorial.html ). It's time to run the spider and take a look at the output. To run a spider you go into the directory from the command line and run 'scrapy crawl \\<spider name>' and to store an output at the same time 'scrapy crawl \\<spider name> -o filename.csv -t csv. Now similar to the previous post, we run a similar analysis and plot with Bokeh! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import pandas as pd import collections from collections import OrderedDict import operator import matplotlib.pyplot as plt import numpy as np import math from bokeh.io import show , output_file from bokeh.plotting import figure from bokeh.models import ColumnDataSource from bokeh.models.tools import HoverTool names_df = pd . read_csv ( 'names.csv' , header = 0 , sep = ',' , quotechar = '\"' ) nz_names = names_df [ 'Place Name' ] . tolist () nz_dict = { i : 0 for i in nz_names } letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' lcount = dict ( OrderedDict ([( l , 0 ) for l in letters ])) for name in nz_names : nz_dict [ name ] = dict ( OrderedDict ([( l , 0 ) for l in letters ])) city_dict = nz_dict [ name ] for c in name : if c . upper () in letters : city_dict [ c . upper ()] += 1 total_df = pd . DataFrame . from_dict ( nz_dict ) total_df = total_df . T max_letters_cities = total_df . idxmax () . tolist () lettercounts = total_df . loc [ total_df . idxmax ()] . max () . tolist () maxletters = dict ( OrderedDict ([( l , 0 ) for l in letters ])) for i , l in enumerate ( letters ): maxletters [ l ] = max_letters_cities [ i ] maxletters [ l ] = ( lettercounts [ i ]) summary_df = pd . DataFrame () scale = 1 summary_df [ 'Word_Name' ] = total_df . idxmax () summary_df [ 'Count' ] = total_df . loc [ total_df . idxmax ()] . max () source = ColumnDataSource ( summary_df ) output_file ( \"letter_count.html\" ) hover = HoverTool () hover . tooltips = [ ( 'Word' , '@Word' ) ] p = figure ( x_range = summary_df . index . tolist (), plot_height = 250 , title = \"Letter Counts\" , toolbar_location = None ) p . vbar ( x = 'index' , top = 'Count' , width = 0.9 , source = source ) p . add_tools ( hover ) p . xgrid . grid_line_color = None p . y_range . start = 0 show ( p )","tags":"Python","url":"https://jackmckew.dev/intro-to-web-scraping.html","loc":"https://jackmckew.dev/intro-to-web-scraping.html"},{"title":"Looking for Patterns in City Names & Interactive Plotting","text":"Recently, I was traveling around New Zealand, and noticed in the Maori language they use letters back to back a lot like in the original Maori name for Stratford (\"whakaahurangi\"). So as any normal person does, I thought, well what town has the most repeated letters, and the idea for this blog post was born. Before we get into the nitty gritty, here is the output of the analysis! In [28]: # output_file(\"NZ_City_Letter_Analysis.html\") # save(p) html = file_html ( p , CDN , \"NZ_City_Letter_Analysis\" ) from IPython.core.display import HTML HTML ( html ) Out[28]: NZ_City_Letter_Analysis Bokeh.set_log_level(\"info\"); {\"9a7c834e-4561-48ee-a338-ba00b9647999\":{\"roots\":{\"references\":[{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1211\",\"type\":\"MercatorTicker\"}},\"id\":\"1218\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1197\",\"type\":\"LinearScale\"},{\"attributes\":{\"attribution\":\"&amp;copy; &lt;a href=\\\"https://www.openstreetmap.org/copyright\\\"&gt;OpenStreetMap&lt;/a&gt; contributors,&amp;copy; &lt;a href=\\\"https://cartodb.com/attributions\\\"&gt;CartoDB&lt;/a&gt;\",\"url\":\"https://tiles.basemaps.cartocdn.com/light_all/{z}/{x}/{y}.png\"},\"id\":\"1189\",\"type\":\"WMTSTileSource\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1232\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null,\"end\":-4000000,\"start\":-6000000},\"id\":\"1195\",\"type\":\"Range1d\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Repeated Letter\",\"@letters\"],[\"City Name\",\"@city_name\"],[\"Count\",\"@lettercount\"]]},\"id\":\"1191\",\"type\":\"HoverTool\"},{\"attributes\":{\"callback\":null,\"end\":17900000,\"start\":20000000},\"id\":\"1193\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1199\",\"type\":\"LinearScale\"},{\"attributes\":{\"below\":[{\"id\":\"1201\",\"type\":\"MercatorAxis\"}],\"center\":[{\"id\":\"1209\",\"type\":\"Grid\"},{\"id\":\"1218\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1210\",\"type\":\"MercatorAxis\"}],\"renderers\":[{\"id\":\"1225\",\"type\":\"TileRenderer\"},{\"id\":\"1230\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1232\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1221\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1193\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"1197\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1195\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"1199\",\"type\":\"LinearScale\"}},\"id\":\"1192\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1219\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1202\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"source\":{\"id\":\"1190\",\"type\":\"ColumnDataSource\"}},\"id\":\"1231\",\"type\":\"CDSView\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1211\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"tile_source\":{\"id\":\"1189\",\"type\":\"WMTSTileSource\"}},\"id\":\"1225\",\"type\":\"TileRenderer\"},{\"attributes\":{\"data_source\":{\"id\":\"1190\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1228\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1229\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"1231\",\"type\":\"CDSView\"}},\"id\":\"1230\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1204\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{\"callback\":null,\"data\":{\"city_name\":[\"Kaingapai Hakataramea Station\",\"Abbotsford\",\"Christchurch\",\"Edendale Town District\",\"Earnscleugh Settlement\",\"Flagstaff\",\"Kyeburn Diggings\",\"Christchurch\",\"Kihikihi Town District\",\"Clarks Junction\",\"Kokakoriki\",\"Bell Hill\",\"Benmore Stream\",\"Frankton Junction\",\"Goodwood\",\"Upper Papamoa\",\"Quarry Hills\",\"Sherry River\",\"Simons Pass\",\"Otautau Town District\",\"Murumuru\",\"Five Rivers\",\"Kawakawa Town District\",\"Bexley\",\"Admiralty Bay\",\"Fitzroy\"],\"latitude\":[-5558767.960998884,-5761672.922065697,-5393506.130539223,-5831241.362079489,-5655696.009663975,-5753680.824788744,-5621521.486192067,-5393506.130539223,-4584135.718572788,-5737718.1425425,-4726447.364137138,-5243726.168310261,-5519766.216040407,-4551210.9196918905,-5708527.338604558,-4546515.922191398,-5868929.46316788,-5076469.831602914,-5496444.98987784,-5804419.495079225,-4759941.634140226,-5719131.201735792,-4216097.3615601715,-5365398.062254726,-5004969.470230386,-4728836.17627514],\"lettercount\":[9,2,3,3,5,3,3,3,6,1,4,4,2,4,4,4,1,4,4,5,4,2,3,1,2,1],\"letters\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\"longitude\":[18987394.516745858,18970696.593126863,19217454.72350563,18788874.683951527,18848245.15325427,18978117.8183001,18955853.92014144,19217454.72350563,19519872.710600525,18929879.40939617,19493898.19985525,19102424.620125744,18924313.434856508,19510596.12347424,19004092.44036485,19620060.252314463,18816704.556649845,19228586.67258496,18955853.92014144,18701674.453269962,19495753.45048881,18751768.224126928,19377012.734522317,19154373.75293577,19360314.810903326,19380723.34710893],\"sizes\":[27,6,9,9,15,9,9,9,18,3,12,12,6,12,12,12,3,12,12,15,12,6,9,3,6,3]},\"selected\":{\"id\":\"1238\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1239\",\"type\":\"UnionRenderers\"}},\"id\":\"1190\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"field\":\"sizes\",\"units\":\"screen\"},\"x\":{\"field\":\"longitude\"},\"y\":{\"field\":\"latitude\"}},\"id\":\"1229\",\"type\":\"Circle\"},{\"attributes\":{\"formatter\":{\"id\":\"1213\",\"type\":\"MercatorTickFormatter\"},\"ticker\":{\"id\":\"1211\",\"type\":\"MercatorTicker\"}},\"id\":\"1210\",\"type\":\"MercatorAxis\"},{\"attributes\":{},\"id\":\"1220\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1238\",\"type\":\"Selection\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1213\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{},\"id\":\"1239\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"ticker\":{\"id\":\"1202\",\"type\":\"MercatorTicker\"}},\"id\":\"1209\",\"type\":\"Grid\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.05},\"fill_color\":{\"value\":\"#FF0000\"},\"line_color\":{\"value\":\"#FF0000\"},\"size\":{\"field\":\"sizes\",\"units\":\"screen\"},\"x\":{\"field\":\"longitude\"},\"y\":{\"field\":\"latitude\"}},\"id\":\"1228\",\"type\":\"Circle\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1191\",\"type\":\"HoverTool\"},{\"id\":\"1219\",\"type\":\"WheelZoomTool\"},{\"id\":\"1220\",\"type\":\"SaveTool\"}]},\"id\":\"1221\",\"type\":\"Toolbar\"},{\"attributes\":{\"formatter\":{\"id\":\"1204\",\"type\":\"MercatorTickFormatter\"},\"ticker\":{\"id\":\"1202\",\"type\":\"MercatorTicker\"}},\"id\":\"1201\",\"type\":\"MercatorAxis\"}],\"root_ids\":[\"1192\"]},\"title\":\"Bokeh Application\",\"version\":\"1.3.1\"}} (function() { var fn = function() { Bokeh.safely(function() { (function(root) { function embed_document(root) { var docs_json = document.getElementById('1434').textContent; var render_items = [{\"docid\":\"9a7c834e-4561-48ee-a338-ba00b9647999\",\"roots\":{\"1192\":\"58e3361f-8aef-4cf7-8d23-0a232c72c24b\"}}]; root.Bokeh.embed.embed_items(docs_json, render_items); } if (root.Bokeh !== undefined) { embed_document(root); } else { var attempts = 0; var timer = setInterval(function(root) { if (root.Bokeh !== undefined) { embed_document(root); clearInterval(timer); } attempts++; if (attempts > 100) { console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\"); clearInterval(timer); } }, 10, root) } })(window); }); }; if (document.readyState != \"loading\") fn(); else document.addEventListener(\"DOMContentLoaded\", fn); })(); Firstly, we have to find a dataset of all the town names, and I found a database of all world cities names hosted on Kaggle here: https://www.kaggle.com/max-mind/world-cities-database . Get the data! 1 2 3 # data source https://www.kaggle.com/max-mind/world-cities-database cities_df = pd . read_csv ( './data/worldcitiespop.csv' , header = 0 , sep = ',' , quotechar = '\"' ) cities_df = cities_df [ cities_df [ 'Country' ] == \"nz\" ] After inspecting the data of this data set, we're able to filter out to look at just New Zealand with the prefix of \"nz\" in the Country column. It must be noted that this data set represents the names of the towns currently, and not the original Maori names (more on this will be covered in a later post). Now we want to extract the town names out of the dataframe with the ones we want to analyze. For ease later on, we will extract this as a dictionary, such that we can assign the value of each to the count of each letter. 1 2 nz_cities = cities_df [ cities_df [ 'Country' ] == \"nz\" ][ 'AccentCity' ] . tolist () nz_dict = { i : 0 for i in nz_cities } Now we will create an ordered dictionary with the help from the collections package which will store the values of the count for each letter in the town name. 1 2 letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' lcount = dict ( OrderedDict ([( l , 0 ) for l in letters ])) Now it's time for the data crunch. To count how many times a letter repeats in a town name we follow these steps: we create a for loop, to loop through all the city names in the table, initialise an ordered dictionary similar to above for each city in the value field of that town's dictionary entry loop through each letter in the town name check if the letter appears in our letter dictionary (mainly to not count spaces), Then if the letter does appear, increment the value for that letter by 1. This results in a dictionary for each town name, with the count of repeated letters. 1 2 3 4 5 6 for city in nz_cities : nz_dict [ city ] = dict ( OrderedDict ([( l , 0 ) for l in letters ])) city_dict = nz_dict [ city ] for c in city : if c . upper () in letters : city_dict [ c . upper ()] += 1 Hooray! Now we have all the data we need broken down and ready for analysis. To help ease the analysis and make it more readable for a human, we convert from our nested dictionaries to a pandas dataframe and transpose it such that we have the town name as the index, the letters as the column and the count of that letter as the values. 1 2 total_df = pd . DataFrame . from_dict ( nz_dict ) total_df = total_df . T Now we want to find which of these names have the maximum count for any particular letter and store it in a summary dataframe. It is to be noted that we could use the pivot function with aggregate types, however, I have not figured a nice way to do this yet. If you do know a nicer way to determine this, please let me know. 1 2 3 4 summary_df = pd . DataFrame () scale = 1 summary_df [ 'City_Name' ] = total_df . idxmax () summary_df [ 'Count' ] = total_df . loc [ total_df . idxmax ()] . max () Now by using the equivalent of an index-match in excel which you can read more about here ( https://towardsdatascience.com/name-your-favorite-excel-function-and-ill-teach-you-its-pandas-equivalent-7ee4400ada9f ). Admittedly, we could've made the join earlier, but since I use index-match so often in Excel, I wanted to learn how to do the same in pandas. This is achieved by using the map function (which is the equivalent of the index), but by using the index of another dataframe as the argument (the match function), we can rejoin the data set by matching the city name from our original data set. 1 2 summary_df [ 'Latitude' ] = summary_df [ 'City_Name' ] . map ( cities_df . set_index ([ 'AccentCity' ])[ 'Latitude' ] . to_dict ()) * scale summary_df [ 'Longitude' ] = summary_df [ 'City_Name' ] . map ( cities_df . set_index ([ 'AccentCity' ])[ 'Longitude' ] . to_dict ()) * scale Now we have a dataframe that contains: an index of the letters, the town name with the most repeated letters, the count of the letters within the name, the longitude and latitude of the town For plotting with Bokeh on a basemap, we need to convert from longitude & latitude to easting and northing. To do this we use the pyproj package to make this very simple. 1 2 3 4 5 6 7 def LongLat_to_EN ( long , lat ): try : easting , northing = transform ( Proj ( init = 'epsg:4326' ), Proj ( init = 'epsg:3857' ), long , lat ) return easting , northing except : return None , None This function can be used to generate the easting and northing for every town from it's longitude & latitude and add it to the dataframe. 1 summary_df [ 'E' ], summary_df [ 'N' ] = zip ( * summary_df . apply ( lambda x : LongLat_to_EN ( x [ 'Longitude' ], x [ 'Latitude' ]), axis = 1 )) Finally, it's time to plot our findings on a map. Before we initialise the map in Bokeh , for most plots, data tables and more in Bokeh , we need to put it in the ColumnDataSource form. We also initialise the interactivity when the user hovers over the data points on the plot. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 source = ColumnDataSource ( data = dict ( longitude = list ( summary_df [ 'E' ]), latitude = list ( summary_df [ 'N' ]), sizes = list ( summary_df [ 'Count' ] * 3 ), lettercount = list ( summary_df [ 'Count' ]), city_name = list ( summary_df [ 'City_Name' ]), letters = list ( summary_df . index ))) hover = HoverTool ( tooltips = [ ( \"Repeated Letter\" , \"@letters\" ), ( \"City Name\" , \"@city_name\" ), ( \"Count\" , \"@lettercount\" ) ]) Finally time for the plot! Now admittedly, I haven't found an easy way to find the limits of the graph, so this was made with a lot of trial and error (If you know a better way, please let me know!). 1 2 3 4 5 6 7 8 9 p = figure ( x_range = ( 20000000 , 17900000 ), y_range = ( - 6000000 , - 4000000 ), x_axis_type = \"mercator\" , y_axis_type = \"mercator\" , tools = [ hover , 'wheel_zoom' , 'save' ]) p . add_tile ( CARTODBPOSITRON ) p . circle ( x = 'longitude' , y = 'latitude' , size = 'sizes' , source = source , line_color = \"#FF0000\" , fill_color = \"#FF0000\" , fill_alpha = 0.05 )","tags":"Python","url":"https://jackmckew.dev/looking-for-patterns-in-city-names-interactive-plotting.html","loc":"https://jackmckew.dev/looking-for-patterns-in-city-names-interactive-plotting.html"},{"title":"PyCon AU 2019 Sunday In Summary","text":"This is a follow on from my last post PyCon AU 2019 Saturday In Summary . Day 2 The real costs of Open Source Sustainability \\@vmbrasseur The key takeaway that I got from this talk, was the typical reaction for problems which are far away from yourself or out of your control is to donate money. Vicky spoke about how sometimes money is not the solution to problems, specifically, for open source projects. Ways you can contribute can be summed up using the term Time, Talent, Treasure. Time: you can donate your time to help a cause, Talent: you can donate your skills and talents, Treasure: you can donate your treasures. Shipping your first Python package and automating future publishing \\@chriswilcox47 Packaging has always been a bit of enigma to me, and Chris Wilcox did an excellent job at explaining not only the structure behind a package, but also how to ship a package. One thing that I have noticed to make sure is to ensure your project structure is in place, and cookie cutter helps with this. Tox & Nox can be used to automate testing of your package over multiple versions and distributions of Python, so you can reassure your users that the package will work no matter the environment. It's dark and my lights aren't working (an asyncio success story) \\@jim_mussared This was one of the talks that really resonated with my previous experience in my thesis project working with the ESP8266. Jim gave a very funny and relatable talk on the experience of using Zigbee communications to link the lights in a new home. The universe as balls and springs: molecular dynamics in Python \\@lilyminium Jupyter notebooks can not only be used for developing, but also as presentations. Lily gave an in-depth talk about the analysis of molecular dynamics, presenting from a jupyter notebook which showed off the power of interactive visualizations making a very complex topic, simple and easy to understand. \"Git hook[ed]\" on images & up your documentation game \\@veronica_hanus As another person that appreciates visual cues to what changes were made in the past, I can definitely see why using Pyppeteer to hook a screenshot onto a git commit can make a massive difference on going back to the commit history and be able to see exactly what changes were made. Sunday Lightning Talks Personally, I really appreciated the talk on https://www.growstuff.org/ , a self proclaimed 'Tinder for Potatos'. Where users can put their plants they are growing & get a progress bar! Then they can interact with other growers, so possibly exchange and grow both the community and the plants. https://youtu.be/q2VmIUaOS9o?t=9 A few of the lightning talks really demonstrated how welcoming the Python & software community is. From Fashion at PyConAU 2019 showing how people can be their true self around a welcoming community to learning what it is like to be Jewish at a conference . To quote \\@UrcherAus , \"And can come out, presenting as female in public for the very first time, and we say to you, welcome to #pyconau , I love your outfit'\". All the great ideas people gave me! Learning that Google has a monolithic \"Monorepo\", where they store all of their projects in one repository to alleviate the problems of maintaining multiple repositories for varying projects that all depend on each other. Finding out that Blender has native support for Python scripting, and produces amazing renders. Very much so looking forward to finding some time to try out Blender and see if I can integrate Python and Blender. Watch this space for a future post on this topic! My to-do list after Pycon Day 2 ~~Write this blog post!~~ (Day 1), Look into tilemill (Day 1), ~~Understand~~ ~~mutable and immutable~~ ~~better (Day 1),~~ ~~Learn what~~ ~~super does in Python~~ ~~(Day 1),~~ Try make some generative art (Day 1), Looking into papermill + jupyterlab (Day 1), Have a go at using XGBoost and text (Day 1), Get (much) better at testing with pytest (Day 1), ~~Look into structuring python projects better~~ (Day 1), Move this website to static html with netlify and more (Day 1), ~~Look into~~ ~~singularity~~ ~~(Day 1),~~ ~~Look into~~ ~~GitFlow Workflow~~ ~~(Day 1),~~ Have a go at using Blender (Day 2), Try make and give a presentation with Jupyter (Day 2), Try making a plot in Plotly (Day 2), Look into Binder for distributing code (Day 2), Listen to Python Bytes podcasts (Day 2). Talks to catch up on \"Extracting tabular data from PDFs with Camelot & Excalibur\" - Vinayak Mehta , \"Using Dash by Plotly for Interactive Visualisation of Crime Data\" - Leo Broska , \"Using Python, Flask and Docker To Teach Web Pentesting\" - Zain Afzal, Carey Li , \"cuDF: RAPIDS GPU-Accelerated Dataframe Library\" - Mark Harris , \"3D Rendering with Python\" - Andrew Williams , Machine Learning and Cyber Security - Detecting malicious URLs in the haystack , Tunnel Snakes Rule! Bringing the many worlds of Python together , \"Goodbye Print Statements, Hello Debugger!\" - Nina Zakharenko , \"Insights into Social Media Data using Entropy Theory\" - Mars Geldard .","tags":"Python","url":"https://jackmckew.dev/pycon-au-2019-sunday-in-summary.html","loc":"https://jackmckew.dev/pycon-au-2019-sunday-in-summary.html"},{"title":"PyCon AU 2019 Saturday In Summary","text":"My first ever conference, learning things I'd never even think of, meeting lots of new people and making my to-do list full of new things to learn. All this happened over the weekend at PyConAU 2019 . This post is dedicated to all the fantastic people I met that gave me new perspectives on python programming and all the amazing talks I had the pleasure of attending. Please note that all the talks written about here were only the ones I was able to attend, there were many other amazing talks that I didn't get the opportunity to go to (will list the follow up ones later in the post) and would recommend to go through the youtube playlist of all the talks found here: https://www.youtube.com/user/PyConAU . A link to all the talks and descriptions can also be found in the headings. To make this post more digestible for the reader (you!), I have broken into parts which are linked here: Day 1 - How to communicate with businesses, metaclasses in python, making generative art, python applications in engineering, refactoring a large scale OSS project and the antipodean approach; Day 1 - Lightning talks; All the great ideas people gave me; My further to-do list following day 1. Day 1 Creating Lasting Change \\@aurynn Day 1 kicked off with a keynote talk from aurynn , who spoke about the lessons learned from talking to your boss. My personal key takeaways were: One word slides directs your focus to the talk rather than distracting, Communicating with people outside your discipline, interest area, etc is made much easier if you put whatever the topic is from their perspective, particularly in the workplace, putting things in terms of risk as this is what matters to businesses. It's Pythons All The Way Down: Python Types & Metaclasses Made Simple \\@judy2k Now classes are admittedly one of my weak points, so what better to do then go straight to metaclasses! My personal key takeaways were: If you run dir() on a type, it'll tell you all the capabilities of that type (eg, dir(int) returns [...,'__add__',...]); Complex numbers and functions have their own type in python; Descriptors override attribute access; There are two types of descriptors (data and non-data) where data descriptors are mutators; Metaclasses can be used as blueprints for generating classes; You can ensure classes are made appropriately with metaclasses. Pretty vector graphics - Playing with SVG in Python Amanda J Hogan By feeding strings of text (which are instructions), you can generate graphics with SVG, mix this with loops and you get generative art! Art being generally a very visual process, watch the video to get a better understanding, personally I liked this one: Python Applications in Infrastructure Planning and Civil Engineering Ben Chu As someone that works for a large engineering firm, this talk resonated with the possibilities of using python to automate jobs and get better results. Firstly, Ben spoke about using Jupyter notebooks to make interactive reports for the environmental teams to utilise for their analysis and using papermill to export these into different formats (excel, pdf, etc). Ben also used python to automate the verification stages of a proposed rail corridor location. By using requests & beautiful soup to scrape the NSW development application website along with a machine learning classification algorithm ( XGBoost ) for the developments impact on the rail corridor. Finally plotting this on an interactive map for the rail designers to use. How I auto-refactored a huge OSS project to use pytest \\@craigds2 Craig gave a great talk about how he used PyBowler, Pytest and importantly pytest-sugar to automatically refactor existing testing framework for a huge open source project GDAL . This post has inspired me to do more testing & refactoring on my code as I develop things! The Antipodes \\@brandon_rhodes Personally this talk really resonated with a habit that I have been trying to employ in my life recently. The basic principle being behind the meaning of the idea of antipodean s , someone standing on the exact other side of the planet from you. I thought this was an amazing segway for moving to a new framework for structuring communications. As most of us do, whenever replying to communication from someone, we normally start with me, me, me, me, you. For example, if someone asked us to make a decision, typically & personally, I would start the reply with stating why I had got to the decision, finally ending the message with the decision and the next steps. A technique that I have recently started employing on my messaging is writing as I normally would and before hitting send, moving what matters most to the reader (the decision) to the top, followed by next steps and then going through all the reasons why I possibly had made that decision. Saturday Lightning Talks Personally, I thought the most interesting lightning talk was about procedurally generating planets, modelling them in 3D and then trying to estimate if the climate on them https://youtu.be/AJqcxEzRdSY?t=140 . What I think was the crowd favourite, was the History and Politics of Australian supermarkets and their mergers . Definitely worth the watch. All the great ideas people gave me! Shoutout to \\@davidjb_ who made me aware of modern static html sites with using Pelican and Netlify . By using a repository to store all the content, you can use these tools in combination to make an automated workflow for a CMS (Content management system) for deploying a website. Will definitely be looking into this for this very website! Shoutout to a guy (who doesn't have socials) that I met a pub that made me aware of Singularity , an alternative to docker, will have to do further research and testing on this one, so watch this space! Further to this, I was also made aware of the Gitflow Workflow , as using Git is admittedly one of my other weakpoints, so will definitely be trying to bring this principle into my development pipeline. My to-do list after Pycon Day 1 Write this blog post! Look into tilemill , Understand mutable and immutable better, Learn what super does in Python , Try make some generative art, Looking into papermill + jupyterlab , Have a go at using XGBoost and text, Get (much) better at testing with pytest , Look into structuring python projects better, Move this website to static html with netlify and more, Look into singularity , Look into GitFlow Workflow .","tags":"Python","url":"https://jackmckew.dev/pycon-au-2019-saturday-in-summary.html","loc":"https://jackmckew.dev/pycon-au-2019-saturday-in-summary.html"},{"title":"Hands On Machine Learning Chapter 2","text":"Chapter 2 is an end to end machine learning project, in which you pretend to be a recently hired data scientist for a real estate company. It cannot be emphasized enough that when learning about machine learning or any topic for that matter, it is best to actually experiment with real-world data and scenarios. Firstly my personal opinion on how a machine learning (or data science) project is structured is a series of steps: Get an understanding of the expected goal or outcome (eg frame the problem), Get an understanding of the current process (if there is one), Get the data behind the problem (or what you expect will be useful for solving the problem), Explore and visualize the data to gain insights, Prepare/massage the data ready for input into algorithms or models, Select a model/algorithm and train it, Tune your model/algorithm to the best you can, Present the solution to the original stakeholder (take the stakeholder on a journey), Launch, monitor and maintain your system. I believe, that if you follow these steps at a minimum, you will find success with your data science/machine learning projects. This methodology also applies for any type of project and can be enhanced with tweaks where you see fit. Anyway, back to Chapter 2, it is very much so reinforced that you should select an appropriate way of scoring performance of your algorithms/models (otherwise you can't compare them effectively). For regression tasks, generally the preferred performance measure is RMSE (Root Mean Square Error), but this may not always be the case depending on the context of the problem. For example, if the data set has many outliers (or outlier groups), it may prove beneficial to consider MAE (Mean Absolute Error). Assumptions are in my opinion, the downfall of any collaborative project if they are not transparent or communicated. A practice that I personally do and recommend doing is to try your best to document every assumption you may make in a project, such that anyone later on can pick up where you were and understand why you chose to do something a certain way. As per Chapter 1, it is again reinforced to split your data set up into a training set, a testing set and a validation set; albeit a more practical example of this concept in action. Whereas you use the K-fold validations with GridSearchCV to understand the best performing hyper parameters for your algorithm/model. Personally, in Chapter 2, the most difficult part to understand is around the pipeline for preparing data ready for use in algorithms/models. Pipelines are essentially a sequence of steps that need to be completed in order before the data is ready. Stemming from the Scikit-learn design principles, I found this the best way to understand the possible steps in a data preparation pipeline: Estimators Any object that estimates parameters based on a data set is known as an Estimator. For example, if you had a data set with lots of missing values, you could estimate what to fill these gaps with an imputer, then you could choose to use the median of the dataset if appropriate. Transformers Any object that transformers a data set is known as a Transformer. For example, if you wanted to now fill those gaps in the data set previously mentioned with the mean, you would use a transformer to 'insert' the median wherever empty values were found. Predictors Any object that is capable of making predictions given a dataset is known as a Predictor. For example, a linear regression model is a predictor, using one feature to extrapolate another feature.","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-2.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-2.html"},{"title":"Python and OCR","text":"This post will demonstrate how to extract the text out of a photo, whether it being handwritten, typed or just a photo of text in the world using Python and OCR (Optical Character Recognition). While this is something that humans do particularly well at distinguishing letters, it is a form of semi-structured data. OCR just like humans also has it's limitations, for example, if you were trying to read someone with really difficult handwriting, it could be a big challenge. In this post, we will use the Tesseract engine (an open source Google project) to undertake the OCR process for us. First of all, as always, we must create a new virtual environment for our project to live in or use a package manager such as Anaconda (as explained in my post Episode - 8: Anaconda . Once initialized, we want to install a few packages to help us on our quest for OCR. Both Pillow and PyTesseract , if you are using Anaconda like I did, you will want to specifically use pip, not conda, to install these packages. Further to this, you will need to install the binary of the Tesseract-OCR engine, which installation instructions can be found: https://github.com/UB-Mannheim/tesseract/wiki . Now we are finally ready to test the engine and see if we can extract text out of an image, first of all we will start with a 'well' written example, the 'logo' of this website! Of course, we have still yet to write any code, so naturally, that is the next step. As always in a python project, you will need to import all the dependencies of the project, in this case, it will be Image from the PIL (pillow) package, and pytesseract (the python wrapper around the Tesseract Engine). 1 2 from PIL import Image import pytesseract Now that we have our dependencies loaded, it's time to check out the documentation behind Pillow and pytesseract to know how to operate the tools, consider these an instruction manual. The documentation for these tools can be found: Pillow , PyTesseract . Luckily for us, the developers have made this so simple it could be a one liner: 1 print ( pytesseract . image_to_string ( Image . open ( 'images/example.png' ))) Which outputs in the console from the example image above: JACK MCKEW'S\\ BLOG\\ Python enthusiast, electrical engineer and\\ tinkerer Great! We can confirm that the text that the tesseract engine detected, is in fact, exactly what the example we gave it was. However, let's go a bit out of the way to make this a function such that it can be called more easily with the filepath to the image as a string. 1 2 3 4 5 6 7 8 9 10 from PIL import Image import pytesseract def ocr_convert_to_text ( filename ): text = pytesseract . image_to_string ( Image . open ( filename )) return text extracted_text = ocr_convert_to_text ( 'images/example.png' ) print ( extracted_text ) Now we have a function that we can call with a file path to easily convert our images to text. Now let's give the tesseract engine a bit of challenge with a full page of handwritten text: Ad Bb Cc Da Fe FEF Ge Hh Ii IS RR lt Hm We\\ 00 PP Ag Rr SsT# Uu Vv Ww Xx 44 Le\\ Aa BS (36 72 Re bebe nme #% Ua ti ke\\ At Au Hee Bo In Fn Le Sim \\ \\(y Rep Ha Wy\\ Ye Unu Uppy bb otn tx 79 Ww 2A\\ Sr be Liki 4\\ IR AS67890\\ so cool! New \\|neerndtinas release of mast\\ famous APP for exhorting Printed text 40\\ handwritten \"Sinyak\\ PacK mY box with. five dozen uguor Jugs\\ Don't 62 @. Earn \\\\) & Put 12 Jar.\\ Ingredienes: Zuss, Chis, CAR Lid.\\ (Screz info), kndw tb. 3\\ Wo xr dA h-(H4+F 060 Cheah]\\ ChiP & Dae.\" fava ys ie m4 mind.\\ Jackdaws uve m4 bi? sPhinx Of quare 2.\\ The five boxing withrds JumP quick.\\ How vexiegi quick date 2ebras sump!\\ {0.0} ainsa & crepim pa. bau! -) Using the same code, we were able to determine most of the text out of the picture that the tesseract engine was given. Obviously this is not perfect, but it is a whole lot easier than typing it all in by hand. For a bit of another challenge and to demonstrate the capabilities, let's try some Australian number plates: (CSE) XcB-962 (66M-059\\ X2ZH:709) EEH:133) (GAA729) Obviously this can and has had a big impact on the way people can utilize images to make their life easier, from scanning in your handwritten notes at school and converting straight on to the computer, to being able to add all the contact information in your phone from a business card. How can OCR help your life at work or at home? Please let me know in the comments.. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Python","url":"https://jackmckew.dev/python-and-ocr.html","loc":"https://jackmckew.dev/python-and-ocr.html"},{"title":"Python and Data Security (Hashing Algorithms)","text":"Data security is becoming more and more prevalent in today's society than ever before. We must make a conscious effort to secure both our physical lives, but also our digital lives as well. With data privacy, sharing of information and access control becoming integrated into most people's life in some way or another. Since this topic is so wide and deep, this will most likely become a series of posts as I am passionate around data security and enjoy getting stuck right into the math behind it. This post will be around hashing algorithms but future topics will include: Hashing Algorithms (this post), Modular Arithmetic and why it's used, Securely sharing keys, Methods of encryption, Methods of data security, Analysing security weaknesses, Many more. As above, this post is dedicated to hashing algorithms and how to interface with them with Python for data security. What is a Hashing Algorithm? The sole purpose of a hashing algorithm is to generate a safe hash which in turn raises the questions of what is a hash and what makes it safe? A hash is a value computed from a base input number using a hashing function. With a hashing function being: A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. https://en.wikipedia.org/wiki/Hash_function The hashing algorithm is intrinsically designed to be a one-way function, meaning it is impractical to revert. Although, as history has shown, as computing advances are made hashing algorithms are becoming compromised. A prime example of this being the MD5 algorithm, which was designed and used a cryptographic hash function (data security), but is now so simply reverse, that it is used for verifying data transfers. There are certain characteristics around what the perfect or ideal hash function for data security should possess: Easy/speed of computation, Impossible/impractical to regenerate source data/message (brute force as only option), Unique hashes for data (also known as hash collisions when there are duplicate hashes), Any change is source data should change the hash value (known as the avalanche effect). What is hashing used for in practice? Hashing algorithms for data security in the real world is used in a variety of situations from ensuring files were successfully delivered correctly or to store sensitive/private information. If you are reading this, I can almost guarantee that you have some interface with a hashing algorithm right now! Whether it be how you're password is stored to indexing data in a database. Using hashes with Python This will be a simple use-case of a hashing algorithm using Python to securely convert passwords and how to verify against them (storing the hashed data is it's own beast in itself). Please note I will be utilising the passlib package which contains over 30 password hashing algorithms, as well as a framework for managing existing password hashes. First of all we must select a hashing algorithm to use, to help with this from the team at passlib they have provided a basic guideline of questions : Does the hash need to be natively supported by your operating system's crypt() api,\\ in order to allow inter-operation with third-party applications on the host? If yes, the right choice is either bcrypt for BSD variants,\\ or sha512_crypt for Linux; since these are natively supported. If no, continue... Does your hosting provider allow you to install C extensions? If no, you probably want to use pbkdf2_sha256 ,\\ as this currently has the fastest pure-python backend. If they allow C extensions, continue... Do you want to use the latest & greatest, and don't mind increased memory usage\\ when hashing? argon2 is a next-generation hashing algorithm,\\ attempting to become the new standard. It's design has been being slightly tweaked\\ since 2013, but will quite likely become the standard in the next few years.\\ You'll need to install the argon2_cffi \\ support library. If you want something secure, but more battle tested, continue... The top choices left are bcrypt and pbkdf2_sha256 .\\ Both have advantages, and their respective rough edges;\\ though currently the balance is in favor of bcrypt\\ (pbkdf2 can be cracked somewhat more efficiently). If choosing bcrypt, we strongly recommend installing the bcrypt \\ support library on non-BSD operating systems. If choosing pbkdf2, especially on python2 \\< 2.7.8 and python 3 \\< 3.4,\\ you will probably want to install fastpbk2 \\ support library. From this, we will use the argon2 hashing algorithm. As normal, it is best practice to set up a virtual environment (or conda environment) and install the dependencies, in this case passlib. First of all, import the hashing algorithm you wish to use from the passlib package: 1 from passlib.hash import argon2 Following importing the hashing algorithm, to hash the password in our case is very simple and we can have a peak at what the output hash looks like: 1 2 3 hash = argon2 . hash ( \"super_secret_password\" ) print ( hash ) \\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) NqY05lyrtdb6v/ee03pvrQ\\$mvLTquN71JPjuC+S9QNXYA The first section (\"\\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) \") is the header information, showing the parameters that the algorithm used to generate the hash. While this seems as if it would make the algorithm easier to break, imagine a scenario where every password is hashed using an hashing algorithm with randomised parameters; verifying passwords would be a nightmare. Let's further break down what this represents: \\$argon2i - the variant of Argon2 algorithm being used, \\$v=19 - the version of Argon2 being used, \\$m=102400,t=2,p=8 - the memory (m), iterations (t) and parallelism (p) parameters being used, \\$NqY05lyrtdb6v/ee03pvrQ - the base64-encoded salt (added randomness), using standard base64 encoding and no padding, \\$mvLTquN71JPjuC+S9QNXYA - the base64-encoded hashed password (derived key), using standard base64 encoding and no padding. If we run this again, we can check that the outputs are completely different due to the randomly generated salt. 1 2 3 hash = argon2 . hash ( \"super_secret_password\" ) print ( hash ) \\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) 8f4/x7hXitGacy6F8N67dw\\$/jPKQ98vLQCxkboxRlHa/g Now that we've generated our new passwords, stored them away in a secure database somewhere, using a secure method of communication somehow, our user wants to login with the password they signed up with (\"super_secret_password\") and we have to check if this is the correct password. To do this with passlib, it is as simply as calling the .verify function with the plaintext and the equivalent hash which will return a boolean value determining whether of not the password is correct or not. 1 print ( argon2 . verify ( \"super_secret_password\" , hash )) True Hooray! Our password verification system works, now we would like to check that if the user inputs a incorrect password that our algorithm returns correctly (false). 1 print ( argon2 . verify ( \"user_name\" , hash )) False Conclusion Hopefully this has given you some insight into what hashing algorithms are, how they are used and how to use them with Python. They can both be an extremely powerful tool for securing data, however, must always be revisited later on down the track as advancements are made and your system may now be compromised. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Python","url":"https://jackmckew.dev/python-and-data-security-hashing-algorithms.html","loc":"https://jackmckew.dev/python-and-data-security-hashing-algorithms.html"},{"title":"Hands On Machine Learning Chapter 1","text":"I've recently been making my way through the book \"Hands-On Machine Learning with Scikit-Learn and Tensorflow\", and thought I will put a summary of the chapter as a post, along with my personal answers to each of the chapter's exercises. The book in particular is published by O'Reilly and can be found https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ . Chapter 1 is around defining when and where to apply machine learning to a problem, as it is not always the best approach to solving a problem. Following, making sure to be aware of the strengths and weaknesses of each 'type' of machine learning systems. Types of machine learning systems can be broken into three broad categories: Is the model trained with human supervision? (supervised, unsupervised, semisupervised and reinforcement learning) Does the model learn incrementally on the fly or not? (online or batch learning) Does the model work by simply comparing new vs known data or detect patterns to build a prediction? (instance based or model based learning) The book then goes into detail around these, I will not as many resources around these topics are abundantly available on the internet. Chapter 1 also goes onto to detail the importance of defining the problem, 'clean' data, training vs testing data and comparing different techniques. From here on are the chapter 1 exercise questions, with my personal answer, and the book's answer. 1. How would you define Machine Learning? My answer: Self-sufficiently improving on a technique.\\ Book's answer: Machine Learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure. 2. Can you name four types of problems where it shines? My answer: Processes which either involve: many complex steps, steps that require 'tuning', ever-changing systems based on variables or to gain insight on a problem from a new perspective.\\ Book's answer: Machine Learning is great for complex problems for which we have no algorithmic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. What is a labeled training set? My answer: A data set with the associated desired answer.\\ Book's answer: A labeled training set is a training set that contains the desired solution (a.k.a. alabel) for each instance. 4. What are the two most common supervised tasks? My answer: Regression and classification.\\ Book's answer: The two most common supervised tasks are regression and classification. 5. Can you name four common unsupervised tasks? My answer: Association rule learning, anomaly detection, simplification and clustering.\\ Book's answer: Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning. 6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? My answer; Reinforcement learning.\\ Book's answer: Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to express the problem as a supervised or semisupervised learning problem, but it would be less natural. 7. What type of algorithm would you use to segment your customers into multiple groups? My answer: k-neighbour clustering.\\ Book's answer: If you don't know how to define the groups, then you can use a clustering algorithm (unsupervised learning) to segment your customers into clusters of similar customers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups . 8. Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem? My answer: Supervised or semisupervised.\\ Book's answer: Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam). 9. What is an online learning system My answer: A system that learns incrementally on the fly from new data.\\ Book's answer: An online learning system can learn incrementally, as opposed to a batch learning system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data. 10. What is out-of-core learning? My answer: Whenever the data set is too large to fit on a single machine.\\ Book's answer: Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these minibatches. 11. What type of learning algorithm relies on a similarity measure to make predictions? My answer: Instance based learning (comparison of new vs old).\\ Book's answer: An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned instances and uses them to make predictions. 12. What is the difference between a model parameter and a learning algorithm's hyperparameter? My answer: A model parameter directly influences and influenced by the way the model behaves, while a hyperparameter is dictates how the model should behave (eg, learn fast or slow).\\ Book's answer: A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). 13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions? My answer: Relationships or trends within the data. Regression is used to find a possible solution to fit to the data and predictions are then extrapolated.\\ Book's answer: Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the system is at making predictions on the training data, plus a penalty for model complexity if the model is regularized. To make predictions, we feed the new instance's features into the model's prediction function, using the parameter values found by the learning algorithm. 14. Can you name four of the main challenges in Machine Learning? My answer: Quality, quantity, irrelevant sections and incorrectly modeled.\\ Book's answer: Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple models that underfit the training data, and excessively complex models that overfit the data 15. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions? My answer: Overfitted or underfitted to the data. Simplify the model, get more useful data and/or reduce noise.\\ Book's answer: If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. What is a test set and why would you want to use it? My answer: A test set is used to understand how your model interacts with unseen data without having to collect new information.\\ Book's answer: A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 17. What is the purpose of a validation set? My answer: To understand how accurate the system interfaces with unseen data.\\ Book's answer: A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters. 18. What can go wrong if you tune hyperparameters using the test set? My answer: The system has been specifically setup to perform under these conditions and may perform unexpectedly in new situations.\\ Book's answer: If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect). 19. What is cross-validation and why would you prefer it to a validation set? My answer: By dividing the training set further into categories, then trained and validated against combinations of other categories.\\ Book's answer: Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate vali dation set. This saves precious training data.","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-1.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-1.html"},{"title":"Parallel Processing in Python","text":"Parallel processing is a mode of operation where the task is executed simultaneously in multiple processors in the same computer. The purpose of this is intended to reduce the overall processing time, however, there is often overhead between communicating processes. For small tasks, the overhead is detrimental to the length of processing, increasing the overall time taken. For this post we will be using the multiprocessing package in Python. Multiprocessing is apart of the standard library within Python and is a package that supports spawning processes using an API similar to the threading module (also apart of the standard library). The main benefit of the multiprocessing package, is that it disregards the global interpreter lock (GIL), by using sub processes instead of threads. The number of processors or threads in your computer dictates the maximum number of processes you can run at a time. To add flexibility to your program when it may be run across multiple machines, it is good practice to make use of the cpu_count() function apart of the multiprocessing, as shown below (please note f strings were only introduced in Python 3.6). 1 2 import multiprocessing as mp print ( f \"Maximum number of processes: {mp.cpu_count()}\" ) In parallel processing, there are two types of execution: Synchronous and Asynchronous. Synchronous meaning where the processes are completed in the same order in which it was started, such that, the output is (normally) in order. While asynchronous means the processes can be in any order, and while the output can be mixed, is usually computed faster. Within multiprocessing there are 2 main classes that you will use for parallel processing: Pool & Process. The two classes are intended to be used in completely different scenarios, but still utilize parallel processing. Pool is beneficial for when you have a long list that need to be processed and combined back together at the end. Process is beneficial for when you need multiple functions running simultaneously, albeit not the same. The Pool Class The pool class has four methods that are particular useful: Pool.apply Pool.map Pool.apply_async Pool.map_async Before we tackle the asynchronous variants of the pool methods (async suffix). Here is a simple example using Pool.apply and Pool.map. We initialize the number of processes to however many is available or the maximum of the system. 1 2 3 4 5 6 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) results = [ pool . apply ( power_n_minus_1 , args = ( x ,)) for x in range ( 1 , 5 )] print ( results ) With the results being: [1, 2, 9, 64] or 1\\&#94;0, 2\\&#94;1,3\\&#94;2,4\\&#94;3. This can also be achieved similarly with Pool.map. 1 2 3 4 5 6 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) results = pool . map ( power_n_minus_1 , range ( 1 , 5 )) print ( results ) Both of these will lock the main program that is calling them until all processes in the pool are finished, use this if you want to obtain results in a particular order. However if you don't care about the order and want to retrieve results as soon as they finished, then use the async variant. 1 2 3 4 5 6 7 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) outputs = [ pool . apply_async ( power_n_minus_1 , args = ( x ,)) for x in range ( 1 , 5 )] results = [ p . get () for p in outputs ] print ( results ) The Process Class The process class is the most basic approach to parallel processing from multiprocessing package. Here we will use a simple queue function to generate 10 random numbers in parallel. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import multiprocessing as mp import random output = mp . Queue () def rand_number ( lower_limit , upper_limit , output ): output . put ( random . randint ( lower_limit , upper_limit )) if __name__ == \"__main__\" : processes = [ mp . Process ( target = rand_number , args = ( 1 , 101 , output )) for x in range ( 10 )] for p in processes : p . start () for p in processes : p . join () results = [ output . get () for p in processes ] print ( results ) With the result being: [76, 40, 76, 27, 64, 94, 30, 71, 70, 40]. By utilizing the multiprocessing package in Python or parallel computing concepts in general, you will now be able to dramatically increase computation times (for large processes).","tags":"Python","url":"https://jackmckew.dev/parallel-processing-in-python.html","loc":"https://jackmckew.dev/parallel-processing-in-python.html"},{"title":"Distributing Python Code","text":"This post will cover a way of distributing Python code such that is can be used by someone that does not have Python installed. One of the major drawbacks with Python that the gap is slowly being closed is how easy it is to distribute Python code. At a minimum, the computer that is to run the code must have the Python compiler (or equivalent). Now while this has been progressively included in more operating systems as a default (May update of Windows being the latest), you must still develop as such that is not present on the users' PC. For this post, I will show you a basic piece of code to demonstrate how it will be packaged and distributed to your users. To show a basic dialog box on the screen with the following code: 1 2 3 import ctypes ctypes . windll . user32 . MessageBoxW ( 0 , \"Hello Windows!\" , \"PyInstaller Example\" , 1 ) Which shows the user with this dialog box: Now to package this code into an executable (.exe), there are multiple packages out there that are possible to use, some examples of these are: cx_freeze py2exe PyInstaller For this post, I will use PyInstaller as it is what I am most familiar with, please get in touch with me if you believe any other package is better suited. I have created an environment in anaconda named \"pyinstall\", in which I have installed PyInstaller with the command \"conda install -c conda-forge pyinstaller\", which includes Python 3.7.3 due to anaconda's packaging system (thereby including ctypes from the standard library). Now to use the PyInstaller package, just open Anaconda Prompt (or cmd if anaconda.exe is in your PATH). Navigate to where the python code is stored, and run the command \"pyinstaller \\<name_of_program>.py. See below for an example: This will create a build & dist folder within the directory you navigated to, which contains the python application and all the required files will be put inside the dist folder which will be shipped to the user later on. There are many other settings that you can use to customize how your package gets built and more, but I won't go into that in this post. Now if we go into the dist folder and find the .exe (which will have the same name as your python file unless you change this setting). Once you hit run, you'll be met by this screen: Now you can send this executable to anyone (although most antivirus will stop you) and it will run on their PC!","tags":"Python","url":"https://jackmckew.dev/distributing-python-code.html","loc":"https://jackmckew.dev/distributing-python-code.html"},{"title":"Python Decorators Explained","text":"Python decorators are one of the most difficult concepts in Python to grasp, and subsequently a lot of beginners struggle. However they help to shorten code and make it more 'Pythonic'. This post is going to go through some basic examples where decorators can shorten your code. Firstly you have to understand functions within python: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def hello ( name = 'Jack' ): return \"Hello \" + name print ( hello ()) # output: 'Hello Jack' greeting = hello # assign a function to a variable, with no parentheses as we are not calling it print ( greeting ()) # output: 'Hello Jack' del hello print ( hello ()) # output: NameError print ( greeting ()) # output: 'Hello Jack' As we can see above we can give functions default arguments (the string 'Jack' for the name variable in hello). Assign functions to variables (ensuring the parentheses are not included otherwise we would be assigning to the returning value from the function. Remove previous functions now that we have 'copied' the function over. Now to take the next step into functions within Python, by defining functions within functions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def hello ( name = 'Jack' ): print ( \"You're now inside the hello() function\" ) def greeting (): return \"Now you are in the greeting() function\" def welcome (): return \"Now you are in the welcome() function\" print ( greet ()) print ( welcome ()) print ( \"You are now back in the hello() function\" hello () # outputs: \"You're now inside the hello() function\" # \"Now you are in the greeting() function\" # \"Now you are in the welcome() function\" # \"You are now back in the hello() function\" welcome () # output: NameError: name 'welcome' is not defined Now we can make nested functions (functions within functions), the next step is, functions returning functions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def hello ( name = 'Jack' ): def greeting (): return \"Now you are in the greeting() function\" def welcome (): return \"Now you are in the welcome() function\" if ( name == 'Jack' ): return greeting else : return welcome returned_function = hello () print ( returned_function ) # output: <function greeting at 0x7f2143c01500> # This clearly shows that the returned function is the greeting() function within the hello() function print ( returned_function ()): # output: \"Now you are in the greeting() function\" From earlier, we know that if we don't include the parentheses then the function does not executed. Another extension of the way this is formatted is that we can now call hello()() which outputs \"Now you are in the greeting() function\". 1 2 3 4 5 6 7 8 9 10 def hello ( name = \"Jack\" ): return \"Hello \" + name def preFunction ( function ): print ( \"This is the prefunction function\" ) print ( hello ()) preFunction ( hello ) # output: \"This is the prefunction function\" # \"Hello Jack\" Now you have all the knowledge to learn what decorators really are, they let you execute code before and after a function. The code above is actually a decorator, but let's make it more usable. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def new_decorator ( function ): def functionWrapped (): print ( \"This is the pre function\" ) function () print ( \"This is the post function\" ) return functionWrapped def function_requiring_decoration (): print ( \"I need some decorations!\" ) function_requiring_decoration () # output: \"I need some decorations\" function_requiring_decoration = new_decorator ( function_requiring_decoration ) # Now our function is wrapped by functionWrapped() function_requiring_decoration () # output: \"This is the pre function\" # \"I need some decoration\" # \"This is the post function\" Now you've made a decorator! We've just used what we learned previously to modify it's behaviour in one way or another. Now to make it even more concise we can just the @ symbol. Here is how we could have used the previous code with @ symbol. 1 2 3 4 5 6 7 8 9 10 11 @new_decorator def function_requiring_decoration (): print ( \"I need some decorations!\" ) function_requiring_decoration () # output: \"This is the pre function\" # \"I need some decoration\" # \"This is the post function\" # The @ operator is a short way of saying: function_requiring_decoration = new_decorator ( function_requiring_decoration ) Hopefully now you are ready to go and explore the world of -decorators within Python, they can be used quite powerfully and allow for you to reuse code and extend capabilities. Some of the best examples for decorators are for authentication or logging, however I will not cover them as they are extensively documented over the internet.","tags":"Python","url":"https://jackmckew.dev/python-decorators-explained.html","loc":"https://jackmckew.dev/python-decorators-explained.html"},{"title":"Explained: Voltage Drop","text":"Voltage drop is a electrical phenomenon in that wires carrying current always have resistance, or impedance to the current flow. Voltage drop is defined as the amount of loss that occurs through part of or all of a circuit due to resistance/impedance. The most well known analogy for explaining voltage, current and voltage drop is a hose carrying water. In the garden hose, the water pressure is the voltage, the amount of water flowing is the current and the type and size of the hose makes up the resistance. Thus meaning that voltage drop is the loss of water pressure from the supply end of the hose to the output. When designing electrical systems within Australia and New Zealand, we are required to design to Australian standards. For voltage drop, the relevant standards as AS/NZS3000 (Wiring Rules) and AS/NZS3008 (Cable Selection). Where AS/NZS3000 nominates the limits to conform to (5% maximum from point of supply) and AS3008 dictates multiple ways that voltage drop can be calculated. For this post, I will demonstrate a simplified method that is outlined in AS3000 Table C7 where it specifies 'Am per %Vd' (Amp meters per % voltage drop) for each cable size: Cable Conductor Size Single Phase (230V) Am per %Vd Three Phase (400V) Am per %Vd 1mm&#94;2&#94; 45 90 1.5mm&#94;2&#94; 70 140 2.5mm&#94;2&#94; 128 256 4mm&#94;2&#94; 205 412 6mm&#94;2&#94; 306 615 10mm&#94;2&#94; 515 1034 16mm&#94;2&#94; 818 1643 25mm&#94;2&#94; 1289 2588 35mm&#94;2&#94; 1773 3560 50mm&#94;2&#94; 2377 4772 70mm&#94;2&#94; 3342 6712 95mm&#94;2&#94; 4445 8927 For example, a 50m run of 10mm\\&#94;2&#94; cable carrying 3 phase 32A will result in 5% drop: 32A * 50m = 1600 / 1034 = 1.5%. In future posts, I will go into the various ways that AS/NZS3008 demonstrates ways of calculating voltage drop.","tags":"Engineering","url":"https://jackmckew.dev/explained-voltage-drop.html","loc":"https://jackmckew.dev/explained-voltage-drop.html"},{"title":"What is MongoDB?","text":"Recently after looking for a different flavour of database apart from MySQL (which is what I am personally use to), I had always heard about MongoDB. So after some investigation, I found that MongoDB has a platform MongoDB University to familiarize yourself with their product. I completed their very first introductory course M001: MongoDB Basics last week, I found it very gentle in the introduction to database management and exploring data sets. This post is dedicated my take on the course and the key takeaways from my point of view. The course is broken into multiple chapters in which a chapter is released each week for the duration of the course. For example, the basics course was broken up into: Intro to MongoDB, Mongo Compass and Basic Queries, Create, Read, Update and Delete (CRUD) operations and more, MongoDB queries. Following all the chapters, you are faced with a final exam which tests if you were participating/listening in the earlier chapters. If you are concerned that you may struggle, this final exam is made up of a few multiple choice questions based on querying the data sets used in the chapters. MongoDB is a open source document-oriented database program, classified as a NoSQL database and utilises JSON-like documents with a schema. They also provide a tool to help sift through the database called 'Compass'. Personally, I really enjoy the functionality within Compass with plotting geographical data, presenting data type variances across the fields in a document and many other features. I found Compass one of the most appealing features as someone that constantly seeks to gain insight from data. Queries within MongoDB are structured like a dictionary in Python, where the field in the document is passed the key and the criteria is the value. For example, a basic query to return all documents within a MongoDB database with score equal to 7 would be: 1 { score : 7 } As a mainly Python developer, I found this to be very appealing as I find myself using dictionaries constantly when writing Python code, and by MongoDB using this format makes for an easy connection between the two. CRUD operations, are the fundamentals on actually using a database usefully. Through the Mongo shell you are able to add documents to the MongoDB database through JSON, XML, etc data formats. Projections within MongoDB are used to specify or restrict the fields to return with the filtered documents if you are specifically looking at a few fields within a densely populated document. In addition to the way queries are structured for filtering documents, it is also possible to use one of the many query or projection operators to further filter the documents. For example a query to return all documents with a score greater than 7 would be: 1 { score : { $ gte : 7 }} This sums up all of the takeaways from the M001 course for MongoDB that I found. I look forward to taking more of the courses on MongoDB university to gain a greater understanding and be able to utilise MongoDB across some of my projects.","tags":"Software Development","url":"https://jackmckew.dev/what-is-mongodb.html","loc":"https://jackmckew.dev/what-is-mongodb.html"},{"title":"Efficient Frontier for Balancing Portfolios","text":"Following last 2 weeks' posts ( Python for the Finance Industry & Portfolio Balancing with Historical Stock Data ), we now know how to extract historical records on stock information from the ASX through an API, present it in a graph using matplotlib, and how to balance a portfolio using randomly generated portfolios. This post is to demonstrate a method in balancing portfolios that does not depend on generating random portfolios, but rather mathematically determining the extremities of boundaries for effective portfolios using the SciPy optimize function (similar to that of Excel's 'solver' ). Returning to last weeks' post when the budget allocations to assets were determined from randomly generated portfolios, it was presented on the graph below: From this plot, it can be visualized that it forms an arch line between the yellow and red crosses. This line is called the efficient frontier . The efficient frontier represents the set of optimal portfolios that offer the highest expected return for a defined level of risk or the lowest risk for a given level of expected return. Simply this means, all the dots (portfolios) to the right of the line will give you a higher risk for the same returns. First of all we must mathematically determine the portfolio with the maximum Sharpe ratio as the greater a portfolio's Sharpe ratio, the better it's risk-adjusted performance. Sharpe ratio is calculated using the formula below: To find the maximum of the Sharpe Ratio programmatically we follow these steps: Firstly, define the formula as the function neg_sharpe_ratio (take note that to find the maximum of function in SciPy , we use the minimize function with an inverse sign), In the max_sharpe_ratio function, define arguments to be passed into the SciPy minimize function: neg_sharpe_ratio: function to be minimized, num*[1/num_assets]: initial guess which is evenly distributed array of values, Arguments that are to be passed into the objective function (neg_sharpe_ratio), Method of Sequential Lease Squares Programming, there are many others which can be seen here, Bounds: between 0% and 100% of our budget allocation, Constraints: given as a dictionary, 'eq' type for equality and 'fun' for the anonymous function which limits the total summed asset allocation to 100% of the budget. The result from the minimize function is returned as a OptimizeResult type. 1 2 3 4 5 6 7 8 9 10 11 12 def neg_sharpe_ratio ( weights , average_returns , covariance_matrix , risk_free_rate ): returns , volatility = portfolio_performance ( weights , average_returns , covariance_matrix ) return - ( returns - risk_free_rate ) / volatility def max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix , risk_free_rate ) constraints = ({ 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bound = ( 0 , 1 ) bounds = tuple ( bound for asset in range ( num_assets )) result = sco . minimize ( neg_sharpe_ratio , num_assets * [ 1 / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result Similarly to the maximum sharpe ratio we do the same for determining the minimum volatility portfolio programmatically. We minimise volatility by trying different weightings on our asset allocations to find the minima. 1 2 3 4 5 6 7 8 9 10 11 12 13 def portfolio_volatility ( weights , average_returns , covariance_matrix ): return portfolio_performance ( weights , average_returns , covariance_matrix )[ 1 ] def min_variance ( average_returns , covariance_matrix ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix ) constraints = ({ 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bound = ( 0.0 , 1.0 ) bounds = tuple ( bound for asset in range ( num_assets )) result = sco . minimize ( portfolio_volatility , num_assets * [ 1. / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result As above, we can also draw a line which depicts the efficient frontier for the portfolios for a given risk rate. Below some functions are defined for computing the efficient frontier. The first function, efficient_return is calculating the most efficient portfolio for a given target return, and the second function efficient frontier is compiling the most efficient portfolio for a range of targets. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def efficient_return ( average_returns , covariance_matrix , target ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix ) def portfolio_return ( weights ): return portfolio_performance ( weights , average_returns , covariance_matrix )[ 0 ] constraints = ({ 'type' : 'eq' , 'fun' : lambda x : portfolio_return ( x ) - target }, { 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bounds = tuple (( 0 , 1 ) for asset in range ( num_assets )) result = sco . minimize ( portfolio_volatility , num_assets * [ 1. / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result def efficient_frontier ( average_returns , covariance_matrix , returns_range ): efficients = [] for ret in returns_range : efficients . append ( efficient_return ( average_returns , covariance_matrix , ret )) return efficients Now it's time to plot the efficient frontier on the graph with the randomly selected portfolios to check if they have been calculated correctly. It is also an opportune time to check if the maximum Sharpe ratio and minimum volatility portfolios have been calculated correctly by comparing them to the previously randomly determined portfolios. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def display_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ): results , weights = generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ) max_sharpe = max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ) max_sharpe_return , max_sharpe_volatility = portfolio_performance ( max_sharpe [ 'x' ], average_returns , covariance_matrix ) max_sharpe_allocations = allocations_ef ( max_sharpe . x , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f}\" . format ( max_sharpe_return )) print ( \"Volatility: {0:.2f}\" . format ( max_sharpe_volatility )) print ( max_sharpe_allocations ) min_vol = min_variance ( average_returns , covariance_matrix ) min_vol_return , min_vol_volatility = portfolio_performance ( min_vol [ 'x' ], average_returns , covariance_matrix ) min_vol_allocations = allocations_ef ( min_vol . x , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f}\" . format ( min_vol_return )) print ( \"Volatility: {0:.2f}\" . format ( min_vol_volatility )) print ( min_vol_allocations ) an_vol = np . std ( returns ) * np . sqrt ( 253 ) an_rt = average_returns * 253 for i , txt in enumerate ( stocks_df . columns ): print ( txt , \":\" , \"Annuaised return\" , round ( an_rt [ i ], 2 ), \", Annualised volatility:\" , round ( an_vol [ i ], 2 )) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( results [ 0 ,:], results [ 1 ,:], c = results [ 2 ,:], cmap = 'YlGnBu' , marker = 'o' , s = 10 , alpha = 0.3 ) plt . colorbar () plt . scatter ( max_sharpe_volatility , max_sharpe_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_vol_volatility , min_vol_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) target = np . linspace ( min_vol_return , max ( an_rt ), 50 ) efficient_portfolios = efficient_frontier ( average_returns , covariance_matrix , target ) plt . plot ([ p [ 'fun' ] for p in efficient_portfolios ], target , linestyle = '-.' , color = 'white' , label = 'efficient frontier' ) plt . title ( 'Calculated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) def allocations_ef ( solution , stocks_df ): allocation = pd . DataFrame ( solution , index = stocks_df . columns , columns = [ 'allocation' ]) return allocation returns = stocks_df . pct_change () average_returns = returns . mean () covariance_matrix = returns . cov () num_portfolios = 25000 risk_free_rate = 0.01977 display_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ) The surprising part is that the calculated result is very close to what we have previously simulated by picking from randomly generated portfolios. The slight differences in allocations between the simulated vs calculated are in most cases less than 1%, which shows how powerful randomly estimating calculations can be albeit sometimes not reliable in small sample spaces. Rather than plotting every randomly generated portfolio, we can plot the individual stocks on the plot with the corresponding values of each stock's return and risk. This way we can compare how diversification is lowering the risk by optimizing the allocations. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def display_efficient_frontier_selected ( average_returns , covariance_matrix , risk_free_rate ): max_sharpe = max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ) max_sharpe_return , max_sharpe_volatility = portfolio_performance ( max_sharpe [ 'x' ], average_returns , covariance_matrix ) max_sharpe_allocations = allocations_ef ( max_sharpe . x , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f}\" . format ( max_sharpe_return )) print ( \"Volatility: {0:.2f}\" . format ( max_sharpe_volatility )) print ( max_sharpe_allocations ) min_vol = min_variance ( average_returns , covariance_matrix ) min_vol_return , min_vol_volatility = portfolio_performance ( min_vol [ 'x' ], average_returns , covariance_matrix ) min_vol_allocations = allocations_ef ( min_vol . x , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f}\" . format ( min_vol_return )) print ( \"Volatility: {0:.2f}\" . format ( min_vol_volatility )) print ( min_vol_allocations ) an_vol = np . std ( returns ) * np . sqrt ( 253 ) an_rt = average_returns * 253 for i , txt in enumerate ( stocks_df . columns ): print ( txt , \":\" , \"Annuaised return\" , round ( an_rt [ i ], 2 ), \", Annualised volatility:\" , round ( an_vol [ i ], 2 )) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( an_vol , an_rt , marker = 'o' , s = 200 ) for i , txt in enumerate ( stocks_df . columns ): plt . annotate ( txt , ( an_vol [ i ], an_rt [ i ]), xytext = ( 10 , 0 ), textcoords = 'offset points' ) plt . scatter ( max_sharpe_volatility , max_sharpe_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_vol_volatility , min_vol_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) target = np . linspace ( min_vol_return , max ( an_rt ), 50 ) efficient_portfolios = efficient_frontier ( average_returns , covariance_matrix , target ) plt . plot ([ p [ 'fun' ] for p in efficient_portfolios ], target , linestyle = '-.' , color = 'white' , label = 'efficient frontier' ) plt . title ( 'Calculated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) display_efficient_frontier_selected ( average_returns , covariance_matrix , risk_free_rate ) From the plot above, the stock with the highest risk is BHP, which accompanies the highest returns. This shows that if the investor is willing to take the risk than they will be rewarded with the higher return. This concludes the 3 part series on Python in the finance industry, if there is any topics in particular you would like to see how software can integrate and improve a service/product please feel free to get in touch!","tags":"Python","url":"https://jackmckew.dev/efficient-frontier-for-balancing-portfolios.html","loc":"https://jackmckew.dev/efficient-frontier-for-balancing-portfolios.html"},{"title":"Portfolio Balancing with Historical Stock Data","text":"Following last weeks' post ( Python for the Finance Industry ). This post is to demonstrate a method of determining an optimized portfolio based on historical stock price data. First of all while attempting to tackle this problem, I stumbled across many very informative articles in which based on what I learned throughout reading them, and trying to replicate their findings with the ASX stocks' data. Ricky Kim ( Efficient Frontier Portfolio Optimisation in Python) Bernard Brenyah ( Markowitz's Efficient Frontier in Python) Now I will not be going into how Markowit'z Efficient Frontier Portfolio Optimization & Sharpe Ratios works as these techniques are extremely well documented across this internet and very easily found. This post will be for implementing these techniques in Python to apply them to an ASX based portfolio. Picking up from the end of the previous post, we had just plotted the percentage change over the time period for our stocks' data. For the sake of this post we will be using a technique called random optimization, where will be taking a number of random attempts and selecting the best one. Further posts will show a more detailed approach to this optimization problem. Now there are multiple steps before we get to the desired outcome of a balanced portfolio. Generate X number of 'random' portfolios, Rate their performance against one another, Pick the desired solution. To generate random portfolios, we define a function such that we can pass it differing variables as to tweak our outcomes in the future. 1 2 3 4 5 6 7 8 9 10 11 12 def generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ): results = np . zeros (( 3 , num_portfolios )) weights_record = [] for portfolio in range ( num_portfolios ): weights = np . random . random ( len ( companies ) - 1 ) weights /= np . sum ( weights ) weights_record . append ( weights ) returns , volatility = portfolio_performance ( weights , average_returns , covariance_matrix ) results [ 0 , portfolio ] = volatility results [ 1 , portfolio ] = returns results [ 2 , portfolio ] = ( returns - risk_free_rate ) / volatility return results , weights_record To step through this function: Define empty location for our portfolio performance results to be stored along with recording weights so we can extract them once selected, For each portfolio to be generated, give a random 'weighting' for each of the company that we have historical data on (eg, 23% NAB.AX), Even out the distribution of the weights such that the sum of the weightings is 100% (eg, total budget), Record the weightings generated in our memory location, Determine the performance of our randomly generated portfolio (more on that soon), Fill in the portfolio performance results for this generated portfolio and repeat for X number of portfolios. In step 5 above, we have to determine how to rank the generated portfolios against each other to work out how to filter our results. To do this, we calculate volatility of the portfolio using the following formula: Bernard Brenyah , whom I mentioned at the beginning of the post, has provided a clear explanation of how the above formula can be expressed in matrix calculation in one of his blog post s. In which we just take the matrix calculation and multiply by 253 for number of trading days in Australia. 1 2 3 4 5 def portfolio_performance ( weights , average_returns , covariance_matrix ): returns = np . sum ( weights * average_returns ) * 253 variance = np . dot ( weights . T , np . dot ( covariance_matrix , weights )) volatility = np . sqrt ( variance ) * np . sqrt ( 253 ) return returns , volatility Now that we have X number of randomly generated portfolios, all ranked against one another, it's time to plot so that our results can be visualized. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def display_random_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ): results , weights = generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ) max_sharpe_index = np . argmax ( results [ 2 ]) max_volatility = results [ 0 , max_sharpe_index ] max_return = results [ 1 , max_sharpe_index ] max_sharpe_allocations = allocations ( max_sharpe_index , weights , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f}\" . format ( max_return )) print ( \"Volatility: {0:.2f}\" . format ( max_volatility )) print ( max_sharpe_allocations ) min_vol_index = np . argmin ( results [ 0 ]) min_volatility = results [ 0 , min_vol_index ] min_return = results [ 1 , min_vol_index ] min_vol_allocations = allocations ( min_vol_index , weights , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f}\" . format ( min_return )) print ( \"Volatility: {0:.2f}\" . format ( min_volatility )) print ( min_vol_allocations ) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( results [ 0 ,:], results [ 1 ,:], c = results [ 2 ,:], cmap = 'YlGnBu' , marker = 'o' , s = 10 , alpha = 0.3 ) plt . colorbar () plt . scatter ( max_volatility , max_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_volatility , min_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) plt . title ( 'Simulated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) def allocations ( index , weights , stocks_df ): allocation = pd . DataFrame ( weights [ index ], index = stocks_df . columns , columns = [ 'allocation' ]) return allocation Using the above function 'display_random_efficient_frontier', this will determine our max sharpe ratio portfolio generated and the minimum volatility portfolio with their respective returns. Now it is entirely up to the trader on how much risk they are willing to take on board with their portfolio. With the settings below in conjunction with the previously defined functions and stock data to generate the portfolios (risk free rate determined from this website ). 1 2 3 4 5 6 7 returns = stocks_df . pct_change () mean_returns = returns . mean () cov_matrix = returns . cov () num_portfolios = 25000 risk_free_rate = 0.01977 display_random_efficient_frontier ( mean_returns , cov_matrix , num_portfolios , risk_free_rate ) With the two portfolios determined, the one gives us the best risk-adjusted (as long as the trader is prepared to take the risk) is the one with the maximum Sharpe ratio, allocating a 67% portion to WOW and 32% to BHP, as these stocks were quite volatile from the daily percentage change calculations. On the other hand, the minimum volatility portfolio is reflecting the more stable of the stocks from the daily percentage change calculations distributing portions over NAB and TLS due to their stability from the percentage change calculations and reducing the portion to WOW.","tags":"Python","url":"https://jackmckew.dev/portfolio-balancing-with-historical-stock-data.html","loc":"https://jackmckew.dev/portfolio-balancing-with-historical-stock-data.html"},{"title":"Python for the Finance Industry","text":"This is the first post in a series of posts dedicated for demonstrating how Python can be applied in the finance industry. Personally, the first thing that comes to mind when I think of the finance industry is the stock market. For fellow Australians, our main stock exchange is the Australian Securities Exchange (ASX). For those who are reading who are not familiar with stocks, there is a plethora of information around stocks across the internet. When it comes to using Python with stocks, the very first thing that you will require, is data. Thankfully, there are multitudes of services out there which provide this data through application programming interfaces (APIs). The data is provided through APIs in a few common formats: JSON, XML, CSV. For this post, I will be utilising the free service, Alpha Vantage, to request historical records of stock information on the ASX. For access to Alpha Vantage's API, head to http://www.alphavantage.co/support/#api-key and register for a free API key. There is also documentation around testing if your API key is operational on the Alpha Vantage website. Now that we have access to an API in which we can extract historical records of stock information in the ASX, it's time to manipulate and analyse the data. As in my previous post Episode 8  Anaconda , I recommend setting up a virtual environment or anaconda environment to install & manage dependencies of external libraries. The packages required for this post in the series are: Pandas (For manipulating the data), Alpha_vantage (To access the historical records through an API), NumPy (For processing across the data), Matplotlib (For visualising and generate plots of the data). To import these libraries into our Python code the following\\ code is required: 1 2 3 4 import pandas as pd from alpha_vantage.timeseries import TimeSeries import matplotlib.pyplot as plt import numpy as np Now that we have imported the packages required to extract,\\ process and display the data. The first step is to extract the data in a useful\\ format from the Alpha Vantage API. First declare a list with all the companies ASX names with the suffix \".AX\" to denominate that it's from the ASX. After that initialise an empty pandas dataframe to be filled with the data to analyse. Now iterate over the list, calling a request through the API to request the data that is required. There are multiple formats of data to be extracted through the API which is detailed in the Alpha_Vantage documentation . For this post, I have used the get_daily function from the timeseries object in alpha_vantage to extract the daily information on a stock for the past 20 years, in particular, the closing value. 1 2 3 4 5 6 7 8 9 10 companies = [ 'NAB.AX' , 'WOW.AX' , 'TLS.AX' , 'BHP.AX' ] stocks_df = pd . DataFrame () for company in companies : data , meta_data = ts . get_daily ( symbol = company , outputsize = 'full' ) print ( data . head ()) stocks_df [ company ] = pd . Series ( data [ '4. close' ]) print ( stocks_df . head ()) Now that the dataframe is full of closing values for the companie's stock's closing values, it's time to begin processing. First of all, for any missing data or erroneous 0 values, the ffill() function is used to fill any missing value by propagating the last valid observation forward. After that, the timestamp on each row is forced to become the index of the dataframe and converted to a datetime type. 1 2 3 stocks_df = stocks_df . replace ( 0 , pd . np . nan ) . ffill () stocks_df . index = pd . to_datetime ( stocks_df [ \"date\" ]) stocks_df = stocks_df . drop ( \"date\" , axis = 1 ) Now that the data has gone through it's pre-processing phase, it's time to begin plotting some figures. To begin, a basic figure, plotting a single for each company's stock price over the past 20 years on a single line graph to enable comparison between the companies. 1 2 3 4 5 6 plt . figure ( figsize = ( 14 , 7 )) for column in stocks_df . columns . values : plt . plot ( stocks_df . index , stocks_df [ column ], lw = 3 , alpha = 0.8 , label = c ) plt . legend ( loc = 'upper left' , fontsize = 12 ) plt . ylabel ( 'price in $' ) plt . show () Another way to plot this data is to show it as the percentage change from the day before AKA daily returns. By plotting the data in this way, instead of showing the actual prices, the graph is showing the stocks' volatility. 1 2 3 4 5 6 7 8 returns = stocks_df . pct_change () plt . figure ( figsize = ( 14 , 7 )) for column in returns . columns . values : plt . plot ( returns . index , returns [ column ], lw = 3 , alpha = 0.8 , label = c ) plt . legend ( loc = 'upper left' , fontsize = 12 ) plt . ylabel ( 'daily returns' ) plt . show () Now that we have some insight to the stocks' data, the next post in this series will demonstrate a way to calculate a balanced portfolio from historical records using Modern Portfolio Theory.","tags":"Python","url":"https://jackmckew.dev/python-for-the-finance-industry.html","loc":"https://jackmckew.dev/python-for-the-finance-industry.html"},{"title":"How to Program an ESP8266 with MicroPython","text":"Following the previous two weeks of topics, Introduction to ESP32/ESP8266 \\ and What is MicroPython? . I wrote an article on maker.pro in which I describe how to program the ESP8266 with MicroPython in detail.","tags":"Engineering","url":"https://jackmckew.dev/how-to-program-an-esp8266-with-micropython.html","loc":"https://jackmckew.dev/how-to-program-an-esp8266-with-micropython.html"},{"title":"What is MicroPython?","text":"From the MicroPython docs themselves \"MicroPython is a lean and efficient implementation of the Python 3 programming language that includes a small subset of the Python standard library and is optimised to run on microcontrollers and in constrained environments.\". But what does all this mean? Python 3 is one of the most widely used, easy to write/read programming languages in the world that is rapidly growing. By default Python comes with a standard library' which includes basic functions such as if statements, loops, printing, etc. Where MicroPython comes in is that the standard library for Python might take up valuable space/computations to run as efficiently it does on a PC, so MicroPython is a slice of the standard library that is able to run more efficiently and take up less space on a microcontroller (RAM and space is crucial when working with microcontrollers). MicroPython also comes with an interactive REPL (Read-Evaluate-Print Loop), which is an often overlooked amazing feature of MicroPython. The REPL allows you to connect to a microcontroller, execute code quickly without the need to compile or upload code. Which gives immediate feedback on whether your program is working as intended. Differences between MicroPython & Python There obviously had to be some changes between Python and MicroPython to make it work efficiently on processors a fraction of the power, but what are they? If you are a beginner-intermediate Python programmer, you'll only run into trouble in very specific scenarios, which can be easily worked around. For example you cannot delete from a list with a step greater than 1. Sample Python Code 1 2 3 L = [ 1 , 2 , 3 , 4 ] del ( L [ 0 : 4 : 2 ]) print ( L ) You'd expect for the output here in Python normally to be: Python Output MicroPython Output [2,4] TypeError: object 'range' isn't a tuple or list However this can be easily worked around with an explicit loop for example: Sample MicroPython/Python Code 1 2 3 4 L = [ 1 , 2 , 3 , 4 ] for i in L : if ( i % 2 == 0 ): del ( L [ i ]) For more information on differences between Python (in particular CPython) and MicroPython you can find the MicroPython documentation here: http://docs.micropython.org/en/latest/genrst/index.html","tags":"Python","url":"https://jackmckew.dev/what-is-micropython.html","loc":"https://jackmckew.dev/what-is-micropython.html"},{"title":"Introduction to ESP32/ESP8266","text":"What is an ESP32/ESP8266? The ESP32 and ESP8266 are low-cost Wi-Fi modules, which are perfect for DIY Internet of Things (IoT) projects. They both come with general purpose input/output pins (GPIOs), support a variety of protocols such as SPI, I2C, UART and many more. The most attractive part of the ESP range is that they come with wireless networking included, separating them from their Arduino microcontroller counterparts. All in all, the ESP series allows you to easily control/monitor devices remotely using Wi-Fi for a very low price. ESP32 vs ESP8266 The ESP32 is the later model' of the ESP8266. It added a whole suite of new functionality such as: touch sensitive pins, built-in temperature and hall effect sensors and upgraded from single core CPU to a dual core, faster Wi-FI, more GPIOs and now supports Bluetooth and BLE (Bluetooth Low Energy). While both boards are very low-cost, the ESP32 costs slightly more, the ESP8266 (here in Australia) costs around \\~\\ \\(10AU, and the ESP32 around \\~\\\\) 22AU. Flavours of ESP boards There are currently many different varieties of ESP flavours you can buy off the shelf, while if you are more into developing the board around your ESP module (the pictures above) you can simply just purchase the relevant ESP module, or if you are like me and don't want to bother soldering and developing your own board there is a solutions for you!\\ ESP32 Development Boards ESP32 Thing - Sparkfun The ESP32 Thing comes with all the functionalities to easily communication and program the ESP32 with your computer (including a on-board USB-Serial). It also features a LiPo charger, so your ESP32 project can use rechargeable batteries without having to solder any terminals and make it easy to replace/disconnect the battery pack. Espressif ESP32 Development Board - Developer Edition If you're not confident on soldering the header pins on the Sparkfun Thing board, then the Espressif board comes with that done for you! The header pins are also nicely spaced out so if you are a breadboard enthusiast, you can just plug and play on your breadboard and start connecting all your header wires. ESP8266 Development Boards NodeMCU The NodeMCU is my personal favourite ESP flavour board because it is friendly to your breadboard, has an on-board USB-Serial and can be powered by USB. This all means that you can test and develop your board straight out of the box without fiddling around with soldering pins, voltages or getting any extra components (except a Micro-usb cable). Adafruit Huzzah ESP8266 Breakout The Huzzah board is Adafruits answer to other development boards that weren't friendly to breadboards, didn't have on-board voltage regulators and weren't CE or FCC emitter certified. The Huzzah board comes with all these functionalities, although unlike the NodeMCU you will need to get a USB-Serial cable to able to program your Huzzah board. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Engineering","url":"https://jackmckew.dev/introduction-to-esp32-esp8266.html","loc":"https://jackmckew.dev/introduction-to-esp32-esp8266.html"},{"title":"Episode 17  Networking Routing & Addressing","text":"Following last weeks post around network topologies, I believe the next topic to cover is routing and addressing. Routing is the process of selecting a path for traffic to flow through in a network while addressing is marking elements within a network. A real-world example of routing and addressing is the postal system, each element (person) is marked with an address (eg, a street address) and the mail makes it to that address from routing it from the sender to the receiver. While the goal for routing may be simple (\"go from sender to receiver in the most efficient/quickest way possible\"), the techniques used to achieve this can be very complex and confusing but when solutions are found that make a network work efficiently, it is a very rewarding experience for all users. Routing can be broken into three broad categories: Protocols  the medium that allows information to move through a network Algorithms  to determine paths between sender and receiver Databases  to store information that the algorithms determine The whole premise around routers in a network ( Networking Basics ) is that they will \"pass it on\", either to their smarter peers or in the correct direction. For example in a star/tree network, devices pass information to their closest router' which then decides either to pass it directly to the correct address or to another router which may have a better idea on where the information is intended on going. Protocols In industry, some of the most common networking protocols are MODBUS and DNP3. Modbus being a de-facto standard for interconnecting electrical equipment and DNP3 (Distributed Network Protocol) commonly being used in the water/electric industries for their flexibility during outages or broken links in a network. Algorithms Routing tables is the most prevalent type of routing algorithms with their fixed nature meaning once the routing decisions for how information travels have been decided, they do not change. The other type of routing algorithms (which are much more exciting) are known as adaptive algorithms, which means the routing changes depending on: topology, delay, load, etc, to try and reach the most efficient path from sender to receiver. Databases Following algorithms, databases can either hold the entire routing table and a router looks up where it wants to go and it which path to take (similar to a bus timetable), or, forwarding tables (technically can be apart of routing tables as well) which detail the communications pathways to utilize for types of traffic.","tags":"Engineering","url":"https://jackmckew.dev/episode-17-networking-routing-addressing.html","loc":"https://jackmckew.dev/episode-17-networking-routing-addressing.html"},{"title":"Episode 16 - Networking Basics","text":"A network is defined as \"A network is a collection of computers, servers, mainframes, network devices, peripherals, or other devices connected to one another to allow the sharing of data\". There are various configurations of networks for specific design scenarios as represented in: Typical residential home networks are configured in a tree topology that is connected to the internet. This typically consists of a single router/modem that serves all the end-user devices on the network with internet connection. The router also acts as a gateway for the devices connected on the network to communicate with one another. Packets of data that are generated by the devices are encapsulated with destination routing information; which is passed to the router at the center which directs the data to it's destination in the network. For example, a user connects to the router to gain access to a wider network that is the internet to load this webpage. If there was a higher risk on losing the communications medium between two devices (cable failure), then bus would be at disadvantage here but ring might prove more beneficial although transmission would be slower as the network connection would be further away (go around the ring in the case of the picture above). Mesh Topology By explaining network topologies by comparing to a basic Wi-Fi network normally gets the message across. A mesh network can prove beneficial to areas in which a star network isn't covering the area you wish it to, for example, if you have 'dark' spots where you don't receive Wi-Fi signal, a mesh network might be better suited. A practical implementation of a mesh network can be seen in shopping centre's Wi-Fi networks where multiple routers are placed strategically such that you can walk around the entire property and not lose signal. Ring Topology While star is a very popular configuration of network, it however is not the most ideal configuration for some types of networks. For example, if you had a series of devices that all needed to talk to each other, even if one was to fail, then a ring/bus/mesh would be more applicable where there is always a path to everyone else if a device was to fall offline. Star Topology If you consider a home Wi-Fi network that doesn't connect out further (no internet connection) then you have a basic star network. The center of the star in this scenario would be the Wi-Fi router, you can still connect to the other devices but not outside of your network and all messages have to travel through the router. Tree Topology A tree topology is just creating multiple star networks off the back of another network. For example, if you considered the network that is your internet connection from the street (or satellite), then connecting to your modem (gateway) then furthering to your devices in your home, you have a basic tree network. Bus Topology Where devices are connected to a single medium (cable) to communicate with each other, you now have a bus network. A bus network proves it's advantages by less cabling than star networks, ease of installation of linear networks and works well with small networks. It is easy enough to add new devices to the network and if one fails (but the medium doesn't) then all devices can still communicate. Disadvantages arise when problems occur as it difficult to determine the cause of an issue on a bus network.","tags":"Engineering","url":"https://jackmckew.dev/episode-16-networking-basics.html","loc":"https://jackmckew.dev/episode-16-networking-basics.html"},{"title":"Episode 15 - What is a C.T?","text":"A C.T is the abbreviated form for a current transformer in electrical terms. It is a simple but effective use of magnetic circuits and transformer characteristics to monitor how power is behaving in a conductor. The C.T works by wrapping a coil of conductor around a core (typically silicon steel) shaped like a ring, the 'power' wire or the active is then passed through the core. When alternating current passes through the active conductor, this then generates a magnetic field around the wire, inducing a current proportional to the number of turns the wire is wrapped around the core. The C.T is a very widely used piece of equipment in instrument electrical/electronics as it allows for an indirect way of monitoring for the power flow unlike a 'flow' meter that must be a part of the pipework to directly measure flow. While it is still possible to monitor current in a 'flow' meter type fashion, it is far less risk to implement a C.T solution. Possibly the frequent implementation of C.Ts in everyday life is within power meter solutions. By attaching a C.T to the active conductor that is powering a piece of equipment, we are then able to measure the power the equipment is using in real time. This can then be further digitised and utilised in a network fashion to provide asset owners with real time energy usage of their equipment. It must be noted when using C.Ts on alternating current systems that the C.T must only have a single conductor pass through it (the active); if both the active and the neutral are passed through the C.T then (in an ideal world) the C.T will have an output of 0. If you also pass through the earth wire to the equipment, it is possible to measure earth leakage or earth fault current (provided the measurement can handle it).","tags":"Engineering","url":"https://jackmckew.dev/episode-15-what-is-a-c-t.html","loc":"https://jackmckew.dev/episode-15-what-is-a-c-t.html"},{"title":"Episode 14 - Types of Machine Learning","text":"With AI and Machine Learning becoming the buzzwords in technology for 2018 and the real world applications now maturing to show the benefits of this technology. It can be very confusing when first entering the world of AI and machine learning with new techniques coming out every other day in search of improving the technology. Hopefully this article will help break down the barriers of the jargon and explain the types of machine learning algorithms out in the wild simplistically. In general, there are 3 different broad categories that current machine learning algorithms fit into: Supervised learning Unsupervised learning Reinforcement learning Supervised Learning Most practical machine learning algorithms use supervised learning. Supervised learning is where you have one or more input variables (x) and output variable(s) (y), and you use an algorithm to learn the mapping function from the input to the output. 1 y = f(x) The end goal of this algorithm is to approximate the mapping function accurately such that then you have a new data input (x), you can predict what the result (y) for that data would be. The name supervised learning comes from the algorithm first learning from a training data set before we present the algorithm to a new data set. The training data set can be thought as the teacher who is supervising the learning process, and learning only stops when the algorithm reaches an acceptable level of performance on predicting the result. Unsupervised Learning Unsupervised learning is when you only have the input variable(s) (x) and no respective output (y). The end goal for unsupervised learning is to model the distribution or structure of the data in order to discover and learn about the data set. Unsupervised learning in contrast to supervised learning is where the omnipresent teacher in supervised learning is gone and there is no correct answers. The algorithm is left alone to discover and present the distribution/structure in the data that it determines. Reinforcement Learning Reinforcement learning is the third broad category that a machine learning algorithm can fall into, where the algorithm has the input variable(s) (x) and through interacting with the input data set receives rewards for performing favoured actions. Learning from interactions with the environment around us comes from our natural experiences in the world. For example, imagine you're a child in a room with a fire. You move closer to the fire and feel it's warmth and it makes you feel good, this is a positive reward; then you try a touch the fire and it burns you hand, this is a negative reward. Reinforcement learning is just a computational approach to learning from interactions to achieve the most favourable result, in our example, we learnt that being close to the fire is a positive thing but too close is a negative thing so our result is to maintain a sufficient distance away to be warm but no burnt.","tags":"Machine Learning","url":"https://jackmckew.dev/episode-13-types-of-machine-learning.html","loc":"https://jackmckew.dev/episode-13-types-of-machine-learning.html"},{"title":"Episode 13 - Lighting Design","text":"Before I started in a more buildings-focused electrical engineering position, I didn't think that much went into selecting lights for buildings. Once you first get started in lighting design, it is like opening a can of worms, there is so much detail that goes into lighting design, it's unfathomable. First of all, lighting design in Australia is dictated by AS1158. Not only does the Australian standard explicitly state illuminance requirements for rooms based on their task (eg 320 lux for office based tasks), it also clearly defines how to calculate these levels based on the environment. What really is lux? Lux is the SI derived unit of illuminance and luminous emittance, measuring luminous flux per unit area. It is equal to one lumen per square meter. Luminous flux? Lumens? What do all these terms mean? A lumen is the SI derived unit of luminous flux, which is a measure of the total amount of visible light emitted by a source. Now when it comes to designing lighting for a building/area, multiple large considerations must be taken. Once you have determined what tasks will be completed within the area you are designing, you must then go to AS1158 and determine the required lux requirements. Following this, you must ensure you have accurate dimensions of the area you are designing for, along with all reflectance (colour) of surfaces within the area. Once you have got all these parameters, it is time to begin modelling the area within any lighting design software package (eg AGI-32). Now the designer must select lights (luminaires) to be specified for the area. The designer must take into consideration the ceiling (if there is one) type, this will dictate how the luminaires are to be mounted, be it: surface, recessed, suspended or pole mounted. Most luminaire fitting manufacturers will provide photometric files (IES files) detailing how their respective lights would behave if they were installed in the design area. Once the designer has verified that the specified luminaires will meet required lux levels in the area, this design must be passed to the electrical designer as they must factor in how much power all the luminaires will require to operate and how the cable routes must be laid out to suit the luminaires on the site. Please note, that this is a very simplistic view at lighting design. Just like any area of work, there is an art to doing a proper job.","tags":"Engineering","url":"https://jackmckew.dev/episode-13-lighting-design.html","loc":"https://jackmckew.dev/episode-13-lighting-design.html"},{"title":"Episode 12 - What is Git?","text":"One of the biggest issues when working on any project regardless of what industry, discipline or context, as soon as a new 'version' of design or update comes along, the issue of version control appears. When this change(s) come along in the life cycle of a project, it is within everyone involved's best interests to maintain some type of version control, to manage and track changes between versions. When it comes to software development version control, the most well-known and commonly used system is Git. Git is a actively maintained open source project originally developed by Linus Torvalds (creator of the Linux OS kernel) in 2005. Multitudes of software projects depend on git for version control, including both commercial and open source. Git has a distributed architecture, meaning rather than having one single location for the full version history of the project, every developer's working copy of the project is also a location that can contain the full history. This also comes with the benefit that if one developer happens to lose the entire project, it can be restored from other developers (pending they have/are working on the latest version). Flexibility Git is flexible in several respects: efficiency at maintaining version control for both small and large projects, capable at handling numerous types of non-linear development workflows and compatibility with existing protocols and systems. Git supports branching and tagging (unlike other commonly used version control systems), and operations that affect branches and tags such as reverting or merging are stored as part of the change history. This level of tracking is not available in many other version control systems. Security The integrity of the source code of the project was identified as a top priority when Git was design. Cryptographically secure hashing algorithms are used to secure: file contents, file and directory relationships, versions, tags and commits. This defends the project's source files and change history against both malicious and accidental changes and ensures that all modifications is fully traceable. Use-case Bob wants to implement a new feature to the project he is working on in anticipation of the upcoming 3.0 release, and commits the new feature with descriptive messages. After being inspired by the new feature, he works on a second new feature and commits those modifications as well. Naturally, these two new features are logged as separate entities within the change history of the project. Bob then returns back to version 2.6 of the same project to fix a problem that only affects version 2.6. This allow Bob (and his team) to distribute that fix release (version 2.6.1), before version 3.0 is ready. Bob can then return to the upcoming version 3.0 'branch' to continue implementing new features. Since Bob has his own copy of the entire project, all of these changes can occur without an network access, thus being reliable and fast. To submit all these committed features and fixes to the remote copy of the project, it simply done by the use of the 'push' command.","tags":"Software Development","url":"https://jackmckew.dev/episode-12-what-is-git.html","loc":"https://jackmckew.dev/episode-12-what-is-git.html"},{"title":"Episode 11 - Power Quality Explained","text":"I've always lived by the rule that if you can't explain something to a 5 year old then you don't know it well enough. I was asked recently by some (non-electrical focused) colleagues on a handful of electrical terms and components. One of the biggest things that kept popping up that I found difficult to explain clearly was power quality and it's issues. So I decided why not dedicate a blog post about it and write a basic example power factor capacitor calculator in Python. Power quality is defined as \"the concept of powering and grounding sensitive electronic equipment in a manner suitable for the equipment with precise wiring system and other connected equipment\" by the IEEE (The Institute of Electrical and Electronics Engineers). In a simplistic view this is just trying to say that electrical equipment is to be installed/configured in a way that is operates as intended. Quality of power is not determined by the one who produces it, it's defined by the end user of the power. Eg, like a physical product, if you buy something from a store and it's poor quality, that's being defined by the end user. Similar to that of a physical product, quality of power can be lost in a variety of forms/ways. Issues with power quality can be categorized into three main categories: Harmonic voltages and currents Poor power factor Voltage instability Harmonics AC (Alternating Current) electricity is generated as a sinusoidal waveform, and harmonics are signals/waves whose frequency is a whole number multiple of the frequency of the reference signal/wave. To visualize this phenomenon, we can use packages like NumPy and Matplotlib, to calculate and plot our base signal and it's harmonics (I encourage you to run this code and change the harmonics to see what they look like). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import matplotlib.pyplot as plt def Harmonic ( i ): x = np . linspace ( 0 , 2 * np . pi , 2000 ) y = [ 0 for _ in x ] for n in range ( 0 , i ): y += np . sin (( 2 * n + 1 ) * ( 2 * np . pi ) * ( x )) / ( 2 * n + 1 ) plt . plot ( x , y ) plt . grid () Harmonic ( 1 ) Harmonic ( 2 ) # Harmonic(3) # Harmonic(4) # Harmonic(5) plt . show () The example above shows us the base signal (fundamental frequency), and it's first harmonic (harmonic of 2 or twice as fast as the fundamental frequency). When these two signals are superimposed on each other, they produce a distorted waveform. Electrical equipment is designed to operate at the base signal (50Hz here in Australia), and typically does not cope with distorted wave like seen below when we superimpose a base signal with it's first harmonic. Luckily, these issues are now easily detected and rectified by harmonic analyzers and active/passive harmonic filters. Power Factor Power factor is a measure of how effectively power is being used in an electrical system, and is defined as the ratio of real (useful) to apparent (total) power. Real power (kW) is the power that actually powers the equipment to produce useful work (such as spinning a motor). It can also be called actual, active or working power. Reactive power (kVAR) is the power required by (some) equipment (eg, motors), to produce a magnetic field to enable the useful work to be produce. It's necessary to operate the equipment, however you don't see any result from the reactive power. Apparent power (kVA) is the vector sum of the real power (kW) and reactive power (kVAR) and is the total power supplied from the mains power required to produce the right amount of real power. Suppose you are running a store, you have to spend an amount of money X (cost) on buying products to sell in the future for a larger amount of money Y, meaning your profit will be P = Y - X. X is not lost money, without spending X you will not be able to make any profit P. The profit P is comparable to the active power, the earnings Y are the equivalent of apparent power and the initial cost X is the reactive power. Therefore, for a given power supply (kVA): The more cost you have (higher percentage of kVAR), the lower the ratio of kW (profit) to kVA (profit + cost), meaning a poorer power factor. The less cost you have (lower percentage of kVAR), the higher your ratio of kW (profit) to kVA (profit + cost) becomes, and the better you power factor. As your cost (kVAR) approaches zero, your power factor approaches 1 (unity). Voltage Instability A stable voltage is when every piece of equipment connected to a network is operating under normal condition without issues, however when a fault or disturbance (harmonics) occurs in this system, the voltage becomes unstable. Due to voltage instability, the electrical system's voltage may collapse, if the voltage is below acceptable limits. Voltage collapse may be a total or partial black, the terms voltage instability and voltage collapse are interchangeable. For example, if 10 generators are running to keep 10 machines working. Suddenly 3 of the generators run out of fuel, but the 10 machines keep going. This would cause a loss of generation, not being able to maintain the power required to keep all the machines working and consequentially since there is not enough power to share between any of the machines, all 10 machines will turn off, causing a total blackout. Capacitor Calculator - Python Correcting power factor from a lagging (\\<1) power factor, can be as simple as reducing reactive power (kVAR) in the system such that the ratio of real power (kW) to apparent power (kVA) is still as close to unity (1) as possible. Since motors require inductive or lagging power for magnetizing before useful work beings, this brings makes the power factor of the system lagging (\\<1). Capacitors provide capacitive or leading reactive power that cancels out the lagging power when used for power-factor improvement. The improved power factor changes the current requirements of the system, but not the one required by the motor. Using these formulas we can calculate just how big of a capacitor we require: Once we input all these required formulas, and our initial data points, we are now able to easily compute the required size of capacitor to amend power factor issues. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from math import sqrt , pi real_power = 2.2 #Real power in kW current = 10 #Current in amps voltage = 240 #Voltage in volts frequency = 50 #Frequency in hertz corrected_pf = 0.95 #Target power factor #Calculate current power factor and apparent power current_pf = 1000 * real_power / ( voltage * current ) S_current = ( voltage * current ) / 1000 #Power factors greater than 1 will give imaginary Q_current, alert user try : #Calculate current reactive power Q_current = sqrt ( pow ( abs ( S_current ), 2 ) - pow ( real_power , 2 )) #Calculate target apparent power S_corrected = real_power / corrected_pf #Calculate required reactive power compensation Q_corrected = sqrt ( pow ( S_corrected , 2 ) - pow ( real_power , 2 )) #Calculate size of capacitor required for reactive power Q_c = Q_current - Q_corrected C_f = 1000 * Q_c / ( 2 * pi * frequency * voltage ) #Print results to user print ( \"Current power factor {0:.3f}\" . format ( current_pf )) print ( \"Current apparent power {0:.3f} kVA\" . format ( S_current )) print ( \"Current reactive power {0:.3f} kVAR\" . format ( Q_current )) print ( \"Capacitor required {0:.3f} Farads\" . format ( C_f )) except ValueError : print ( \"Current power factor > 1\" )","tags":"Engineering","url":"https://jackmckew.dev/episode-11-power-quality-explained.html","loc":"https://jackmckew.dev/episode-11-power-quality-explained.html"},{"title":"Episode 10 - Python Package Cheat Sheet","text":"One of the biggest skills in any career path comes solely from knowing where to look and what to look for when breaking down a problem. The same principle applies for Python programming. Since there are millions of different packages out there that all serve different purposes, it is often difficult to even know if there is a package out there that will solve your problem. I will be updating this table in the future as well as I personally find more and more solutions to my problems, and hope to share this insight with everyone. I will not be including packages from Python's standard library. Package Name Description Used For Pandas pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Data Analysis Numpy NumPy is the fundamental package for scientific computing with Python. Data Analysis SciPy SciPy (pronounced \"Sigh Pie\") is a Python-based ecosystem of open-source software for mathematics, science, and engineering. Data Analysis Matplotlib Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Data Visualisation Spyder Spyder is a powerful scientific environment written in Python, for Python, and designed by and for scientists, engineers and data analysts. Data Visualisation/Data Analysis Folium folium builds on the data wrangling strengths of the Python ecosystem and the mapping strengths of the Leaflet.js library. Manipulate your data in Python, then visualize it in a Leaflet map via folium. Data Visualiation Bokeh Bokeh is an interactive visualization library that targets modern web browsers for presentation. Data Visualisation Camelot Camelot is a Python library that makes it easy for anyone to extract tables from PDF files! PDF Manipulation tqdm tqdm is a lightweight package for displaying progress bars within a console General Selenium Selenium automates browsers . That's it! What you do with that power is entirely up to you. Web Automation Beautiful Soup Beautiful Soup is a Python library for pulling data out of HTML and XML files. Web Scraping Scrapy An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way. Web Scraping Requests Requests is an elegant and simple HTTP library for Python, built for human beings. Web Interaction Flask Flask is a microframework for Python based on Werkzeug, Jinja 2 and good intentions. And before you ask: It's BSD licensed ! Web Development Django Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Web Development Opencv OpenCV (Open Source Computer Vision Library) is released under a BSD license and hence it's free for both academic and commercial use. Image Analysis Pygame pygame ( the library ) is a Free and Open Source python programming language library for making multimedia applications like games built on top of the excellent SDL library. Game Development Pyinstaller PyInstaller freezes (packages) Python applications into stand-alone executables, under Windows, GNU/Linux, Mac OS X, FreeBSD, Solaris and AIX. Distribution cx_freeze cx_Freeze is a set of scripts and modules for freezing Python scripts into executables Distribution PyQt5 Qt is set of cross-platform C++ libraries that implement high-level APIs for accessing many aspects of modern desktop and mobile systems. GUI Development","tags":"Python","url":"https://jackmckew.dev/episode-10-python-package-cheat-sheet.html","loc":"https://jackmckew.dev/episode-10-python-package-cheat-sheet.html"},{"title":"Episode 9 - Web Enabled Universal Remote - Part 1","text":"I have a habit of misplacing all kinds of remotes within the house, TV, air conditioner, fans, etc, and having a different remote for everything can be quite annoying at times. So I decided to re-use some leftover components from a previous project to make a web enabled universal remote. Since most existing remotes use infrared to send the signal from the remote to the device, I figured it would be simple enough to create a infrared signal 'decoder' and then use a infrared diode to then replicate this signal back to the device. Next consideration was what do hardware is needed to get this project up and running. After researching a few other DIY remote control guides on the internet, I came up with a plan to use a wifi-enabled microcontroller together with an infrared receiver and an infrared diode. After rummaging through my spare hardware box, I happened to find a spare NodeMCU (ESP8266) that I could use for this project, this brings my part list to: Wifi-Enabled Microcontroller (NodeMCU) A resistor to dampen the diode signal (100 ohm) A transistor to boost the current from the NodeMCU so the diode signal gets to the device (2N222) Infrared receiver (TSOP4136) Infrared diode (L-7113F3BT) Now before connecting the entire circuit together, one should always test that components work in an expected way. To achieve this for the infrared receiver, a basic program to interface between the receiver and the microcontroller is needed. For a basic test, an LED would light up whenever the infrared is receiving a signal. By following the circuit diagram with the corresponding code for the NodeMCU, this test for the receiver should be reproduce-able at home, please note that for other infrared receivers you will need to check the pin outs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #define ledPin D0 //Connection at GPIO16 (D0) for the builtin LED on the NodeMCU board #define inputPin D5 //Connection at GPIO14 (D5) for the infrared receiver int val = 0 ; // variable for reading the pin status void setup () { pinMode ( ledPin , OUTPUT ); // declare LED as output pinMode ( inputPin , INPUT ); // declare Infrared sensor as input } void loop () { val = digitalRead ( inputPin ); // read input value if ( val == HIGH ) { // check if the input is HIGH digitalWrite ( ledPin , LOW ); // turn LED OFF } else { digitalWrite ( ledPin , HIGH ); // turn LED ON } } In the code above, this defines the pins that the sensor and LED are connected to, checks if the sensor is receiving a signal and then switches the builtin LED accordingly. Since the microcontroller loops without delay and an infrared remote control sends signals very quickly with delay in between, the LED only flickers when a remote is aimed at it, however proving the component works as expected. Now that we have confirmed the receiver works as expected, we have to integrate and interface the infrared diode with the microcontroller such that we are able to send the decoded signals back at the device. Since the NodeMCU can only pass a maximum current of 12mA through the GPIO pins, this will not be enough for the infrared diode peak spectrum which occurs at 20mA. To boost the current up from 12mA to 20mA+, it is best to use a simple transistor, for this project I had some 2N2222 transistors lying around so decided to use them. The following circuit diagram shows how the infrared diode, transistor and microcontroller integrate together. Since the human eye cannot see the infrared diode turning on/off, this creates a challenge for testing this component before implementing the project. I did not create a test specifically for the diode, and will test whether it works correctly later on in some function testing of the project. This completes the hardware component of this project, the next part of this project will be the software. I am planning to utilise both docker and django on my home Raspberry Pi to act as a webserver that will issue commands to the microcontroller over a network to mimic the device's remote control.","tags":"Engineering","url":"https://jackmckew.dev/episode-9-web-enabled-universal-remote-part-1.html","loc":"https://jackmckew.dev/episode-9-web-enabled-universal-remote-part-1.html"},{"title":"Episode 8 - Anaconda","text":"Python is one of my favourite languages to develop in (if you haven't noticed yet). My favourite feature of Python is how easy it is to share your work with others and integrate other's code into your own projects. However as a project grows and gets older as time goes on it can be cumbersome to keep track of hundreds of dependencies that your project relies on to work. Even more so when all of these package dependencies are also being updated and changing functionality. One elegant solution that I always use when first starting a new project is to use Anaconda (https://www.anaconda.com/). Anaconda is a free, easy-to-install package and environment manager for Python. It is very simple to use in that when you are starting a new project, you just need to create a new environment (within the Anaconda navigator) with the python version you wish to use and then activate it. Simple as that. 1 conda create --name new_environment_name python = 3 .5 In one single line, we have just created a new environment named \"new_environment_name\" and specified that this environment will use Python version 3.5. Now to activate the environment it is as simple as typing \"activate new_environment_name\". 1 activate new_environment_name Now to see what packages are contained within our newly created environment, or to ever see what packages and their versions are listed the command is: 1 conda list Now that we have created, activated and peeked inside our newly created environment we need to add some packages that we might use! This is as simple as the command \"conda install PACKAGENAME\", for example we might want to install matplotlib, a widely used data visualization package. Installing matplotlib into our environment is done by the command: 1 conda install matplotlib You will note that when this runs, it also asks to install all the dependencies that matplotlib relies on and will also notify you later when you have more packages that some might clash and need to be upgraded/downgraded so that all packages have a common version to work with. With regards to working with certain version numbers of packages within an Anaconda environment, to install a specific version of a package, or even if you know what the minimum requirement is, can by following the table below: Constraint Specification Result Fuzzy numpy=1.11 1.11.0, 1.11.1, 1.11.2, 1.11.18, etc Exact numpy==1.11 1.11.0 Greater than or equal to \"numpy>=1.11\" 1.11.0 or higher OR \"numpy=1.11.1|1.11.3\" 1.11.1 or 1.11.3 AND \"numpy>1.8,<2\" 1.8, 1.9, not 2.0 By following these simple constraint rules, it is very easy to manage package version to maintain dependencies within your project without tearing your hair out when packages update and break your project. Another major benefit of using Anaconda to manage your project's package dependencies is that when you're developing simultaneously with other projects and you may discover some bugs and wish to share them with your colleagues. To share all the dependencies (and their respective versions) with your colleague is as easy as generating an \"environment file\" and sharing the file with them so they have exactly the same environment as you. This is done by the following command: 1 conda env export > environment.yml Similarly, if you colleague sends you their \"environment file\", the command to reproduce their environment is (Please note that the name of the environment is encoded within the first line of the .yml file): 1 conda env create -f environment.yml In summary, Anaconda can be used to easily manage packages and dependencies across a project and fast track test/bug reproduction across multiple machines seamlessly. Personally, I would always advise to use a package manager across projects no matter the size.","tags":"Python","url":"https://jackmckew.dev/episode-8-anaconda.html","loc":"https://jackmckew.dev/episode-8-anaconda.html"},{"title":"Episode 7  Planning","text":"With 2018 coming to an end, we welcome in the new year with the first episode of Code Fridays for 2018. Continuing with the theme of things starting a new, this episode is dedicated to a major factor or stage in any type of development, planning. Planning is one of the most crucial steps when beginning to tackle a project or even a problem. One of the most effective ways to deal with a problem coming from a time or financial perspective is prevention of the problem. By considering what problems may arise in a project's lifetime, the developer or designer can implement prevention before the problem comes to fruition. Being an effective planner can help you in all walks of life. Not only can one just plan for problems that may arise in a project, one can also plan to educate themselves with the knowledge to tackle unforeseen problems within a project. With an entire new year ahead of us all, I'd like to plan on what I'd like to learn in 2019 and in turn share what I learn with you all through this blog. Containerization  integrating containerization as mentioned in Episode 6 into projects Javascript  javascript is still an enigma to me at this point, I plan to work on beginner projects and hopefully integrate some of them to enhance this website Developing, deploying and maintaining an Android Application  I am currently midway through developing an Android application, in future episodes I will write tutorials on how to develop certain elements with an Android application to make it more interactive Big Data tools  Apache arrow and Hadoop are also new to me at this point, hoping to integrate these elements into projects that I'm currently working on This is only just a small taste of what I am planning to cover at the very least on my weekly updates in this blog so stay tuned throughout the whole year!","tags":"Principles","url":"https://jackmckew.dev/episode-7-planning.html","loc":"https://jackmckew.dev/episode-7-planning.html"},{"title":"Episode 6 - Containerization","text":"Recently I was researching into ways to more efficiently and effectively distribute software and I stumbled across containerization of applications. Containerization of application is when an application is run on an OS-level virtualization without spinning up an entire virtual machine for the application. Previously the way I had been distributing software I had been developing in my preferred language (Python) was by using PyInstaller (https://pyinstaller.readthedocs.io/en/stable/). However I was running into issues with distributing a single executable throughout users, although since the software was only used by a small userbase at this stage, I was able to continue to use PyInstaller. I started researching containerization as in the future the software I will be developing will be used by a larger userbase. This will hopefully be more effective at managing versions and distributing updates to said userbase. Most other professionals in the software space have been constantly mentioning the use of Docker (https://www.docker.com/), I am now integrating my projects into Docker and have had no issues thus far. By utilizing OS-level containerization this also allows the developer to run on any OS they wish. For multiple projects of mine, I had been intending to use influxDB (https://www.influxdata.com/), however was limited to a strictly Windows only network. I see Docker as a solution to this problem, by being able to create a linux based container to run an instance of an influxDB that can be spun up within a Windows environment and communicate back to the Windows users in the network. Lastly, I'd like to wish a happy holidays to everyone reading and will be bringing more weekly content in the new year. Please do not hesitate to comment below if there is any topics/projects that you would like for me to research and write about my findings.","tags":"Software Development","url":"https://jackmckew.dev/episode-6-containerization.html","loc":"https://jackmckew.dev/episode-6-containerization.html"},{"title":"Episode 5 - Android Multi-Touch","text":"This week's episode of Code Fridays will go into detail on how to handle multi-touch inputs within Android. Firstly to handle the location on where the screen in being touched we need to create a class to handle the interaction. By creating a public class like Finger.java as can be seen below it contains 3 values: x_pos, y_pos and id. It is also useful to create a constructor so that other classes can easily construct the Finger class. 1 2 3 4 5 6 7 8 9 10 11 12 13 public class Finger { public float x_pos ; public float y_pos ; public int id ; Finger ( float init_x , float init_y , int init_id ) { x_pos = init_x ; y_pos = init_y ; id = init_id ; } } Now that we have a class to store our details on how each finger is touching the screen, we now need to interact with some base level Java. Firstly we need to extend a view within the Android application so that the application knows what boundaries to deal, in my test application, I've just used the entire screen as a view. After that an array is needed to store the data of multiple inputs touching the screen. I've used a TreeMap in this example as this allows for ease later on so that they are in order on how they were input, however this comes with a downside to this example as lifting a input in the middle of the order touched crashes the array, this will be fixed in a later episode. A paint is initialized for both the stroke paint for drawing lines between the touches and a paint for the text that is to come. Generic constructors for the view are also listed below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class TouchView extends View { private TreeMap < Integer , Finger > lineMap = new TreeMap <>(); @SuppressLint ( \"UseSparseArrays\" ) private HashMap < Integer , Path > fingerMap = new HashMap <>(); private Paint myPaint ; private Paint textPaint ; public TouchView ( Context context ) { super ( context ); init (); } public TouchView ( Context context , AttributeSet attrs , int defStyle ) { super ( context , attrs , defStyle ); init (); } public TouchView ( Context context , AttributeSet attrs ) { super ( context , attrs ); init (); } private void init () { myPaint = new Paint (); myPaint . setStyle ( Paint . Style . FILL_AND_STROKE ); myPaint . setStrokeWidth ( 5 ); myPaint . setColor ( Color . RED ); textPaint = new Paint (); textPaint . setTextSize ( 50 ); } Now that everything is initialized and ready to draw some graphics on the screen so that the application is interactive, now we have to interface with touch events. This is done by creating a new function within our View class, that takes in a MotionEvent on the View so that we can detect different types of touch events. Documentation on this can be found ( https://developer.android.com/training/graphics/opengl/touch#java ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @SuppressLint ( \"ClickableViewAccessibility\" ) @Override public boolean onTouchEvent ( MotionEvent event ) { int action = event . getAction () & MotionEvent . ACTION_MASK ; switch ( action ) { case MotionEvent . ACTION_DOWN : { int id = event . getPointerId ( 0 ); fingerMap . put ( id , createCirPath ( event . getX (), event . getY (), id )); lineMap . put ( id , createFinger ( event . getX (), event . getY (), id )); break ; } case MotionEvent . ACTION_MOVE : { int touchCounter = event . getPointerCount (); for ( int t = 0 ; t < touchCounter ; t ++) { int id = event . getPointerId ( t ); fingerMap . remove ( id ); lineMap . remove ( id ); fingerMap . put ( id , createCirPath ( event . getX ( t ), event . getY ( t ), id )); lineMap . put ( id , createFinger ( event . getX ( t ), event . getY ( t ), id )); } } case MotionEvent . ACTION_POINTER_DOWN : { int id = event . getPointerId ( getIndex ( event )); fingerMap . put ( id , createCirPath ( event . getX ( getIndex ( event )), event . getY ( getIndex ( event )), getIndex ( event ))); lineMap . put ( id , createFinger ( event . getX ( getIndex ( event )), event . getY ( getIndex ( event )), getIndex ( event ))); break ; } case MotionEvent . ACTION_POINTER_UP : { int id = event . getPointerId ( getIndex ( event )); fingerMap . remove ( id ); lineMap . remove ( id ); break ; } case MotionEvent . ACTION_UP : { int id = event . getPointerId ( 0 ); fingerMap . remove ( id ); lineMap . remove ( id ); break ; } } invalidate (); return true ; } private int getIndex ( MotionEvent event ) { return ( event . getAction () & MotionEvent . ACTION_POINTER_INDEX_MASK ) >> MotionEvent . ACTION_POINTER_INDEX_SHIFT ; } private Finger createFinger ( float x , float y , int id ) { return new Finger ( x , y , id ); } Now that we've created a new Finger class inside our TreeMap by the order that the screen is touched in and we're removing that class when the screen input has been released, we are now ready to draw on the screen from our inputs. By iterating through the TreeMap, in each loop we know what the previous and what the next value in the array we can draw a circle for where the input is and a line between. This also allows us to determine whereabouts is the point in between these two points so we can write text. For this example, I've chosen to write the length of the distance between the two inputs to demonstrate that this can also be dynamic in nature. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 private Path createCirPath ( float x , float y , int id ) { Path p = new Path (); p . addCircle ( x , y , 50 , Path . Direction . CW ); return p ; } @Override protected void onDraw ( Canvas canvas ) { for ( Integer key : fingerMap . keySet ()) { Path p = fingerMap . get ( key ); canvas . drawPath ( p , myPaint ); } if ( lineMap . size () > 1 ) { Integer key = lineMap . firstKey (); for ( int i = 0 ; i < lineMap . size (); i = i + 1 ) { Finger start_fin = lineMap . get ( key ); if ( key + 1 != lineMap . size ()) { Integer new_key = lineMap . higherKey ( key ); Finger end_fin = lineMap . get ( new_key ); canvas . drawLine ( start_fin . x_pos , start_fin . y_pos , end_fin . x_pos , end_fin . y_pos , myPaint ); String lineText = \"Length: \" + new DecimalFormat ( \"#.##\" ). format ( Math . sqrt ( Math . pow ( end_fin . x_pos - start_fin . x_pos , 2 ) + Math . pow ( end_fin . y_pos - start_fin . y_pos , 2 ))); canvas . drawText ( lineText ,(( start_fin . x_pos + end_fin . x_pos ) / 2 ), (( start_fin . y_pos + end_fin . y_pos ) / 2 ), textPaint ); key = new_key ; } } } } In summary, it is quite simple to develop multi-touch interactions between the user and the application to enhance usability and interactivity. This is part of a application that I am developing at the moment and hope to share more insights into development as I progress on.","tags":"Android","url":"https://jackmckew.dev/episode-5-android-multi-touch.html","loc":"https://jackmckew.dev/episode-5-android-multi-touch.html"},{"title":"Episode 4 - Visualization","text":"In an ever growing world of data, every person perceives data in their own personalized way. This calls for data analysis to be visualized in a clear straightforward way so that it is accessible by anyone may come into contact with the system. By further making the data analysis system interactive, this adds an extreme amount of personalization to the analysis. Allowing the user to interact with the data set in their own way. With the help of python, it was simple to create an interactive map from a data set containing geographic co-ordinates allowing users to visually determine where they would like to select their data set from. This can then be embedding into any web browser or mobile device allowing for extreme flexibility and interactivity.","tags":"Python","url":"https://jackmckew.dev/episode-4-visualization.html","loc":"https://jackmckew.dev/episode-4-visualization.html"},{"title":"Episode 3 - Open Mind","text":"While it always may seem to be easiest to keep using what you've always used in the past, sometimes it pays off to keep an open mind about how you approach problems. Recently was asked to create a database with minute interval data from 600-700 data recording stations for up to the past 60 years, truly a lot of a data to handle. My first pass over was to use the python pandas module, with great success, however iterating over the data sets took around a week. By looking out for new ways to tackle problems, I was able to increase the speed 450 times faster by using dask to parallelize my data frames and multiprocessing allowing multiple workers to work across many cores of the PC. This meant going from around 60,000 rows per second to 1.5 million rows/second and 18 workers at one time. For the next version I am planning to investigate how to utilize influxDB and Apache Spark/Hadoop to try and optimize this process further.","tags":"Python","url":"https://jackmckew.dev/episode-3-open-mind.html","loc":"https://jackmckew.dev/episode-3-open-mind.html"},{"title":"Episode 2 - Kew-It","text":"Yesterday, I submitted my Electrical Engineering honours thesis. My project consisted of creating a hardware/software solution to schedule appliances in home to minimize energy costs through time of use pricing. The hardware is a \"black box\" that monitors power usage of appliances and logs this data through Wi-Fi to a database hosted locally. The software utilized an multi-objective evolutionary algorithm to then determine what the most beneficial time for each of the appliances to run. By using python for these computations, directly when the results are determined, a control strategy sends control messages back out to the \"black boxes\" to control the appliances automatically. By scheduling appliances in this manner, showed up to 50% reduction in cost of energy daily. As this can be scaled to any size of implementation, this project could show significant savings in cost of energy for any building/business. The project has an estimated payback period of 5 months, comparable to that of solar with 4-5 years.","tags":"Engineering","url":"https://jackmckew.dev/episode-2-kew-it.html","loc":"https://jackmckew.dev/episode-2-kew-it.html"},{"title":"Episode 1 - Optimization","text":"Recently I had to opportunity to optimize some workflows that involved heavy data processing, before the users were completing calculations/statistics by hand on up to 10 million rows in Excel, causing many complications (and crashes). With the use of Python this data analysis has been reduced to a matter of seconds speeding up workflows in some cases down from a whole working day to a matter of seconds allowing users to work on more important tasks and almost eliminating risk of human error.","tags":"Python","url":"https://jackmckew.dev/episode-1-optimization.html","loc":"https://jackmckew.dev/episode-1-optimization.html"}]};