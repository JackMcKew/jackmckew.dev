<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jack McKew's Blog - Software</title><link href="https://jackmckew.dev/" rel="alternate"></link><link href="https://jackmckew.dev/feeds/software.atom.xml" rel="self"></link><id>https://jackmckew.dev/</id><updated>2020-12-11T00:00:00+11:00</updated><subtitle>Engineer | Software Developer | Data Scientist</subtitle><entry><title>Releasing Cordova Apps on Google Play &amp; App Store</title><link href="https://jackmckew.dev/releasing-cordova-apps-on-google-play-app-store.html" rel="alternate"></link><published>2020-12-11T00:00:00+11:00</published><updated>2020-12-11T00:00:00+11:00</updated><author><name>Jack McKew</name></author><id>tag:jackmckew.dev,2020-12-11:/releasing-cordova-apps-on-google-play-app-store.html</id><summary type="html">&lt;body&gt;&lt;p&gt;This post is going to go into how to upload and release a Cordova app on both the Google Play Store and the Apple App Store. Cordova is an open source framework that wraps HTML/Javascript apps into a native container which can access the device's functionality, akin to a …&lt;/p&gt;&lt;/body&gt;</summary><content type="html">&lt;body&gt;&lt;p&gt;This post is going to go into how to upload and release a Cordova app on both the Google Play Store and the Apple App Store. Cordova is an open source framework that wraps HTML/Javascript apps into a native container which can access the device's functionality, akin to a 'borderless windowed' browser on a mobile device. The beauties of Cordova is it enables developers to have a single codebase which builds to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Android&lt;/li&gt;
&lt;li&gt;iOS&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While this post is not going to go into how to develop a Cordova app, and makes the assumption that we have a Cordova app ready to go to upload. Other assumptions that have been made that the developer has set up an account with the respective store (make note that to become an Apple Developer it's a $99 USD per year price and a Google Play developer is a $25 USD one-time fee).&lt;/p&gt;
&lt;h2 id="building-the-app"&gt;Building the App&lt;/h2&gt;
&lt;p&gt;There's thorough documentation on these steps found at: https://cordova.apache.org/docs/en/latest/guide/cli/&lt;/p&gt;
&lt;p&gt;Once cordova has been installed and configured to reflect the app, it's time to build the app in the respective target platforms (note to add platforms it's as easy as &lt;code&gt;cordova platform add android&lt;/code&gt;). If both platforms have been added (eg, iOS/Android), then running &lt;code&gt;cordova build&lt;/code&gt; will build for all platforms enabled. Once built, there will be folders found within &lt;code&gt;./platforms&lt;/code&gt; for each platform respectively in which there will be a:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Android Studio Project&lt;/li&gt;
&lt;li&gt;XCode Project (iOS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These can be opened within the native editors Android Studio and XCode, and will be required for the following steps. This is also a fantastic point to deploy the built app onto physical devices to test everything works as expected.&lt;/p&gt;
&lt;h2 id="google-play-store"&gt;Google Play Store&lt;/h2&gt;
&lt;p&gt;First of all, once your developer account is set up, head to &lt;a href="https://play.google.com/apps/publish"&gt;Google Play Console&lt;/a&gt;, as this will be the main point of contact for the app on the Google Play Store. Once you've successfully created your app on Google Play Console, it's now time to create a new release for your app. This is within &lt;code&gt;Production &amp;gt; Release Dashboard&lt;/code&gt;, and is time to fill in all the details such as release notes, etc. Under &lt;code&gt;Build&lt;/code&gt;, it should be a prompt to say 'Upload your files (.aab, etc)' and this is the file that we'll be creating in the next step.&lt;/p&gt;
&lt;h3 id="bundling-in-android-studio"&gt;Bundling in Android Studio&lt;/h3&gt;
&lt;p&gt;Provided the app built successfully, and tested fine on a physical device, it's now time to generate a signed bundle/apk. Open Android Studio and open the project found in &lt;code&gt;./platforms/android&lt;/code&gt;. Once open gradle will compile your app and have it ready to be deployed to a device, after gradle has finished running (and likely asking for updates), head to the &lt;code&gt;Build&lt;/code&gt; tab in the navigation bar, this is where we can find the option to &lt;code&gt;Generate Signed Bundle / APK&lt;/code&gt;, by clicking this will initiate the wizard.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Android Build Option" class="img-fluid" src="https://jackmckew.dev/img/android-build.png"/&gt;&lt;/p&gt;
&lt;p&gt;Now it's time to decide whether to bundle an Android App Bundle or an APK, and this will be specific for the project, but most likely going with Android App Bundle will be the better option.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Android App Bundle" class="img-fluid" src="https://jackmckew.dev/img/android-app-bundle.png"/&gt;&lt;/p&gt;
&lt;p&gt;Now it's time for the signing, and there's thorough documentation over on Android Developers (https://developer.android.com/studio/publish/app-signing) for going through these steps and covers all the scenarios such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New app to be released&lt;/li&gt;
&lt;li&gt;Existing app to be updated&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After following these steps an &lt;code&gt;.aab&lt;/code&gt; file will be generated for your app which can then be uploaded into Google Play Console and thereafter released!&lt;/p&gt;
&lt;h2 id="app-store-ios"&gt;App Store (iOS)&lt;/h2&gt;
&lt;p&gt;Once you've signed up for the Apple developer program, head to &lt;a href="https://appstoreconnect.apple.com/"&gt;App Store Connect&lt;/a&gt; and create a new app on the platform. This will be the main point of contact for the app on the Apple App Store. Once set up, it's time to create a new release for the app, which is the blue plus button underneath the app name. This will prompt you to fill in details such as release notes, etc and select a build. But likely, there won't be any builds to select from, which is a problem, so let's solve that.&lt;/p&gt;
&lt;h3 id="uploading-the-build-to-app-store-connect"&gt;Uploading the Build to App Store Connect&lt;/h3&gt;
&lt;p&gt;Now there's many ways to achieve this (eg, &lt;a href="https://apps.apple.com/us/app/transporter/id1450874784?mt=12"&gt;Transporter&lt;/a&gt;, XCode, etc). But let's focus on uploading it through the Archive process within XCode which I've personally found to be the most straightforward and simplest way. This means we'll need to open XCode and open the project file found in &lt;code&gt;./platforms/ios/APP_NAME.xcodeproj&lt;/code&gt;. This will likely ask you to update many things as XCode is consistently getting updates. Note that once opened, there will be 3 potential target 'Schemes' in XCode which is directly next to the run button in the top left. Those 3 schemes will be Cordova, CordovaLib &amp;amp; Your App, ensure to have your app selected as the scheme before moving forward. This is the perfect time to deploy your app to a physical device and test all functionality.&lt;/p&gt;
&lt;p&gt;Provided the app has built and deployed to a device (whether simulated or physical), it's time to get it sent to App Store Connect ready for release. Ensuring that you've selected the target device as &lt;code&gt;Any iOS Device (arm64)&lt;/code&gt;, this will enable us to have the &lt;code&gt;Archive&lt;/code&gt; option in the &lt;code&gt;Product&lt;/code&gt; tab of the navigation menu.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Target iOS Device" class="img-fluid" src="https://jackmckew.dev/img/ios-device.png"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Archive Option" class="img-fluid" src="https://jackmckew.dev/img/ios-archive.png"/&gt;&lt;/p&gt;
&lt;p&gt;Upon archiving the project, this will build the project and bundle it with whichever developer certificates you've enabled with your XCode account. Finally opening &lt;code&gt;Organizer&lt;/code&gt; when finished (and can be found within &lt;code&gt;Window &amp;gt; Organizer&lt;/code&gt;) containing all previously built versions of this project. By then selecting the version which was archived, we can then send this straight to App Store Connect via the &lt;code&gt;Distribute App&lt;/code&gt; button.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Distribute Option" class="img-fluid" src="https://jackmckew.dev/img/ios-distribute.png"/&gt;&lt;/p&gt;
&lt;p&gt;Once clicking this option, it'll go through the process of validating all certificates and such, and send it straight to your Apple Developer account. Now's the time to head back to App Store Connect (after some time), for which that select build button will now have an option and allow you to submit your app to the Apple App Store!&lt;/p&gt;&lt;/body&gt;</content><category term="Software"></category><category term="Software"></category><category term="javascript"></category></entry><entry><title>Deploy a Node Web App to AWS Elastic Beanstalk with Docker</title><link href="https://jackmckew.dev/deploy-a-node-web-app-to-aws-elastic-beanstalk-with-docker.html" rel="alternate"></link><published>2020-12-04T00:00:00+11:00</published><updated>2020-12-04T00:00:00+11:00</updated><author><name>Jack McKew</name></author><id>tag:jackmckew.dev,2020-12-04:/deploy-a-node-web-app-to-aws-elastic-beanstalk-with-docker.html</id><summary type="html">&lt;body&gt;&lt;p&gt;We've gone through how to use Docker to help develop our web applications, now we want to be able to deploy them out in the wild. Let's use Amazon Web Services (AWS) Elastic Beanstalk to do this. Note that there is a free tier of AWS that we can make …&lt;/p&gt;&lt;/body&gt;</summary><content type="html">&lt;body&gt;&lt;p&gt;We've gone through how to use Docker to help develop our web applications, now we want to be able to deploy them out in the wild. Let's use Amazon Web Services (AWS) Elastic Beanstalk to do this. Note that there is a free tier of AWS that we can make use of! We will also be making use of GitHub actions to automate the CI/CD, in which it'll build the Docker container to test our web application, and then deploy it to AWS automatically.&lt;/p&gt;
&lt;p&gt;Let's deploy the application we built in a previous post &lt;a href="https://jackmckew.dev/develop-and-deploy-with-docker.html"&gt;Develop and Deploy with Docker&lt;/a&gt;. It's the default output from &lt;code&gt;create-react-app&lt;/code&gt;, but we can further develop this and it'll update as soon as we push to the repository. This post assumes that we've already set up the &lt;code&gt;create-react-app&lt;/code&gt; and dockerized it as such in the previous post.&lt;/p&gt;
&lt;p&gt;This post is apart of a series on Docker/Kubernetes, find the other posts at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/intro-to-docker.html"&gt;Intro to Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-with-docker.html"&gt;Develop and Develop with Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-multi-container-applications.html"&gt;Develop and Develop Multi Container Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/intro-to-kubernetes.html"&gt;Intro to Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/developing-with-kubernetes.html"&gt;Developing with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/deploying-with-kubernetes.html"&gt;Deploying with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="github-action"&gt;GitHub Action&lt;/h2&gt;
&lt;p&gt;Let's begin by setting up the CI/CD workflow in GitHub Actions. We create a yml file in our repository under &lt;code&gt;.github/workflows/build-docker.yml&lt;/code&gt;. To step through the actions we want to do each time a new version is pushed into our repository are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clone the latest version of the repository&lt;/li&gt;
&lt;li&gt;Build the development Docker container&lt;/li&gt;
&lt;li&gt;Execute tests on our web app and fail if there's any failing tests&lt;/li&gt;
&lt;li&gt;Generate a packaged version to deploy&lt;/li&gt;
&lt;li&gt;Deploy to AWS&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the most part, we will be making use of &lt;code&gt;run&lt;/code&gt; commands, as if we are interacting with the terminal in the runtime of ubuntu (Linux). Otherwise, we can make use of pre-made actions from the marketplace. One note to be made is that the AWS Elastic Beanstalk application has been set up to run specifically on Docker, and as such we need to upload the relevant Dockerfile (production) along with any assets.&lt;/p&gt;
&lt;p&gt;The contents of the Github Action in whole will be:&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Test &amp;amp; Deploy&lt;/span&gt;
&lt;span class="nt"&gt;on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;push&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;branches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;master&lt;/span&gt;

&lt;span class="nt"&gt;jobs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;test-and-deploy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;runs-on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class="nt"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Checkout Latest Repo&lt;/span&gt;
        &lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;actions/checkout@master&lt;/span&gt;

      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Build Dev Docker Image&lt;/span&gt;
        &lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;docker build -t jackmckew/docker-react-dev -f Dockerfile.dev .&lt;/span&gt;

      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Run Test Suite&lt;/span&gt;
        &lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;docker run -e CI=true jackmckew/docker-react-dev npm run test -- --coverage&lt;/span&gt;

      &lt;span class="c1"&gt;# Zip Dockerfile for upload&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate Deployment Package&lt;/span&gt;
        &lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;zip -r deploy.zip * -x "**node_modules**"&lt;/span&gt;

      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Get Timestamp&lt;/span&gt;
        &lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;gerred/actions/current-time@master&lt;/span&gt;
        &lt;span class="nt"&gt;id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;current-time&lt;/span&gt;

      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Run String Replace&lt;/span&gt;
        &lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;frabert/replace-string-action@master&lt;/span&gt;
        &lt;span class="nt"&gt;id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;format-time&lt;/span&gt;
        &lt;span class="nt"&gt;with&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="nt"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;'[:\.]+'&lt;/span&gt;
          &lt;span class="nt"&gt;string&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"${{&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;steps.current-time.outputs.time&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;}}"&lt;/span&gt;
          &lt;span class="nt"&gt;replace-with&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"-"&lt;/span&gt;
          &lt;span class="nt"&gt;flags&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"g"&lt;/span&gt;

      &lt;span class="c1"&gt;# Deploy to AWS&lt;/span&gt;
      &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install Dependencies &amp;amp; Deploy to AWS&lt;/span&gt;
        &lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
          &lt;span class="no"&gt;sudo npm install -g beanstalk-deploy --unsafe-perm&lt;/span&gt;
          &lt;span class="no"&gt;sudo AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID}} AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY}} beanstalk-deploy "docker-react" "DockerReact-env-1" "docker-react-${{ steps.format-time.outputs.replaced }}" "us-east-2" deploy.zip&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;There is an action for &lt;a href="https://github.com/marketplace/actions/beanstalk-deploy"&gt;beanstalk-deploy&lt;/a&gt;, although it didn't work properly, and as such the workaround is to use the npm package on it's own.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="aws-elastic-beanstalk"&gt;AWS Elastic Beanstalk&lt;/h2&gt;
&lt;p&gt;Next up we need to set up our instance of Elastic Beanstalk on AWS. We need to complete a few steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a new application &amp;amp; environment in AWS Elastic Beanstalk&lt;/li&gt;
&lt;li&gt;Create API keys for our GitHub Action (these go in as secrets)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Since the previous post relies upon a multi-stage Dockerfile to build the app and run the app within nginx, we must ensure to use the platform &lt;code&gt;Docker running on 64bit Amazon Linux/2.15.2&lt;/code&gt;, as &lt;code&gt;Docker running on Amazon Linux 2&lt;/code&gt;, &lt;a href="https://stackoverflow.com/questions/61462646/unable-to-deploy-docker-application-in-elasticbeanstalk-using-travis-ci"&gt;does not support multi-stage Dockerfiles&lt;/a&gt;. Furthermore, we exposed the ports in the Dockerfile through docker-compose or the Docker CLI previously, we can also do this by adding the command &lt;code&gt;EXPOSE 80&lt;/code&gt; in the production Dockerfile.&lt;/p&gt;
&lt;p&gt;Once we've set the application, and the keys as secrets, we are now able to push into our repository, and this will update our application on AWS Elastic Beanstalk.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is a production grade workflow for developing web applications in React and deploying to AWS. Find the complete repository of this post over at: &lt;a href="https://github.com/JackMcKew/docker-react"&gt;https://github.com/JackMcKew/docker-react&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;</content><category term="Software"></category><category term="software"></category></entry><entry><title>Deploying with Kubernetes</title><link href="https://jackmckew.dev/deploying-with-kubernetes.html" rel="alternate"></link><published>2020-11-26T00:00:00+11:00</published><updated>2020-11-26T00:00:00+11:00</updated><author><name>Jack McKew</name></author><id>tag:jackmckew.dev,2020-11-26:/deploying-with-kubernetes.html</id><summary type="html">&lt;body&gt;&lt;p&gt;Following on with previous posts on this blog. This post will be going through how to deploy our fibonacci application we previously built in a Kubernetes cluster. To reiterate we will be using the following technologies:&lt;/p&gt;
&lt;table class="table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Docker&lt;/td&gt;
&lt;td&gt;Docker will be used to containerization of the services inside our …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;</summary><content type="html">&lt;body&gt;&lt;p&gt;Following on with previous posts on this blog. This post will be going through how to deploy our fibonacci application we previously built in a Kubernetes cluster. To reiterate we will be using the following technologies:&lt;/p&gt;
&lt;table class="table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Docker&lt;/td&gt;
&lt;td&gt;Docker will be used to containerization of the services inside our app&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vue&lt;/td&gt;
&lt;td&gt;Vue is the front-end JavaScript framework that we will use&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Google Cloud&lt;/td&gt;
&lt;td&gt;The cloud provider that will host our Kubernetes cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Express&lt;/td&gt;
&lt;td&gt;Express is responsible for the API between Redis, PostgreSQL and Vue&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Redis&lt;/td&gt;
&lt;td&gt;Redis will store/retrieve any local data used by our users&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PostgreSQL&lt;/td&gt;
&lt;td&gt;PostgreSQL will be our database&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nginx&lt;/td&gt;
&lt;td&gt;Nginx will handle the routing between our services&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Github Actions&lt;/td&gt;
&lt;td&gt;The CI/CD for our project&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This post is apart of a series on Docker/Kubernetes, find the other posts at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/intro-to-docker.html"&gt;Intro to Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-with-docker.html"&gt;Develop and Develop with Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-multi-container-applications.html"&gt;Develop and Develop Multi Container Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/intro-to-kubernetes.html"&gt;Intro to Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/developing-with-kubernetes.html"&gt;Developing with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/deploy-a-node-web-app-to-aws-elastic-beanstalk-with-docker.html"&gt;Deploy a Node Web App to AWS Elastic Beanstalk with Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Previously we made use of services provided by AWS for Redis &amp;amp; PostgreSQL, in this post these services will be run inside their own pods.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="the-architecture"&gt;The Architecture&lt;/h2&gt;
&lt;p&gt;To make our application run on Kubernetes, we need to make a few changes. In essence though the architecture will&lt;/p&gt;
&lt;div class="mermaid"&gt;
  graph LR
    subgraph Node
        tr&amp;gt;Traffic] --&amp;gt; is[Ingress Service]

        is --&amp;gt; cl[Client Deployment]

        is --&amp;gt; se[Server Deployment]

        se --&amp;gt; re[(Redis Deployment)]

        se --&amp;gt; po[(PostgreSQL Deployment)]

        po --&amp;gt; pv[PostgreSQL PVC]

        wo(Worker Deployment) --&amp;gt; re
    end
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;The above chart was made with &lt;a href="https://mermaid-js.github.io/mermaid/#/"&gt;mermaid.js&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;See the following posts for developing in different contexts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Container context: &lt;a href="https://jackmckew.dev/develop-and-deploy-multi-container-applications.html"&gt;Develop and Develop Multi Container Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kubernetes context: &lt;a href="https://jackmckew.dev/developing-with-kubernetes.html"&gt;Developing with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cloud-provider"&gt;Cloud Provider&lt;/h2&gt;
&lt;p&gt;To deploy our Kubernetes cluster, we need a cloud provider, for this post we will be using Google Cloud. Other options are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services (AWS)&lt;/li&gt;
&lt;li&gt;Digital Ocean&lt;/li&gt;
&lt;li&gt;Microsoft Azure&lt;/li&gt;
&lt;li&gt;Many more&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="google-cloud"&gt;Google Cloud&lt;/h2&gt;
&lt;p&gt;Now that we've logged into our Google Cloud console, we create a new project, enable billing and then head to Kubernetes Engine. This will go through some one time set up to enable the Kubernetes Engine API for our project and upon completion we can create a cluster. As soon as we hit &lt;code&gt;Create Cluster&lt;/code&gt;, this begins the billing so be careful!&lt;/p&gt;
&lt;h3 id="service-account"&gt;Service Account&lt;/h3&gt;
&lt;p&gt;Before moving onto the next step, we need to authorize Github Actions to be able run inside the cluster on Google Cloud. This is done by creating a service account, exporting the keys as JSON and storing it as a secret within the repository on Github.&lt;/p&gt;
&lt;h2 id="cicd"&gt;CI/CD&lt;/h2&gt;
&lt;p&gt;For our CI/CD we will be using GitHub Actions. Github Actions are free to public repositories, and I presented at PyconAU 2020 on the topic, find the recording at: &lt;a href="https://www.youtube.com/watch?v=7aBjzZkaGhU&amp;amp;feature=emb_logo"&gt;https://www.youtube.com/watch?v=7aBjzZkaGhU&amp;amp;feature=emb_logo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's get into setting up the action for this project, we will be aiming to achieve a few things in this action:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialise &amp;amp; configure the Google Cloud SDK&lt;/li&gt;
&lt;li&gt;Login to Docker Hub so we can push our images up&lt;/li&gt;
&lt;li&gt;Build our application and run tests&lt;/li&gt;
&lt;li&gt;Build and tag images ready to be pushed to Docker Hub&lt;/li&gt;
&lt;li&gt;Push our tagged images to Docker Hub&lt;/li&gt;
&lt;li&gt;Update the Kubernetes configuration on Google Cloud&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To see the full configuration which achieves this head to: &lt;a href="https://github.com/JackMcKew/multi-docker/blob/master/.github/workflows/google-cloud-cluster.yaml"&gt;https://github.com/JackMcKew/multi-docker/blob/master/.github/workflows/google-cloud-cluster.yaml&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We make sure to use the &lt;code&gt;${GITHUB_SHA}&lt;/code&gt;, when we're tagging our images versions, this is due to ensure that our kubernetes cluster ensures to update the images when we are running &lt;code&gt;kubectl apply -f k8s&lt;/code&gt;. If we didn't add the SHA, they would be all tagged with &lt;code&gt;:latest&lt;/code&gt;, and kubernetes wouldn't see an update.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Make sure to update the cluster id in the command &lt;code&gt;gcloud container clusters get-credentials my-first-cluster-1&lt;/code&gt; with the cluster desired to update the kubernetes configuration, otherwise there'll be errors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once this action has successfully completed, we can head to the Workloads page of our project and we should be able to see all the deployment configurations that we set up to be deployed.&lt;/p&gt;
&lt;h2 id="setting-up-ingress-nginx"&gt;Setting up Ingress-Nginx&lt;/h2&gt;
&lt;p&gt;Before we can access our application through an IP or web address, we need to set up &lt;code&gt;ingress-nginx&lt;/code&gt;, similar to how we did with &lt;code&gt;docker-compose&lt;/code&gt; in previous posts. Luckily, we can make use of &lt;code&gt;helm&lt;/code&gt; to add this functionality for us (provided we'd set up nginx configuration like we already have). This can be done by sshing into the terminal of our Kubernetes cluster, or similarly making use of the Cloud Shell provided by Google Cloud.&lt;/p&gt;
&lt;p&gt;Firstly which we need to install helm (&lt;a href="https://helm.sh/docs/intro/install/#from-script"&gt;https://helm.sh/docs/intro/install/#from-script&lt;/a&gt;):&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod &lt;span class="m"&gt;700&lt;/span&gt; get_helm.sh
./get_helm.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Followed by setting up &lt;code&gt;ingress-nginx&lt;/code&gt; (&lt;a href="https://kubernetes.github.io/ingress-nginx/deploy/#using-helm"&gt;https://kubernetes.github.io/ingress-nginx/deploy/#using-helm&lt;/a&gt;):&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm install my-release ingress-nginx/ingress-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id="success"&gt;Success&lt;/h2&gt;
&lt;p&gt;Now we head to our &lt;code&gt;Services &amp;amp; Ingress&lt;/code&gt; page in our project, where we can see all the pods that are used for hosting endpoints. Provided the &lt;code&gt;ingress-nginx&lt;/code&gt; service has been created, there should be a &lt;code&gt;External Load Balancer&lt;/code&gt; service with an IP, that we can access. Heading to this IP will lead us to our application!&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kubernetes In Action on Google Cloud" class="img-fluid" src="https://jackmckew.dev/img/google-cloud-kubernetes.gif"/&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If we wanted to set up an actual web address, we'd need to purchase a domain, set the A record as the IP for our external load balancer, and finally set up a certificate manager to handle the https authentication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Javascript Source(s):
&lt;a href="https://jackmckew.dev/js/mermaid.min.js"&gt;mermaid.min.js&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;</content><category term="Software"></category><category term="software"></category></entry><entry><title>Developing with Kubernetes</title><link href="https://jackmckew.dev/developing-with-kubernetes.html" rel="alternate"></link><published>2020-11-20T00:00:00+11:00</published><updated>2020-11-20T00:00:00+11:00</updated><author><name>Jack McKew</name></author><id>tag:jackmckew.dev,2020-11-20:/developing-with-kubernetes.html</id><summary type="html">&lt;body&gt;&lt;p&gt;Following on with previous posts on this blog. This post will be going through how to develop &amp;amp; later on deploy our fibonacci application we previously built in a multi-container context. To reiterate we will be using the following technologies:&lt;/p&gt;
&lt;table class="table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Docker&lt;/td&gt;
&lt;td&gt;Docker will be used to containerization of the …&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;</summary><content type="html">&lt;body&gt;&lt;p&gt;Following on with previous posts on this blog. This post will be going through how to develop &amp;amp; later on deploy our fibonacci application we previously built in a multi-container context. To reiterate we will be using the following technologies:&lt;/p&gt;
&lt;table class="table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Technology&lt;/th&gt;
&lt;th&gt;Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Docker&lt;/td&gt;
&lt;td&gt;Docker will be used to containerization of the services inside our app&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vue&lt;/td&gt;
&lt;td&gt;Vue is the front-end JavaScript framework that we will use&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Express&lt;/td&gt;
&lt;td&gt;Express is responsible for the API between Redis, PostgreSQL and Vue&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Redis&lt;/td&gt;
&lt;td&gt;Redis will store/retrieve any local data used by our users&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PostgreSQL&lt;/td&gt;
&lt;td&gt;PostgreSQL will be our database&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nginx&lt;/td&gt;
&lt;td&gt;Nginx will handle the routing between our services&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;Previously we made use of services provided by AWS for Redis &amp;amp; PostgreSQL, in this post these services will be run inside their own pods.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This post is apart of a series on Docker/Kubernetes, find the other posts at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/intro-to-docker.html"&gt;Intro to Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-with-docker.html"&gt;Develop and Develop with Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-multi-container-applications.html"&gt;Develop and Develop Multi Container Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/intro-to-kubernetes.html"&gt;Intro to Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/developing-with-kubernetes.html"&gt;Developing with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/deploying-with-kubernetes.html"&gt;Deploying with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/deploy-a-node-web-app-to-aws-elastic-beanstalk-with-docker.html"&gt;Deploy a Node Web App to AWS Elastic Beanstalk with Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="the-architecture"&gt;The Architecture&lt;/h2&gt;
&lt;p&gt;To make our application run on Kubernetes, we need to make a few changes. In essence though the architecture will&lt;/p&gt;
&lt;div class="mermaid"&gt;
  graph LR
    subgraph Node
        tr&amp;gt;Traffic] --&amp;gt; is[Ingress Service]

        is --&amp;gt; cl[Client Deployment]

        is --&amp;gt; se[Server Deployment]

        se --&amp;gt; re[(Redis Deployment)]

        se --&amp;gt; po[(PostgreSQL Deployment)]

        po --&amp;gt; pv[PostgreSQL PVC]

        wo(Worker Deployment) --&amp;gt; re
    end
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;The above chart was made with &lt;a href="https://mermaid-js.github.io/mermaid/#/"&gt;mermaid.js&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For each of the deployments (except the worker) we will be creating a ClusterIP service for maintaining the connection between each of the deployments. The PostgreSQL PVC is for a Persistent Volume Claim, which allows our node to consume abstract storage resources (we'll go into this further later on).&lt;/p&gt;
&lt;h2 id="clusterip-service"&gt;ClusterIP Service&lt;/h2&gt;
&lt;p&gt;We need to set up a ClusterIP service for each of our deployments except the worker deployment. This will allow our services to communicate with others inside the node.&lt;/p&gt;
&lt;p&gt;To do this, we create a configuration &lt;code&gt;yaml&lt;/code&gt; file:&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;v1&lt;/span&gt;
&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Service&lt;/span&gt;
&lt;span class="nt"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;client-clusterip-service&lt;/span&gt;
&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;selector&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;component&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;web&lt;/span&gt;
  &lt;span class="nt"&gt;ports&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;port&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;80&lt;/span&gt;
      &lt;span class="nt"&gt;targetPort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Note that we keep the same selector as our deployments.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ClusterIP is the default ServiceType in kubernetes.
We can create multiple objects in a single yaml file by separating with &lt;code&gt;---&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="persistent-volume-claim"&gt;Persistent Volume Claim&lt;/h2&gt;
&lt;p&gt;A persistent volume allows a pod to share memory and read/write data on the host PC. The use-case for this are if our PostgreSQL database pod had crashed without a PVC, the data would essentially be lost as it was entirely contained within the pod, but with a persistent volume claim, our pod can restart by using the data that is stored on the host PC.&lt;/p&gt;
&lt;p&gt;We use a PVC over a persistent volume or a volume, as this allows us to declare the requirement that our pod needs storage at some point, rather than create an instance of storage prematurely, and gives more control to Kubernetes to solve our storage problem for us.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Volumes on their lonesome are tied directly to pods, and thus if the pod crashes, the volume will be lost. Hence why we are not using volumes in this case. A volume is also different between Kubernetes and Docker.
A &lt;em&gt;persistant&lt;/em&gt; volume is not tied directly to pods, but is tied to the node overall, and thus if the node as a whole crashes, the data will be lost.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="pvc-configuration"&gt;PVC Configuration&lt;/h3&gt;
&lt;p&gt;Similar to how we attach a ClusterIP service to a pod, let's attach a PVC to a pod. What this will do, will instruct Kubernetes to 'advertise' storage space for pods to consume. If a pod consumes this claim, then it'll go and create a persistent volume or point to an already created persistent volume for us automatically. There is also many access modes we can define for our PVC:&lt;/p&gt;
&lt;table class="table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Access Mode&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ReadWriteOnce&lt;/td&gt;
&lt;td&gt;Can be used by a single node&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ReadOnlyMany&lt;/td&gt;
&lt;td&gt;Multiple nodes can &lt;strong&gt;read&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ReadWriteMany&lt;/td&gt;
&lt;td&gt;Can be &lt;strong&gt;read&lt;/strong&gt;/&lt;strong&gt;written to&lt;/strong&gt; by many nodes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;v1&lt;/span&gt;
&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span class="nt"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;database-persistent-volume-claim&lt;/span&gt;
&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;accessModes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span class="nt"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;requests&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="nt"&gt;storage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;2Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;This configuration will allow a single node access to 2 gigabytes of storage space available for both read &amp;amp; write operations.&lt;/p&gt;
&lt;p&gt;Ensure that the pods that will access this volume claim have provided it under the &lt;code&gt;spec&lt;/code&gt; tag in the pod configuration.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Similarly, we can specify resource requirements/restraints in pods (eg, CPU resources).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="environment-variables"&gt;Environment Variables&lt;/h2&gt;
&lt;p&gt;Some of our pods depend on environment variables being set to work correctly (eg, REDIS_HOST, PGUSER, etc). We add using the &lt;code&gt;env&lt;/code&gt; key to our &lt;code&gt;spec&lt;/code&gt; &amp;gt; &lt;code&gt;containers&lt;/code&gt; configuration.&lt;/p&gt;
&lt;p&gt;For example, for our worker to connect to the redis deployment:&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8
9&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;containers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;worker&lt;/span&gt;
      &lt;span class="nt"&gt;image&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;jackmckew/multi-docker-worker&lt;/span&gt;
      &lt;span class="nt"&gt;env&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;REDIS_HOST&lt;/span&gt;
          &lt;span class="nt"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;redis-cluster-ip-service&lt;/span&gt;
        &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;REDIS_PORT&lt;/span&gt;
          &lt;span class="nt"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;6379&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Note that for the value of the &lt;code&gt;REDIS_HOST&lt;/code&gt; we are stating the name of the ClusterIP service we had previously set up. Kubernetes will automatically resolve this for us to be the correct IP, how neat!&lt;/p&gt;
&lt;h3 id="secrets"&gt;Secrets&lt;/h3&gt;
&lt;p&gt;Secrets are another type of object inside of Kubernetes that are used to store sensitive information we don't want to live in the plain text of the configuration files. We do this through a &lt;code&gt;kubectl&lt;/code&gt; commad:&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;kubectl create secret &lt;span class="o"&gt;[&lt;/span&gt;secret_type&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;secret_name&lt;span class="o"&gt;]&lt;/span&gt; --from-literal &lt;span class="nv"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;value
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;There are 3 types of secret types, &lt;code&gt;generic&lt;/code&gt;, &lt;code&gt;docker_registry&lt;/code&gt; and &lt;code&gt;tls&lt;/code&gt;, most of the time we'll be making use of the &lt;code&gt;generic&lt;/code&gt; secret type. Similar to how we consume other services, we will be consuming the secret from the &lt;code&gt;secret_name&lt;/code&gt; parameter. The names (but not the value) can always be retrieved through &lt;code&gt;kubectl get secrets&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Secrets are pertained only on the current machine, so this will not be transferred when moving to production or another machine, so be sure to repeat the process.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="consuming-secrets-as-environment-variable"&gt;Consuming Secrets as Environment Variable&lt;/h3&gt;
&lt;p&gt;Consuming a secret as an environment variable for a container is a little different to other environment variables. As secrets can contain multiple key value pairs, we need to specify the secret and the key to retrieve the value from:&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ENVIRONMENT_VAR_NAME&lt;/span&gt;
  &lt;span class="nt"&gt;valueFrom&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;secretKeyRef&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;secret_name&lt;/span&gt;
      &lt;span class="nt"&gt;key&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;h2 id="ingress-service"&gt;Ingress Service&lt;/h2&gt;
&lt;p&gt;The ingress service allows us to connect to other Kubernetes cluster from outside, and thus maintains how we should treat incoming requests and how to route them.&lt;/p&gt;
&lt;p&gt;The entirety of our configuration for the ingress service is:&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;apiVersion&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;networking.k8s.io/v1beta1&lt;/span&gt;
&lt;span class="nt"&gt;kind&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;Ingress&lt;/span&gt;
&lt;span class="nt"&gt;metadata&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;ingress-service&lt;/span&gt;
  &lt;span class="nt"&gt;annotations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;kubernetes.io/ingress.class&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;nginx&lt;/span&gt;
    &lt;span class="nt"&gt;nginx.ingress.kubernetes.io/use-regex&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;"true"&lt;/span&gt;
    &lt;span class="nt"&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/$1&lt;/span&gt;
&lt;span class="nt"&gt;spec&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nt"&gt;rules&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nt"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/?(.*)&lt;/span&gt;
            &lt;span class="nt"&gt;backend&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
              &lt;span class="nt"&gt;serviceName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;client-cluster-ip-service&lt;/span&gt;
              &lt;span class="nt"&gt;servicePort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;8080&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/fib/?(.*)&lt;/span&gt;
            &lt;span class="nt"&gt;backend&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
              &lt;span class="nt"&gt;serviceName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;client-cluster-ip-service&lt;/span&gt;
              &lt;span class="nt"&gt;servicePort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;8080&lt;/span&gt;
          &lt;span class="p p-Indicator"&gt;-&lt;/span&gt; &lt;span class="nt"&gt;path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;/api/?(.*)&lt;/span&gt;
            &lt;span class="nt"&gt;backend&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
              &lt;span class="nt"&gt;serviceName&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;server-cluster-ip-service&lt;/span&gt;
              &lt;span class="nt"&gt;servicePort&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="l l-Scalar l-Scalar-Plain"&gt;5000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Note that we make use of rewrite-target, this means that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;test.com/something&lt;/code&gt; rewrites to &lt;code&gt;test.com/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.com/somethingelse&lt;/code&gt; rewrites to &lt;code&gt;test.com/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.com/fib&lt;/code&gt; rewrites to &lt;code&gt;test.com/fib/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any requests for &lt;code&gt;test.com/api&lt;/code&gt; get routed to the specific server service for handling, while any others get sent to the front end.&lt;/p&gt;
&lt;h2 id="deploy"&gt;Deploy!&lt;/h2&gt;
&lt;p&gt;Now we are ready to deploy our Kubernetes cluster onto a cloud provider, this was originally detailed in this part of the post, but grew far longer than expected so another post was created!&lt;/p&gt;
&lt;p&gt;Javascript Source(s):
&lt;a href="https://jackmckew.dev/js/mermaid.min.js"&gt;mermaid.min.js&lt;/a&gt;&lt;/p&gt;&lt;/body&gt;</content><category term="Software"></category><category term="software"></category></entry><entry><title>Intro to Kubernetes</title><link href="https://jackmckew.dev/intro-to-kubernetes.html" rel="alternate"></link><published>2020-11-13T00:00:00+11:00</published><updated>2020-11-13T00:00:00+11:00</updated><author><name>Jack McKew</name></author><id>tag:jackmckew.dev,2020-11-13:/intro-to-kubernetes.html</id><summary type="html">&lt;body&gt;&lt;p&gt;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. We use Kubernetes as a platform for orchestrating multiple Docker containers for our application, and enables us to scale our application easily.&lt;/p&gt;
&lt;p&gt;Kubernetes is managed via a master node, and …&lt;/p&gt;&lt;/body&gt;</summary><content type="html">&lt;body&gt;&lt;p&gt;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. We use Kubernetes as a platform for orchestrating multiple Docker containers for our application, and enables us to scale our application easily.&lt;/p&gt;
&lt;p&gt;Kubernetes is managed via a master node, and worker nodes, in combination we call this a cluster. We give instructions to the master node on how we want the cluster to run, and how many workers we need.&lt;/p&gt;
&lt;p&gt;This post is apart of a series on Docker/Kubernetes, find the other posts at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/intro-to-docker.html"&gt;Intro to Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-with-docker.html"&gt;Develop and Develop with Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/develop-and-deploy-multi-container-applications.html"&gt;Develop and Develop Multi Container Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/developing-with-kubernetes.html"&gt;Developing with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/deploying-with-kubernetes.html"&gt;Deploying with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jackmckew.dev/deploy-a-node-web-app-to-aws-elastic-beanstalk-with-docker.html"&gt;Deploy a Node Web App to AWS Elastic Beanstalk with Docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Kubernetes Logo" class="img-fluid" src="https://jackmckew.dev/img/Kubernetes-logo.png"/&gt;&lt;/p&gt;
&lt;h2 id="minikube"&gt;Minikube&lt;/h2&gt;
&lt;p&gt;Minikube is a way of running a development cluster on our local PC. When running in production however, we use managed services offered by different platforms (eg, AWS, GCP, etc). To interact with &lt;code&gt;minikube&lt;/code&gt; as it is running though, we use another tool called &lt;code&gt;kubectl&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="docker-compose-to-kubernetes"&gt;Docker Compose to Kubernetes&lt;/h2&gt;
&lt;table class="table-striped table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Docker Compose Terminology&lt;/th&gt;
&lt;th&gt;Kubernetes Terminology&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Each entry could build an image&lt;/td&gt;
&lt;td&gt;Kubernetes expects all images to be built&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Each entry represents a container&lt;/td&gt;
&lt;td&gt;One config file per object we want to create&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Each entry defines the networking configuration (ports)&lt;/td&gt;
&lt;td&gt;We have to set up all networking manually&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="whats-an-object"&gt;What's an Object?&lt;/h3&gt;
&lt;p&gt;Notice that we mentioned objects as the equivalent in Kubernetes, but what does this mean? Objects serve different purposes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Running a container&lt;/li&gt;
&lt;li&gt;Monitoring a container&lt;/li&gt;
&lt;li&gt;Setting up networking&lt;/li&gt;
&lt;li&gt;etc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example object types include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;StatefulSet&lt;/li&gt;
&lt;li&gt;ReplicaController&lt;/li&gt;
&lt;li&gt;Pod&lt;/li&gt;
&lt;li&gt;Service&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;There are multiple API versions which gives us access to a different set of object types&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id="pods"&gt;Pods&lt;/h4&gt;
&lt;p&gt;Pods let us run containers within nodes. These are one of the most basic objects we can create within Kubernetes. Typically we only put containers that are tightly coupled together within a pod. For example, we might run a database pod which is comprised of 3 containers, the database runtime, a logger and a backup manager. Since if any of these are solely dependant on other containers running, it makes sense to group them together in a pod.&lt;/p&gt;
&lt;h4 id="services"&gt;Services&lt;/h4&gt;
&lt;p&gt;Services let us set up networking within a Kubernetes cluster. There is also 4 sub-types of services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP&lt;/li&gt;
&lt;li&gt;NodePort&lt;/li&gt;
&lt;li&gt;Ingress&lt;/li&gt;
&lt;li&gt;LoadBalancer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NodePort services allow us to expose a container to the outside network (only for development purposes). We can use selectors and labels to be the equivalent of our service names in docker-compose.&lt;/p&gt;
&lt;h4 id="deployment"&gt;Deployment&lt;/h4&gt;
&lt;p&gt;The deployment object type is better for running groups of identical pods, as the master can manage all the changes &amp;amp; updates for our pods for us (see below for limitations when using pods alone).&lt;/p&gt;
&lt;p&gt;Similar to the pod yaml file, the &lt;code&gt;template&lt;/code&gt; tag takes the exact same information to create any number of pods (replicas) as specified.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ensure to use matchLabels if using labels for the pods, as this will give the master information for updating the cluster.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can check all the deployments currently running with &lt;code&gt;kubectl get deployments&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="kubectl"&gt;Kubectl&lt;/h2&gt;
&lt;p&gt;Kubectl is the tool that we use to manage our Kubernetes clusters. If we want to pass a config file into &lt;code&gt;kubectl&lt;/code&gt; we use the command &lt;code&gt;kubectl apply -f [filename]&lt;/code&gt;. Similar to &lt;code&gt;docker ps&lt;/code&gt;, if we want to see all the running pods in our cluster, we can run &lt;code&gt;kubectl get pods&lt;/code&gt;. Furthermore, to get all the running services we can run &lt;code&gt;kubectl get services&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once we have a pod running, we can check to see what containers are running with &lt;code&gt;docker ps&lt;/code&gt;. If we kill the container running inside the pod, we will notice that if we run &lt;code&gt;docker ps&lt;/code&gt; once again, it'll be live again. Kubernetes will try to restart any containers if anything goes wrong. Kubernetes will try it's best to keep the application in the state that we provide in the configuration.&lt;/p&gt;
&lt;h3 id="update-existing-object"&gt;Update Existing Object&lt;/h3&gt;
&lt;p&gt;If a configuration has been provided a &lt;code&gt;name&lt;/code&gt; in the &lt;code&gt;metadata&lt;/code&gt;, the running object can be updated by changing the configuration provided the &lt;code&gt;name&lt;/code&gt; remains the same.&lt;/p&gt;
&lt;p&gt;This updated configuration can then be applied with &lt;code&gt;kubectl apply -f [filename]&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is only a specific number of parameters we can change with this (eg for pods: image, activeDeadlineSeconds, etc). Which you will see an error if the variable falls outside the provided.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For maintaining sets of identical pods, we can bypass the limitations on what fields we can update with the &lt;code&gt;Deployment&lt;/code&gt; object kind. Pods are good for one-off development purposes, while Deployments are better for development &amp;amp; production.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Deployment updates work by attempting to make the changes, and if the above error occurs, it'll automatically kill the pod and restart with the updated configuration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="get-info-about-an-existing-object"&gt;Get Info about an Existing Object&lt;/h3&gt;
&lt;p&gt;If we had an object running within a cluster, and we wanted to get information about it, we can run &lt;code&gt;kubectl describe [object_type] [object_name]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, we can extract information about all objects of a certain type in the cluster by omitting the &lt;code&gt;object_name&lt;/code&gt;. So our command would be &lt;code&gt;kubectl [object_type]&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id="deleting-existing-objects"&gt;Deleting Existing Objects&lt;/h3&gt;
&lt;p&gt;Similar to &lt;code&gt;docker stop&lt;/code&gt;, we can use &lt;code&gt;kubectl delete -f [config_yaml]&lt;/code&gt; to stop and delete an object from the cluster.&lt;/p&gt;
&lt;h3 id="update-deployment-images"&gt;Update Deployment Images&lt;/h3&gt;
&lt;p&gt;A workflow for Kubernetes is we want our application to keep running, and when we push a new image to Docker Hub, we want our Kubernetes cluster to update the objects running on this image, with the updated image.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is very challenging, here is a very thorough thread on a conversation discussing ways to do this: &lt;a href="https://github.com/kubernetes/kubernetes/issues/33664"&gt;https://github.com/kubernetes/kubernetes/issues/33664&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To do this imperatively, we ensure that the image we will be pulling is tagged with versioning on Docker Hub. After this we are able to run the command&lt;/p&gt;
&lt;table class="table-striped table highlighttable"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;kubectl &lt;span class="nb"&gt;set&lt;/span&gt; image &lt;span class="o"&gt;[&lt;/span&gt;object_type&lt;span class="o"&gt;]&lt;/span&gt; / &lt;span class="o"&gt;[&lt;/span&gt;object_name&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;container_name&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;new_image_to_use&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;After running this command, the deployment will update the running pods with the new image.&lt;/p&gt;&lt;/body&gt;</content><category term="Software"></category><category term="software"></category></entry></feed>